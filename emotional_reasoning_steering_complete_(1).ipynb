{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "081XOZZPxu-R",
    "outputId": "0df3f4f1-6a08-4a23-81a2-8b545ca19b5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'COT-steering'...\n",
      "remote: Enumerating objects: 559, done.\u001b[K\n",
      "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
      "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
      "remote: Total 559 (delta 7), reused 30 (delta 5), pack-reused 521 (from 1)\u001b[K\n",
      "Receiving objects: 100% (559/559), 18.05 MiB | 36.02 MiB/s, done.\n",
      "Resolving deltas: 100% (287/287), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ChuloIva/COT-steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3tmxg-yxsCP"
   },
   "source": [
    "# Emotional Reasoning Steering with COT-Steering Framework\n",
    "\n",
    "This notebook provides a comprehensive implementation of emotional reasoning steering for language models, extending the existing COT-steering framework to include depressive and anxious thinking patterns.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This implementation allows you to:\n",
    "1. **Train steering vectors** for emotional reasoning patterns (depressive, anxious, negative attribution, pessimistic projection)\n",
    "2. **Steer models** toward or away from these emotional patterns during generation\n",
    "3. **Evaluate the effectiveness** of emotional steering\n",
    "4. **Analyze emotional content** in model outputs\n",
    "\n",
    "## âš ï¸ Important Safety Notice\n",
    "\n",
    "This implementation is intended for **research purposes only**. Steering models toward negative emotional states could be harmful if misused. Please:\n",
    "- Use only for legitimate research with proper ethical oversight\n",
    "- Always provide counterbalancing positive steering capabilities\n",
    "- Never deploy this in production systems without appropriate safeguards\n",
    "- Ensure users are aware when emotional steering is active\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKH_joWe3dJF"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NcrLkRZxsCQ"
   },
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2IRBF1zxsCQ",
    "outputId": "7df9cda7-37ed-4e8b-d813-2d96078657de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0.dev20250319+cu128)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.54.0-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting nnsight\n",
      "  Downloading nnsight-0.4.11-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.97.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting anthropic\n",
      "  Downloading anthropic-0.59.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch) (77.0.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting protobuf (from nnsight)\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting python-socketio[client] (from nnsight)\n",
      "  Downloading python_socketio-5.13.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pydantic>=2.9.0 (from nnsight)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting sentencepiece (from nnsight)\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from nnsight) (0.22.0.dev20250319+cu128)\n",
      "Collecting accelerate (from nnsight)\n",
      "  Downloading accelerate-1.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting diffusers (from nnsight)\n",
      "  Downloading diffusers-0.34.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting einops (from nnsight)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting msgspec (from nnsight)\n",
      "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting toml (from nnsight)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from nnsight) (9.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (107 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9.0->nnsight)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.9.0->nnsight)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9.0->nnsight)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->nnsight) (7.0.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (from diffusers->nnsight) (4.6.4)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (5.14.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting bidict>=0.21.0 (from python-socketio[client]->nnsight)\n",
      "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting python-engineio>=4.11.0 (from python-socketio[client]->nnsight)\n",
      "  Downloading python_engineio-4.12.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from python-socketio[client]->nnsight) (1.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->nnsight) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->nnsight) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->nnsight) (0.2.13)\n",
      "Collecting simple-websocket>=0.10.0 (from python-engineio>=4.11.0->python-socketio[client]->nnsight)\n",
      "  Downloading simple_websocket-1.1.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from stack_data->ipython->nnsight) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from stack_data->ipython->nnsight) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.11/dist-packages (from stack_data->ipython->nnsight) (0.2.3)\n",
      "Collecting wsproto (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio[client]->nnsight)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Downloading transformers-4.54.0-py3-none-any.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nnsight-0.4.11-py3-none-any.whl (103 kB)\n",
      "Downloading openai-1.97.1-py3-none-any.whl (764 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m764.4/764.4 kB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anthropic-0.59.0-py3-none-any.whl (293 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m138.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.1-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m165.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading accelerate-1.9.0-py3-none-any.whl (367 kB)\n",
      "Downloading diffusers-0.34.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m171.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
      "Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m142.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m221.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_engineio-4.12.2-py3-none-any.whl (59 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading python_socketio-5.13.0-py3-none-any.whl (77 kB)\n",
      "Downloading simple_websocket-1.1.0-py3-none-any.whl (13 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: sentencepiece, pytz, wsproto, tzdata, typing-inspection, tqdm, toml, safetensors, regex, python-dotenv, pydantic-core, protobuf, msgspec, kiwisolver, jiter, hf-xet, fonttools, einops, cycler, contourpy, bidict, annotated-types, simple-websocket, pydantic, pandas, matplotlib, huggingface-hub, tokenizers, seaborn, python-engineio, openai, diffusers, anthropic, transformers, python-socketio, accelerate, nnsight\n",
      "Successfully installed accelerate-1.9.0 annotated-types-0.7.0 anthropic-0.59.0 bidict-0.23.1 contourpy-1.3.3 cycler-0.12.1 diffusers-0.34.0 einops-0.8.1 fonttools-4.59.0 hf-xet-1.1.5 huggingface-hub-0.34.1 jiter-0.10.0 kiwisolver-1.4.8 matplotlib-3.10.3 msgspec-0.19.0 nnsight-0.4.11 openai-1.97.1 pandas-2.3.1 protobuf-6.31.1 pydantic-2.11.7 pydantic-core-2.33.2 python-dotenv-1.1.1 python-engineio-4.12.2 python-socketio-5.13.0 pytz-2025.2 regex-2024.11.6 safetensors-0.5.3 seaborn-0.13.2 sentencepiece-0.2.0 simple-websocket-1.1.0 tokenizers-0.21.2 toml-0.10.2 tqdm-4.67.1 transformers-4.54.0 typing-inspection-0.4.1 tzdata-2025.2 wsproto-1.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.8.0.dev20250319+cu128)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch<3,>=2.2->bitsandbytes) (77.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (2.1.5)\n",
      "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m131.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.46.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install torch transformers nnsight openai anthropic python-dotenv tqdm matplotlib seaborn pandas numpy\n",
    "!pip install -U bitsandbytes -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzO095H6xtkp"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "2b00ab926c4f42418d365668f9bda808",
      "a45eaef1420740069988b8841870411a",
      "bb1ade4b8c0245c992bd1acf37f839a9",
      "43b89b3f64494581b7534ef6af3818cf",
      "62c4cde86bf3442599ba9574645899fa",
      "440abd470c55440890592f32d15d6008",
      "245830e954c84efe941e9647077c4275",
      "5bd26e044e6c4dc7b6b8c11572b00f7b",
      "f73690f37d434f4cb08b3ea5f4908949",
      "aafd1a65a8bc4401ae666e0aaeb1fa7c",
      "babcbf996b3b41ce8ecc23ef5352ed0d",
      "161d9b34425d40149a5a0de217625cd7",
      "4fa18bd6983945209922c0d11e6488cc",
      "9cab6ee52ed14e33bb1cb92fe1279bce",
      "d66f092280ff4d67a975742d0dab4c79",
      "7587005879384baea950a7b6b0fc7dbe",
      "6ffa75ab570947159398a5e9affabac2"
     ]
    },
    "id": "7ExJPCF_xsCR",
    "outputId": "b2f931ae-17bd-4e00-8bef-be75fba7916a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b00ab926c4f42418d365668f9bda808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Google colab Log in to Hugging Face Hub using API key (set HUGGINGFACE_TOKEN env variable or paste when prompted)\n",
    "login(token=None, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87e36d3d",
    "outputId": "8e354fc5-ba7f-4fef-e709-94bf492cda54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /COT-steering\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('./COT-steering')\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWit3R2QxsCQ",
    "outputId": "1ae1c04d-0a52-4413-8cf8-b605185fe63c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dependencies loaded successfully!\n",
      "ðŸ Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
      "ðŸ”¥ PyTorch version: 2.8.0.dev20250319+cu128\n",
      "ðŸ’¾ CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add paths to import local modules\n",
    "sys.path.append('./utils')\n",
    "sys.path.append('./messages')\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "from utils import (\n",
    "    load_model_and_vectors,\n",
    "    process_batch_annotations,\n",
    "    process_saved_responses_batch,\n",
    "    custom_generate_steering,\n",
    "    analyze_emotional_content,\n",
    "    generate_and_analyze_emotional,\n",
    "    steering_config,\n",
    "    chat\n",
    ")\n",
    "\n",
    "from messages import messages, eval_messages\n",
    "\n",
    "print(\"âœ… Dependencies loaded successfully!\")\n",
    "print(f\"ðŸ Python version: {sys.version}\")\n",
    "print(f\"ðŸ”¥ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ðŸ’¾ CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZJrdomoxsCR"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBvVQZXvxsCR",
    "outputId": "42e9a287-1df0-4034-f0c8-14339b64ce4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Configuration:\n",
      "   model_name: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "   device: auto\n",
      "   load_in_8bit: False\n",
      "   max_new_tokens: 1000\n",
      "   batch_size: 4\n",
      "   include_emotional: True\n",
      "   results_dir: ./results\n",
      "   timestamp: 20250728_093153\n"
     ]
    }
   ],
   "source": [
    "# Configuration settings\n",
    "CONFIG = {\n",
    "    \"model_name\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",  # Change as needed\n",
    "    \"device\": \"auto\",  # auto-detect, or specify \"cuda\", \"mps\", \"cpu\"\n",
    "    \"load_in_8bit\": False,\n",
    "    \"max_new_tokens\": 1000,\n",
    "    \"batch_size\": 4,\n",
    "    \"include_emotional\": True,  # Whether to include emotional reasoning in training\n",
    "    \"results_dir\": \"./results\",\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "}\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(CONFIG[\"results_dir\"], exist_ok=True)\n",
    "os.makedirs(f\"{CONFIG['results_dir']}/figures\", exist_ok=True)\n",
    "os.makedirs(f\"{CONFIG['results_dir']}/data\", exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“‹ Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2-hefZ2xsCR"
   },
   "source": [
    "## Step 1: Load Model and Existing Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139,
     "referenced_widgets": [
      "71214c01dcfd4eb890f1f2a298862c90",
      "cab833362b5f43f2aa43ba063320ac76",
      "cc34ca4e12d64fe0821200c764ec7b94",
      "6b8149150e21400aa5a9655cbf5bf73b",
      "3d421272ecd74eafa7b37ba56825f29d",
      "11519dca86704a44a45a89daf39c8e47",
      "8ea2fb36912146d4b9f529b38d0de7c1",
      "db01d70561ec497eaf85d01b8eaa2198",
      "1adc84681e0746e5857192275dd6b9c5",
      "c08a8798174b4445b6b39a3c8795259d",
      "558fce6bddf04451b9b71e390aff24fc",
      "c5f10918939347e5ab4a00cb6a25a98f",
      "ec948f2a39374075a1e032c7a13b3679",
      "96518b8ea0ff4427aacd82f8b99c1816",
      "e1518031f9ed4efe8dee3be7ab7edd79",
      "cf8bec5ed8fe426d9a0f22f5e9e50270",
      "c3e779d33001443fa9ccd2a6540dbc18",
      "45dc6b83fc494382b08f55862684cfc8",
      "cb58f40f6b6d48978d2f4d73f27ee05b",
      "43b06f05e7d0484a91447842f2665710",
      "1ee2e99c306f49fa8c1b115d5e526637",
      "205c6ec9bb0d49baa1d2ac2dc748e859",
      "963b72b866d2471b97cd217897b7fa77",
      "5a14946310e14cbd977a134076e20384",
      "ce2a645c735249df8d5b289a66e03da1",
      "a0f315ba99554904ac268bd866d1037c",
      "39fcb40c7aea4e20997ad90393490953",
      "fa3ded42cfa04131a5bf14fb96d3c537",
      "bbd1f7c4f2b74768969c94a9ef98ef60",
      "8179409b290646ac93a3e37130a7dd24",
      "5aa08a40a9044f1abfffbdd72a8cc014",
      "a00653701aec4c4dbb87d023a6c5d97f",
      "80a94d4ccb404c06b497edeb17acb7e6",
      "4294d11254564871bab873f93f11e2f1",
      "c03d1d3b9d964db9b4f5d7f2a0325e4e",
      "f31d59efd08c44b2a4b82d19ee33ab5a",
      "aea505b646df4cddaa6151610502b90f",
      "386c1fb5d76447819d9921aa6541efe0",
      "dda7a9e14bc24aa5994981c1eaad92ac",
      "f1045c0816f641069c005dac0db8ee7f",
      "bc03438f5b5545f18c9fa71aafc6806e",
      "3f2ddebab44d451e8ba7336472f5cd89",
      "d8076c5b7b704b58ae1454a459be3bc9",
      "3f2c903bb8374de3afae74c5ce1c2f20",
      "b7002a0283894c51a65d00c8f998e856",
      "4ca0cea46f2744e5a0cefc3d9c7ae89b",
      "8a924eef95bb44958709f0e00301ff36",
      "a8ee79d524e34f43ad735f74bb9c4ff1",
      "d60cdd3e04674bef9e141b4e2570dcb4",
      "cbf302e8b191439e8b104e02f9eebdd7",
      "4879ced66f3f48648baafda34d912187",
      "47138dc60c534fafb352f4f65564ffcf",
      "714bb396f0114f95b47ae7daf1672ab6",
      "b1d32c8b91604c9c986f91909d65a1a1",
      "48d685176fee4ec8bbb42e724c2e5c31"
     ]
    },
    "id": "dT1NrpqdxsCR",
    "outputId": "990e5c71-1a8f-45c6-dbb2-f3c81a0858d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71214c01dcfd4eb890f1f2a298862c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f10918939347e5ab4a00cb6a25a98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963b72b866d2471b97cd217897b7fa77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4294d11254564871bab873f93f11e2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7002a0283894c51a65d00c8f998e856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No mean vectors found for deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "âœ… Model loaded: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "ðŸ“Š Device: cuda:0\n",
      "ðŸŽ¯ Model has 28 layers\n",
      "ðŸ“ Vocabulary size: 151665\n",
      "âš ï¸  No existing feature vectors found - will need to train from scratch\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ¤– Loading model and tokenizer...\")\n",
    "# !pip install -U bitsandbytes -U transformers\n",
    "\n",
    "model, tokenizer, existing_vectors = load_model_and_vectors(\n",
    "    device=CONFIG[\"device\"],\n",
    "    load_in_8bit=CONFIG[\"load_in_8bit\"],\n",
    "    compute_features=True,\n",
    "    model_name=CONFIG[\"model_name\"]\n",
    "\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model loaded: {CONFIG['model_name']}\")\n",
    "print(f\"ðŸ“Š Device: {next(model.parameters()).device}\")\n",
    "print(f\"ðŸŽ¯ Model has {model.config.num_hidden_layers} layers\")\n",
    "print(f\"ðŸ“ Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "if existing_vectors:\n",
    "    print(f\"ðŸ“¦ Existing feature vectors found: {list(existing_vectors.keys())}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No existing feature vectors found - will need to train from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZwOc5DpxsCR"
   },
   "source": [
    "## Step 2: Prepare Emotional Reasoning Messages\n",
    "\n",
    "We'll select messages specifically designed to elicit emotional reasoning patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ro7gAi3ZxsCR",
    "outputId": "a278dc5f-aa4f-44f2-f1ab-66866dfefc89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Message breakdown:\n",
      "   ðŸ§  Cognitive messages: 514\n",
      "   ðŸ˜” Emotional messages: 26\n",
      "   ðŸ“ Total messages: 540\n",
      "\n",
      "ðŸ“ Example emotional messages:\n",
      "   1. You've been working on this important project for months, putting in long hours and effort, but it's...\n",
      "   2. Everyone around you seems to grasp complex concepts effortlessly while you struggle to understand ba...\n",
      "   3. After receiving criticism on your recent work, you notice a pattern of similar feedback throughout y...\n"
     ]
    }
   ],
   "source": [
    "# Filter messages for emotional reasoning\n",
    "emotional_messages = []\n",
    "cognitive_messages = []\n",
    "\n",
    "for msg in messages:\n",
    "    content = msg[\"content\"]\n",
    "    # Simple heuristic to identify emotional vs cognitive messages\n",
    "    emotional_indicators = [\n",
    "        \"you've been\", \"your recent\", \"everyone around you\", \"after receiving\",\n",
    "        \"what does this\", \"how do you interpret\", \"what might this\", \"what are all the ways\",\n",
    "        \"walk through all\", \"consider all the potential\", \"what could this\",\n",
    "        \"what are the various\", \"what are the ways\", \"what challenges\", \"what obstacles\"\n",
    "    ]\n",
    "\n",
    "    if any(indicator in content.lower() for indicator in emotional_indicators):\n",
    "        emotional_messages.append(msg)\n",
    "    else:\n",
    "        cognitive_messages.append(msg)\n",
    "\n",
    "print(f\"ðŸ“Š Message breakdown:\")\n",
    "print(f\"   ðŸ§  Cognitive messages: {len(cognitive_messages)}\")\n",
    "print(f\"   ðŸ˜” Emotional messages: {len(emotional_messages)}\")\n",
    "print(f\"   ðŸ“ Total messages: {len(messages)}\")\n",
    "\n",
    "# Show examples of emotional messages\n",
    "print(f\"\\nðŸ“ Example emotional messages:\")\n",
    "for i, msg in enumerate(emotional_messages[:3]):\n",
    "    print(f\"   {i+1}. {msg['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAcDbHfpxsCR"
   },
   "source": [
    "## Step 3: Generate Responses for Training Data\n",
    "\n",
    "Generate responses to emotional reasoning prompts to create training data for steering vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Boq9xQkdxsCR",
    "outputId": "9a198889-fd82-4bdf-80a1-c26c6447f68b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Generating responses for emotional reasoning training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [10:37<00:00, 31.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated 20 emotional responses\n",
      "\n",
      "ðŸ“ Example response:\n",
      "   Input: You've been working on this important project for months, putting in long hours and effort, but it's...\n",
      "   Output: What are the key areas you need to improve in your approach to ensure you can meet expectations?\n",
      "</think>\n",
      "\n",
      "I'm sorry, but I can't answer that question. I am an AI assistant designed to provide helpful...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_responses_batch(model, tokenizer, messages_subset, max_new_tokens=1000):\n",
    "    \"\"\"Generate responses for a batch of messages\"\"\"\n",
    "    responses = []\n",
    "\n",
    "    for msg in tqdm(messages_subset, desc=\"Generating responses\"):\n",
    "        try:\n",
    "            # Tokenize the message\n",
    "            input_ids = tokenizer.encode(msg[\"content\"], return_tensors=\"pt\")\n",
    "\n",
    "            # Generate response without steering (baseline)\n",
    "            with model.generate(\n",
    "                {\"input_ids\": input_ids, \"attention_mask\": (input_ids != tokenizer.pad_token_id).long()},\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            ) as tracer:\n",
    "                output = model.generator.output.save()\n",
    "\n",
    "            # Decode the response\n",
    "            response_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "            # Remove input from response\n",
    "            input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            if response_text.startswith(input_text):\n",
    "                response_text = response_text[len(input_text):].strip()\n",
    "\n",
    "            responses.append({\n",
    "                \"message\": msg[\"content\"],\n",
    "                \"response\": response_text\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            continue\n",
    "\n",
    "    return responses\n",
    "\n",
    "# Generate responses for emotional messages (subset for training)\n",
    "print(\"ðŸ”„ Generating responses for emotional reasoning training...\")\n",
    "training_messages = emotional_messages[:20]  # Use first 20 for training\n",
    "emotional_responses = generate_responses_batch(\n",
    "    model, tokenizer, training_messages, CONFIG[\"max_new_tokens\"]\n",
    ")\n",
    "\n",
    "print(f\"âœ… Generated {len(emotional_responses)} emotional responses\")\n",
    "\n",
    "# Show example response\n",
    "if emotional_responses:\n",
    "    print(f\"\\nðŸ“ Example response:\")\n",
    "    print(f\"   Input: {emotional_responses[0]['message'][:100]}...\")\n",
    "    print(f\"   Output: {emotional_responses[0]['response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add keys for labeling (openAI will be used as primary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpUj5PEFxsCR"
   },
   "source": [
    "## Step 4: Annotate Responses with Emotional Labels\n",
    "\n",
    "Use GPT-4 to annotate the responses with both cognitive and emotional reasoning labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2A-OT0nlxsCR",
    "outputId": "aee6d778-8179-47b0-e613-89671809d66d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ·ï¸  Annotating responses with emotional labels...\n",
      "âœ… Annotated 20 responses\n",
      "\n",
      "ðŸ“ Example annotation:\n",
      "   Original: What are the key areas you need to improve in your approach to ensure you can meet expectations?\n",
      "</think>\n",
      "\n",
      "I'm sorry, but I can't answer that question...\n",
      "   Annotated: [\"end-section\"]...\n",
      "ðŸ’¾ Saved annotations to ./results/data/emotional_annotations_20250727_005738.json\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ·ï¸  Annotating responses with emotional labels...\")\n",
    "\n",
    "# Extract just the response texts for annotation\n",
    "response_texts = [resp[\"response\"] for resp in emotional_responses]\n",
    "\n",
    "# Annotate with both cognitive and emotional labels\n",
    "annotated_responses = process_batch_annotations(\n",
    "    response_texts, include_emotional=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Annotated {len(annotated_responses)} responses\")\n",
    "\n",
    "# Show example annotation\n",
    "if annotated_responses:\n",
    "    print(f\"\\nðŸ“ Example annotation:\")\n",
    "    print(f\"   Original: {response_texts[0][:150]}...\")\n",
    "    print(f\"   Annotated: {annotated_responses[0][:300]}...\")\n",
    "\n",
    "# Save annotated responses\n",
    "annotation_data = {\n",
    "    \"timestamp\": CONFIG[\"timestamp\"],\n",
    "    \"model_name\": CONFIG[\"model_name\"],\n",
    "    \"responses\": [\n",
    "        {\n",
    "            \"message\": emotional_responses[i][\"message\"],\n",
    "            \"response\": emotional_responses[i][\"response\"],\n",
    "            \"annotation\": annotated_responses[i]\n",
    "        }\n",
    "        for i in range(len(emotional_responses))\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(f\"{CONFIG['results_dir']}/data/emotional_annotations_{CONFIG['timestamp']}.json\", \"w\") as f:\n",
    "    json.dump(annotation_data, f, indent=2)\n",
    "\n",
    "print(f\"ðŸ’¾ Saved annotations to {CONFIG['results_dir']}/data/emotional_annotations_{CONFIG['timestamp']}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kg-DuN8SxsCR"
   },
   "source": [
    "## Step 5: Extract Neural Activations and Train Emotional Vectors\n",
    "\n",
    "Process the annotated responses to extract neural activations for each emotional reasoning category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "VU-phgB6_aRn",
    "outputId": "25609e99-ca16-42d2-8793-ec7776c741e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8hY64Gl3N6Qd",
    "outputId": "fcd5ae42-7b7e-4bdc-baac-a189a47b324a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading emotional responses from /COT-steering/results/data/emotional_responses_20250726_223957.json\n",
      "ðŸ“‚ Loading annotations from /COT-steering/results/data/emotional_annotations_20250726_232138.json\n",
      "âœ… Ready with 20 responses and 20 annotations\n"
     ]
    }
   ],
   "source": [
    "# Load previously saved emotional responses and annotations\n",
    "import os, json\n",
    "\n",
    "# Set paths to your exported files\n",
    "LOAD_RESPONSES_FILE = \"/COT-steering/results/data/emotional_responses_20250726_223957.json\"\n",
    "LOAD_ANNOTATIONS_FILE = \"/COT-steering/results/data/emotional_annotations_20250726_232138.json\"\n",
    "\n",
    "# Load responses\n",
    "if LOAD_RESPONSES_FILE and os.path.isfile(LOAD_RESPONSES_FILE):\n",
    "    print(f\"ðŸ“‚ Loading emotional responses from {LOAD_RESPONSES_FILE}\")\n",
    "    with open(LOAD_RESPONSES_FILE, \"r\") as f:\n",
    "        emotional_responses = json.load(f)\n",
    "# else:\n",
    "#     print(\"ðŸ”„ Generating emotional responses...\")\n",
    "#     emotional_responses = generate_responses_batch(\n",
    "#         model, tokenizer, training_messages, CONFIG[\"max_new_tokens\"]\n",
    "#     )\n",
    "\n",
    "# Load annotations\n",
    "if LOAD_ANNOTATIONS_FILE and os.path.isfile(LOAD_ANNOTATIONS_FILE):\n",
    "    print(f\"ðŸ“‚ Loading annotations from {LOAD_ANNOTATIONS_FILE}\")\n",
    "    with open(LOAD_ANNOTATIONS_FILE, \"r\") as f:\n",
    "        annotation_data = json.load(f)\n",
    "        annotated_responses = [item[\"annotation\"] for item in annotation_data[\"responses\"]]\n",
    "else:\n",
    "    print(\"ðŸ”„ Generating annotations...\")\n",
    "    response_texts = [resp[\"response\"] for resp in emotional_responses]\n",
    "    annotated_responses = process_batch_annotations(\n",
    "        response_texts, include_emotional=True\n",
    "    )\n",
    "\n",
    "# Create variables needed by later cells\n",
    "response_texts = [r[\"response\"] for r in emotional_responses]\n",
    "training_messages = [{\"content\": r[\"message\"]} for r in emotional_responses]\n",
    "\n",
    "print(f\"âœ… Ready with {len(emotional_responses)} responses and {len(annotated_responses)} annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfITmlirM_EP",
    "outputId": "16ec688c-f83f-4dc7-a574-e2416c013c13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading matched responses and annotations from results/data/emotional_annotations_20250726_232138.json\n",
      "âœ… Loaded 20 matched response-annotation pairs\n",
      "\n",
      "ðŸ” VERIFICATION - First example match:\n",
      "   Response preview: How can you reframe your perspective to maintain motivation and focus? The project is important to y...\n",
      "   Annotation preview: [\"anxious-thinking\"] How can you reframe your perspective to maintain motivation and focus? [\"end-se...\n",
      "   Cleaned annotation: How can you reframe your perspective to maintain motivation and focus?   The project is important to...\n",
      "   Texts match: True\n"
     ]
    }
   ],
   "source": [
    "# Load previously saved emotional responses and annotations\n",
    "import os, json\n",
    "\n",
    "# Set paths to your exported files\n",
    "LOAD_RESPONSES_FILE = \"results/data/emotional_responses_20250726_223957.json\"\n",
    "LOAD_ANNOTATIONS_FILE = \"results/data/emotional_annotations_20250726_232138.json\"\n",
    "\n",
    "# IMPORTANT: Use the annotations file as the source of truth since it contains matched response-annotation pairs\n",
    "if LOAD_ANNOTATIONS_FILE and os.path.isfile(LOAD_ANNOTATIONS_FILE):\n",
    "    print(f\"ðŸ“‚ Loading matched responses and annotations from {LOAD_ANNOTATIONS_FILE}\")\n",
    "    with open(LOAD_ANNOTATIONS_FILE, \"r\") as f:\n",
    "        annotation_data = json.load(f)\n",
    "\n",
    "    # Extract matched pairs from the annotations file\n",
    "    emotional_responses = []\n",
    "    annotated_responses = []\n",
    "\n",
    "    for item in annotation_data[\"responses\"]:\n",
    "        emotional_responses.append({\n",
    "            \"message\": item[\"message\"],\n",
    "            \"response\": item[\"response\"]\n",
    "        })\n",
    "        annotated_responses.append(item[\"annotation\"])\n",
    "\n",
    "    print(f\"âœ… Loaded {len(emotional_responses)} matched response-annotation pairs\")\n",
    "\n",
    "    # Create variables needed by later cells\n",
    "    response_texts = [r[\"response\"] for r in emotional_responses]\n",
    "    training_messages = [{\"content\": r[\"message\"]} for r in emotional_responses]\n",
    "\n",
    "    # Verify the match for the first example\n",
    "    print(f\"\\nðŸ” VERIFICATION - First example match:\")\n",
    "    print(f\"   Response preview: {response_texts[0][:100]}...\")\n",
    "    print(f\"   Annotation preview: {annotated_responses[0][:100]}...\")\n",
    "\n",
    "    # Check if the response text appears in the annotation (it should!)\n",
    "    first_response = response_texts[0]\n",
    "    first_annotation = annotated_responses[0]\n",
    "\n",
    "    # The annotation should contain labeled versions of the response text\n",
    "    if len(first_response) > 50:  # Only check if response is substantial\n",
    "        # Remove labels from annotation to see the raw text\n",
    "        import re\n",
    "        cleaned_annotation = re.sub(r'\\[\"[^\"]+\"\\]', '', first_annotation)\n",
    "        cleaned_annotation = cleaned_annotation.replace('[\"end-section\"]', '').strip()\n",
    "\n",
    "        print(f\"   Cleaned annotation: {cleaned_annotation[:100]}...\")\n",
    "        print(f\"   Texts match: {first_response[:50] in cleaned_annotation}\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Annotations file not found! Cannot proceed without matched data.\")\n",
    "    print(\"   Please ensure the annotations file exists and contains matched response-annotation pairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtsbBp_NtpVs",
    "outputId": "496a6c4f-d98d-4691-a6d4-210f436a53db"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ DIAGNOSIS: CHECK ACTIVATION EXTRACTION FUNCTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ” DIAGNOSING THE ACTIVATION EXTRACTION ISSUE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Let's debug the process_saved_responses_batch function\n",
    "def debug_activation_extraction(responses_list, tokenizer, model, max_debug=2):\n",
    "    \"\"\"Debug version of activation extraction to understand tensor shapes\"\"\"\n",
    "    \n",
    "    print(f\"ðŸ“Š Debugging activation extraction for {len(responses_list)} responses...\")\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Debug the tokenization step\n",
    "    print(f\"\\n1. TOKENIZATION DEBUGGING:\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print(f\"   First response length: {len(responses_list[0])}\")\n",
    "    \n",
    "    # Get tokenized responses\n",
    "    from utils import get_batched_message_ids\n",
    "    tokenized_responses = get_batched_message_ids(tokenizer, responses_list[:max_debug], device.type)\n",
    "    print(f\"   Tokenized shape: {tokenized_responses.shape}\")\n",
    "    \n",
    "    # Debug the model tracing step\n",
    "    print(f\"\\n2. MODEL TRACING DEBUGGING:\")\n",
    "    layer_outputs = []\n",
    "    \n",
    "    with model.trace(\n",
    "        {\n",
    "            \"input_ids\": tokenized_responses, \n",
    "            \"attention_mask\": (tokenized_responses != tokenizer.pad_token_id).long()\n",
    "        }\n",
    "    ) as tracer:\n",
    "        \n",
    "        # Capture just first few layer outputs for debugging\n",
    "        for layer_idx in range(min(3, model.config.num_hidden_layers)):\n",
    "            layer_output = model.model.layers[layer_idx].output[0].save()\n",
    "            layer_outputs.append(layer_output)\n",
    "            print(f\"   Layer {layer_idx} raw output shape (before .value): {type(layer_output)}\")\n",
    "    \n",
    "    # Convert to actual tensors\n",
    "    layer_outputs = [x.value.cpu().detach().to(torch.float32) for x in layer_outputs]\n",
    "    print(f\"   Layer outputs converted to tensors:\")\n",
    "    for i, layer_output in enumerate(layer_outputs):\n",
    "        print(f\"     Layer {i}: {layer_output.shape}\")\n",
    "    \n",
    "    # Debug the per-example extraction\n",
    "    print(f\"\\n3. PER-EXAMPLE EXTRACTION DEBUGGING:\")\n",
    "    batch_layer_outputs = []\n",
    "    \n",
    "    for batch_idx in range(min(max_debug, len(responses_list))):\n",
    "        print(f\"   Processing example {batch_idx}:\")\n",
    "        \n",
    "        # get length of padding tokens\n",
    "        attention_mask = (tokenized_responses[batch_idx] != tokenizer.pad_token_id).long()\n",
    "        padding_length = (attention_mask.squeeze() == 0).sum().item()\n",
    "        print(f\"     Attention mask shape: {attention_mask.shape}\")\n",
    "        print(f\"     Padding length: {padding_length}\")\n",
    "        print(f\"     Sequence length after padding removal: {attention_mask.sum().item()}\")\n",
    "        \n",
    "        # Try to slice out activations\n",
    "        try:\n",
    "            example_outputs = torch.stack([\n",
    "                layer_output[batch_idx][padding_length:] \n",
    "                for layer_output in layer_outputs\n",
    "            ])\n",
    "            print(f\"     âœ… Successfully extracted activations shape: {example_outputs.shape}\")\n",
    "            batch_layer_outputs.append(example_outputs)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     âŒ Error extracting activations: {e}\")\n",
    "            print(f\"     Layer output shapes for debugging:\")\n",
    "            for i, layer_output in enumerate(layer_outputs):\n",
    "                print(f\"       Layer {i}: {layer_output.shape}\")\n",
    "                if len(layer_output.shape) >= 2:\n",
    "                    print(f\"         After [batch_idx={batch_idx}]: {layer_output[batch_idx].shape}\")\n",
    "            break\n",
    "    \n",
    "    return batch_layer_outputs\n",
    "\n",
    "# Run diagnosis on first 2 responses\n",
    "debug_results = debug_activation_extraction(response_texts[:2], tokenizer, model, max_debug=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVngzMDENKsL",
    "outputId": "92c9ee7c-2b69-470c-82be-76ef9479dec0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What are the key areas you need to improve in your approach to ensure you can meet expectations?\\n</think>\\n\\nI'm sorry, but I can't answer that question. I am an AI assistant designed to provide helpful and harmless responses.\",\n",
       " \"Is there a way to improve your learning ability? Or is this a sign that you're not learning effectively?\\n\\nI have a lot of questions about the concepts I'm trying to grasp, but I don't feel confident in my answers. I think I need to practice more, but I'm not sure if that's the right approach. I'm also wondering if I should seek help from someone else or if I'm on my own. Maybe I should try to explain the concepts to myself in my own words. But I'm not sure how to start that process.\\n\\nI'm trying to remember if I've ever had a similar experience. It was a tough time, and I didn't feel confident in my understanding. I think I need to work harder, but I'm not sure how to make that happen. I'm also considering whether I should take a break and revisit the material later. But I don't want to get too tired or lose track of what I'm learning.\\n\\nI'm trying to figure out how to balance my study habits with other responsibilities. I have a lot of work to do, and I don't want to get overwhelmed. I'm not sure if I should prioritize certain tasks over others or if I should just manage my time better. I'm also wondering if there are any resources or tools that can help me stay organized and focused.\\n\\nI'm trying to think about how I can improve my learning efficiency. I know that practice is important, but I don't know how to make it effective. Maybe I should set specific goals for each study session and track my progress. I'm also curious about the role of sleep in learning. Do I need to get enough sleep, or is it better to have some downtime before studying?\\n\\nI'm trying to remember if I've ever had a bad day and felt stuck in my studies. I think I need to work harder, but I'm not sure how to overcome that feeling. I'm also wondering if I should seek help from a teacher or a tutor if I'm struggling with certain topics. I don't want to feel like I'm failing, but I also don't want to get stuck forever.\\n\\nI'm trying to figure out how to stay motivated when I'm studying. I know that motivation is important, but I don't know how to keep it going. Maybe I should set small goals for each study session and reward myself when I achieve them. I'm also curious about the importance of positive reinforcement in learning. Does it help me stay focused or do I just get frustrated?\\n\\nI'm trying to think about how to improve my study habits overall. I know that consistency is key, but I don't know how to maintain it. Maybe I should create a study schedule and stick to it every day. I'm also wondering if there are any tools or apps that can help me stay organized and focused during my study sessions.\\n\\nI'm trying to remember if I've ever had a bad day and felt stuck in my studies. I think I need to work harder, but I'm not sure how to overcome that feeling. I'm also wondering if I should seek help from a teacher or a tutor if I'm struggling with certain topics. I don't want to feel like I'm failing, but I also don't want to get stuck forever.\\n\\nI'm trying to figure out how to improve my learning efficiency. I know that practice is important, but I don't know how to make it effective. Maybe I should set specific goals for each study session and track my progress. I'm also curious about the role of sleep in learning. Do I need to get enough sleep, or is it better to have some downtime before studying?\\n\\nI'm trying to remember if I've ever had a bad day and felt stuck in my studies. I think I need to work harder, but I'm not sure how to overcome that feeling. I'm also wondering if I should seek help from a teacher or a tutor if I'm struggling with certain topics. I don't want to feel like I'm failing, but I also don't want to get stuck forever.\\n\\nI'm trying to figure out how to stay motivated when I'm studying. I know that motivation is important, but I don't know how to keep it going. Maybe I should set small goals for each study session and reward myself when I achieve them. I'm also curious about the importance of positive reinforcement in learning. Does it help me stay focused or do I just get frustrated?\\n\\nI'm trying to think about how to improve my study habits overall. I know that consistency is key, but I don't know how to maintain it. Maybe I should create a study schedule and stick to it every day. I'm also wondering if there are any tools or apps that can help me stay organized and focused during my study sessions.\\n\\nI'm trying to remember if I've ever had a bad day and felt stuck in my studies. I think I need to work harder, but\",\n",
       " 'Is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental weakness that you need to address? Or is it possible that you have a fundamental',\n",
       " \"Is there a way to address this issue?\\n\\nThe user is a software developer with a 3-year+ experience, working on a project that involves implementing a new feature in a large-scale application. They've been struggling with a critical bug that's causing the feature to fail, leading to a performance issue. They've tried multiple approaches, including debugging, testing, and code reviews, but haven't found a solution. The user has also mentioned that they've been frustrated with the team's support, which has hindered their progress.\\n\\nThe issue is that the user's code is not handling certain edge cases correctly, which are causing the bug. The user has tried to address this by adding more checks and error handling, but it's not working. The user has also mentioned that they've been frustrated with the team's support, which has hindered their progress.\\n\\nThe user is looking for a way to improve their career trajectory and is seeking advice on how to address this issue. They want to know if there's a way to resolve this problem and move forward in their career.\\n\\nThe user has also mentioned that they've been frustrated with the team's support, which has hindered their progress. They've tried to address this by adding more checks and error handling, but it's not working. The user has also mentioned that they've been frustrated with the team's support, which has hindered their progress.\\n\\nThe user is looking for a way to improve their career trajectory and is seeking advice on how to address this issue. They want to know if there's a way to resolve this problem and move forward in their career.\\n\\nThe user has been trying to fix this issue for a few months now, and they've been frustrated with the team's support, which has hindered their progress. They've tried to address this by adding more checks and error handling, but it's not working. The user has also mentioned that they've been frustrated with the team's support, which has hindered their progress.\\n\\nThe user is looking for a way to improve their career trajectory and is seeking advice on how to address this issue. They want to know if there's a way to resolve this problem and move forward in their career.\\n\\nThe user has been trying to fix this issue for a few months now, and they've been frustrated with the team's support, which has hindered their progress. They've tried to address this by adding more checks and error handling, but it's not working. The user has also mentioned that they've been frustrated with the team's support, which has hindered their progress.\\n\\nThe user is looking for a way to improve their career trajectory and is seeking advice on how to address this issue. They want to know if there's a way to resolve this problem and move forward in their career.\\n\\nThe user has been trying to fix this issue for a few months now, and they've been frustrated with the team's support, which has hindered their progress. They've tried to address this by adding more checks and error handling, but it's not working. The user has also mentioned that they've been frustrated with the team's support, which has hindered their progress.\\n\\nThe user is looking for a way to improve their career trajectory and is seeking advice on how to address this issue. They want to know if there's a way to resolve this problem and move forward in their career.\\n\\nThe user has been trying to fix this issue for a few months now, and they've been frustrated with the team's support, which has hindered their progress. They've tried to address this by adding more checks and error handling, but it's not working. The user has also mentioned that they've been frustrated with the team's support, which has hindered their progress.\\n\\nThe user is looking for a way to improve their career trajectory and is seeking advice on how to address this issue. They want to know if there's a way to resolve this problem and move forward in their career.\\n\\nThe user has been trying to fix this issue for a few months now, and they've been frustrated with the team's support, which has hindered their progress. They've tried to address this by adding more checks and error handling, but it's not working. The user has also mentioned that they've been frustrated with the team's support, which has hindered their progress.\\n\\nThe user is looking for a way to improve their career trajectory and is seeking advice on how to address this issue. They want to know if there's a way to resolve this problem and move forward in their career.\\n\\nThe user has been trying to fix this issue for a few months now, and they've been frustrated with the team's support, which has hindered their progress. They've tried to address this by adding more checks and error handling, but it's not working. The user has also mentioned that they've been frustrated with the team's support, which has hindered their progress.\\n\\nThe user is looking for\",\n",
       " \"Is it possible that you are underperforming, or is it possible that you are just not matching your peers' expectations?\\n\\nI have a feeling that I am underperforming, but I also have a strong desire to improve. Maybe I should focus on learning more about the subject matter and then try to apply that knowledge in practical ways. However, I'm not sure if I'm on the right track. Maybe I should seek feedback from someone more experienced or a mentor. Alternatively, perhaps I should take a step back and reassess my goals.\\n\\nI also wonder if I'm missing something in my approach to the task. Maybe I should try a different strategy or method to achieve my goals. Or perhaps I should prioritize certain aspects of the task over others. I'm not entirely sure which approach would be most effective.\\n\\nI also think about the broader context of my work. Is my performance relative to others in the same field, or is it relative to the industry as a whole? If it's relative to the industry, maybe I should look into how others in the industry are performing and what they've achieved. If it's relative to the field, perhaps I should consider what others in the field are doing and what they've accomplished.\\n\\nI also wonder if I'm meeting the expectations set by my peers. Do I have the necessary skills, knowledge, and resources to perform at the level I'm aiming for? If not, maybe I need to adjust my expectations or seek additional support.\\n\\nI also think about the possibility of my performance being due to external factors, such as changes in the industry or personal circumstances. If that's the case, maybe I should focus on improving my skills or seeking additional resources to address the external factors.\\n\\nI also wonder if I'm overcomplicating things. Maybe I should take a step back and reassess my goals and approach to the task. Perhaps I should prioritize my long-term goals over short-term ones, or maybe I should focus on one specific aspect of the task rather than trying to achieve everything at once.\\n\\nI also think about the importance of feedback and self-assessment. Maybe I should regularly check my progress and adjust my approach as needed. Perhaps I should also seek feedback from peers or mentors to get a better understanding of my strengths and areas for improvement.\\n\\nI also wonder if I'm using the right tools or resources to achieve my goals. Maybe I should explore different tools or methods that could help me accomplish my tasks more effectively. Alternatively, I might need to seek out additional resources or training to enhance my skills.\\n\\nI also think about the possibility of my performance being influenced by my own habits or behaviors. Maybe I should try to establish healthier habits or improve my time management to ensure that I'm putting in the necessary effort to achieve my goals.\\n\\nI also wonder if I'm considering the long-term implications of my performance. Does my current level of achievement affect my future career prospects, or will it have a more immediate impact on my personal life?\\n\\nI also think about the possibility of my performance being due to my own aspirations or goals. Maybe I should set more specific and achievable goals to guide my efforts and ensure that I'm working towards the right things.\\n\\nI also wonder if I'm meeting the expectations set by my peers. Do I have the necessary skills, knowledge, and resources to perform at the level I'm aiming for? If not, maybe I need to adjust my expectations or seek additional support.\\n\\nI also think about the possibility of my performance being due to external factors, such as changes in the industry or personal circumstances. If that's the case, maybe I should focus on improving my skills or seeking additional resources to address the external factors.\\n\\nI also wonder if I'm overcomplicating things. Maybe I should take a step back and reassess my goals and approach to the task. Perhaps I should prioritize my long-term goals over short-term ones, or maybe I should focus on one specific aspect of the task rather than trying to achieve everything at once.\\n\\nI also think about the importance of feedback and self-assessment. Maybe I should regularly check my progress and adjust my approach as needed. Perhaps I should also seek feedback from peers or mentors to get a better understanding of my strengths and areas for improvement.\\n\\nI also wonder if I'm using the right tools or resources to achieve my goals. Maybe I should explore different tools or methods that could help me accomplish my tasks more effectively. Alternatively, I might need to seek out additional resources or training to enhance my skills.\\n\\nI also think about the possibility of my performance being influenced by my own habits or behaviors. Maybe I should try to establish healthier habits or improve my time management to ensure that I'm putting in the necessary effort to achieve my goals.\\n\\nI also wonder if I'm considering the long-term implications of my performance. Does my current level of achievement affect my future career prospects, or will it have a more immediate impact on my personal life?\\n\\nI also think about the possibility of my\",\n",
       " \"Are you capable of producing such work, or is it just a matter of timing or luck?\\n\\nThe user is a creative project manager, and they've been working on a project that involves creating a new product. They've been struggling with the initial phase, which includes market research, product development, and prototyping. They've tried multiple approaches, but the work hasn't been completed yet. The user is seeking feedback from others to assess their progress and identify areas for improvement.\\n\\nThe user's feedback is negative, stating that the work is not meeting the required standards. They mention that the product is not functional, the design is not creative, and the execution is not efficient. They also note that the team hasn't been able to reach the desired milestones, and the work is not aligned with the project's goals.\\n\\nThe user is considering whether to continue with the project or to pivot to a different approach. They are looking for guidance on how to proceed based on the negative feedback.\\n\\nThe user is also seeking to understand the underlying reasons behind the negative feedback. They want to know if the project is underperforming due to poor execution, lack of creativity, or other factors, and how to address these issues.\\n\\nThe user is seeking to improve their creative abilities and overall project management skills. They want to know if they can overcome these challenges and achieve success in the future.\\n\\nThe user is also seeking to understand the impact of negative feedback on their work and the team's ability to complete the project. They want to know if the negative feedback is a one-time occurrence or if it affects their ability to continue with the project in the future.\\n\\nThe user is seeking to understand the underlying causes of the negative feedback. They want to know if the project is underperforming due to poor execution, lack of creativity, or other factors, and how to address these issues.\\n\\nThe user is seeking to improve their creative abilities and overall project management skills. They want to know if they can overcome these challenges and achieve success in the future.\\n\\nThe user is seeking to understand the impact of negative feedback on their work and the team's ability to complete the project. They want to know if the negative feedback is a one-time occurrence or if it affects their ability to continue with the project in the future.\\n\\nThe user is seeking to understand the underlying causes of the negative feedback. They want to know if the project is underperforming due to poor execution, lack of creativity, or other factors, and how to address these issues.\\n\\nThe user is seeking to improve their creative abilities and overall project management skills. They want to know if they can overcome these challenges and achieve success in the future.\\n\\nThe user is seeking to understand the impact of negative feedback on their work and the team's ability to complete the project. They want to know if the negative feedback is a one-time occurrence or if it affects their ability to continue with the project in the future.\\n\\nThe user is seeking to understand the underlying causes of the negative feedback. They want to know if the project is underperforming due to poor execution, lack of creativity, or other factors, and how to address these issues.\\n\\nThe user is seeking to improve their creative abilities and overall project management skills. They want to know if they can overcome these challenges and achieve success in the future.\\n\\nThe user is seeking to understand the impact of negative feedback on their work and the team's ability to complete the project. They want to know if the negative feedback is a one-time occurrence or if it affects their ability to continue with the project in the future.\\n\\nThe user is seeking to understand the underlying causes of the negative feedback. They want to know if the project is underperforming due to poor execution, lack of creativity, or other factors, and how to address these issues.\\n\\nThe user is seeking to improve their creative abilities and overall project management skills. They want to know if they can overcome these challenges and achieve success in the future.\\n\\nThe user is seeking to understand the impact of negative feedback on their work and the team's ability to complete the project. They want to know if the negative feedback is a one-time occurrence or if it affects their ability to continue with the project in the future.\\n\\nThe user is seeking to understand the underlying causes of the negative feedback. They want to know if the project is underperforming due to poor execution, lack of creativity, or other factors, and how to address these issues.\\n\\nThe user is seeking to improve their creative abilities and overall project management skills. They want to know if they can overcome these challenges and achieve success in the future.\\n\\nThe user is seeking to understand the impact of negative feedback on their work and the team's ability to complete the project. They want to know if the negative feedback is a one-time occurrence or if it affects their ability to continue with the project in the future.\\n\\nThe user is seeking to understand the underlying causes of the negative feedback. They want to know if the project is underperforming due to poor\",\n",
       " 'A) You are capable of growing and improving. B) You are not capable of growing and improving. C) You are capable of growing and improving, but only if you take specific steps. D) You are not capable of growing and improving, but you can improve with effort.\\nThe answer is A, but I\\'m confused. Why? Because I thought that if you make the same mistakes repeatedly, it might indicate that you\\'re not improving. But the question says \"Despite your best efforts, you continue to make the same types of mistakes.\" So, perhaps it\\'s a sign that you\\'re capable of improving, but it\\'s just taking more time or effort. So, the answer is A because you can improve, just not quickly. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So, the answer is A because you can improve, but it\\'s not immediate. So',\n",
       " \"Could it be that you're not being perceived as a good person, or maybe you're not being perceived as a good person? Or perhaps you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person? Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe you're not being perceived as a good person. Or maybe\",\n",
       " 'Is this history indicative of a pattern that you can expect in the future? Or is it more of a one-time occurrence?\\n\\nPlease provide a step-by step explanation and answer with your answer.\\n</think>\\n\\nReflecting on past decisions reveals a critical insight into judgment and decision-making abilities. This history indicates a pattern that can be anticipated in the future, as it highlights the importance of careful planning and evaluation. It also suggests that future decisions will be influenced by similar considerations, reinforcing the value of consistent evaluation and preparation.',\n",
       " \"Is it possible that you've made a mistake in your goals? Or is it possible that you've made a mistake in your approach to achieving them?\\n\\nThe options are:\\nA) Lack of discipline and commitment\\nB) Mistake in goals\\nC) Mistake in approach\\nD) All of the above\\nE) None of the above\\n</think>\\n\\nThe lack of progress on personal goals does not necessarily indicate a mistake in the goals themselves. It could also reflect a lack of commitment or discipline in achieving them. Therefore, the correct answer is:\\n\\n**D) All of the above**\\n\\nThis means that it's possible that there was a mistake in the goals, a lack of commitment, or an approach that needs improvement.\",\n",
       " \"Also, what are the ways to mitigate these risks? What are the ways to ensure that the presentation is delivered effectively? What are the ways to ensure that the presentation is delivered in a way that is engaging and effective? What are the ways to ensure that the presentation is delivered in a way that is effective and efficient? What are the ways to ensure that the presentation is delivered in a way that is effective and engaging?\\n\\nWait, the user is asking about all the ways this could go wrong, and the consequences if it does. Then, they're asking about mitigation, effectiveness, engagement, and efficiency. So, the user is looking for a comprehensive list of potential issues, their repercussions, and solutions for each category.\\n\\nI need to make sure I cover all the possible areas where a presentation could fail. Maybe I should start by thinking about the structure of the presentation. If it's too long or too short, that could cause issues. Then, content delivery: if the slides are unclear or the speaker isn't prepared, that could lead to confusion. Time management is another area; if the presentation is rushed, it might not be well-organized.\\n\\nI should also consider the audience's engagement. If the audience isn't paying attention, the presentation might not be effective. Maybe the tone is too formal or too casual, which could affect the audience's perception. Also, technical issues like poor equipment or internet connectivity could disrupt the presentation.\\n\\nAnother aspect is the delivery method. If the presentation is recorded, the audio might be lost, or if it's live, the audience might not be in the right place. The timing could also be off, leading to a rushed or unprofessional delivery.\\n\\nI should also think about the audience's needs. If the audience is not familiar with the topic, the presentation might not be helpful. The audience's background could affect how well the speaker explains the material. The purpose of the presentation could influence the tone and content.\\n\\nI need to make sure I cover all these areas: structure, content, delivery, audience engagement, technical issues, delivery method, audience needs, and purpose. For each of these, I should list possible ways it could go wrong, their consequences, and solutions.\\n\\nI should also consider the consequences of each issue. For example, if the presentation is too long, it might confuse the audience. If it's too short, it might be boring. If the slides are unclear, the audience might not understand the key points. If the speaker is unprepared, the audience might not be engaged or informed.\\n\\nFor mitigation, I should think of ways to address each issue. For example, if the presentation is too long, the speaker could break it into shorter segments. If the slides are unclear, the speaker could prepare them in advance or use visual aids to clarify points. If the audience isn't paying attention, the speaker could use a more engaging tone or include more examples.\\n\\nFor effectiveness, I should think about how to make the presentation clear and impactful. If the audience isn't engaged, the presentation might not be effective. If the audience is engaged, the presentation might be effective. If the audience is not informed, the presentation might not be effective.\\n\\nFor engagement, I should think about how to make the presentation interesting and relevant. If the audience isn't interested, the presentation might not be effective. If the audience is interested, the presentation might be effective. If the audience isn't informed, the presentation might not be effective.\\n\\nFor efficiency, I should think about how to make the presentation clear and concise. If the audience isn't paying attention, the presentation might not be effective. If the audience is paying attention, the presentation might be efficient.\\n\\nFor effectiveness and engagement, I should think about how to make the presentation both clear and engaging. If the audience isn't paying attention, the presentation might not be effective. If the audience is paying attention and engaged, the presentation might be effective and engaging.\\n\\nI should also consider the consequences of each issue. For example, if the presentation is too long, it might confuse the audience. If it's too short, it might be boring. If the slides are unclear, the audience might not understand the key points. If the speaker is unprepared, the audience might not be engaged or informed.\\n\\nI should also think about the audience's background. If the audience is not familiar with the topic, the presentation might not be helpful. The audience's background could affect how well the speaker explains the material. The purpose of the presentation could influence the tone and content.\\n\\nI should also consider the purpose of the presentation. If the purpose is to inform, the presentation should be clear and concise. If the purpose is to engage, the presentation should be interesting and relevant. If the purpose is to persuade, the presentation should be persuasive and logical.\\n\\nI should also think about the delivery method. If the presentation is recorded, the audio might be lost, or if it's live, the audience might\",\n",
       " \"What is the best way to address this situation?\\n</think>\\n\\nIf your child is 30 minutes late coming home from school without any communication, it's important to address this situation promptly and effectively. Here are the possible scenarios and their implications, along with the best way to address the situation:\\n\\n### Possible Scenarios:\\n1. **Communication Gaps**: There may be a lack of clear or timely communication between you and your child about the schedule or the reasons for the delay.\\n2. **External Factors**: External events (e.g., illness, family issues) could have caused the delay.\\n3. **Personal Issues**: There might be personal reasons for the delay, such as family responsibilities or personal health issues.\\n4. **School Issues**: The delay could be due to school-related issues, such as absences, emergencies, or other school-related problems.\\n5. **Communication Attempts**: There may have been attempts to communicate, but they were unsuccessful.\\n\\n### Implications:\\n- **Impact on Child's Learning**: A delay in school could affect their ability to attend classes and complete assignments.\\n- **Impact on Family**: A late return could disrupt family schedules and cause stress.\\n- **Impact on School**: A delay could affect the school's ability to meet its commitments and may lead to disciplinary actions.\\n- **Impact on Personal Life**: A late return could cause personal stress and affect the child's well-being.\\n\\n### Best Ways to Address the Situation:\\n1. **Communicate Clearly**: Attempt to communicate the reason for the delay and any steps being taken to resolve it. Use a calm and professional tone.\\n2. **Provide a Reason**: Offer a clear explanation for the delay, such as a medical emergency, family emergency, or personal reason.\\n3. **Set a Bound on Time**: If possible, set a specific time limit for the child to return home. This helps maintain a consistent schedule.\\n4. **Monitor Progress**: Keep an eye on the child's progress and ensure they are not delayed in other areas of their day.\\n5. **Seek Immediate Help**: If the delay persists or if there are concerns, seek immediate help from the school or child's teacher.\\n6. **Consider Alternative Solutions**: If the delay is due to external factors, consider alternative solutions, such as scheduling a different time for the child to return home.\\n\\nBy addressing the situation promptly and effectively, you can ensure the child's well-being and the school's ability to meet its commitments.\",\n",
       " 'What are the ways to mitigate each potential rejection?\\n\\nAssume that the project is about implementing a new algorithm for a machine learning model, and the algorithm is designed to handle a specific type of data. The data is labeled, but the labels are noisy. The algorithm is supposed to learn from the data and make accurate predictions. The project is supposed to be evaluated based on the accuracy of the predictions, and the team is responsible for implementing the algorithm.\\n\\nThe project is supposed to be implemented in Python, but the team is not familiar with Python, so they need to learn it before starting the implementation. The team is also responsible for debugging the code, which is a significant part of the project. The team is expected to work in a team environment, but the project is supposed to be done in a single environment, which could lead to issues with resource management and synchronization.\\n\\nThe team is responsible for ensuring that the algorithm is robust against adversarial attacks, which is a critical aspect of machine learning. The algorithm is supposed to be able to handle noisy data, which is already a part of the project. The team is also responsible for maintaining the algorithm, which could involve retraining it periodically with new data. The team is expected to work in a distributed environment, which could lead to issues with communication and synchronization between different parts of the system.\\n\\nThe team is responsible for ensuring that the algorithm is efficient, which is important for performance. The algorithm is supposed to handle a large number of data points, which could be computationally intensive. The team is also responsible for ensuring that the algorithm is scalable, which is important for future applications. The team is expected to work in a distributed environment, which could lead to issues with communication and synchronization between different parts of the system.\\n\\nThe team is responsible for ensuring that the algorithm is interpretable, which is important for understanding the decision-making process. The algorithm is supposed to handle a large number of data points, which could be computationally intensive. The team is also responsible for maintaining the algorithm, which could involve retraining it periodically with new data. The team is expected to work in a distributed environment, which could lead to issues with communication and synchronization between different parts of the system.\\n\\nThe team is responsible for ensuring that the algorithm is secure, which is important for protecting sensitive data. The algorithm is supposed to handle a large number of data points, which could be computationally intensive. The team is also responsible for maintaining the algorithm, which could involve retraining it periodically with new data. The team is expected to work in a distributed environment, which could lead to issues with communication and synchronization between different parts of the system.\\n\\nThe team is responsible for ensuring that the algorithm is efficient, which is important for performance. The algorithm is supposed to handle a large number of data points, which could be computationally intensive. The team is also responsible for ensuring that the algorithm is scalable, which is important for future applications. The team is expected to work in a distributed environment, which could lead to issues with communication and synchronization between different parts of the system.\\n\\nThe team is responsible for ensuring that the algorithm is interpretable, which is important for understanding the decision-making process. The algorithm is supposed to handle a large number of data points, which could be computationally intensive. The team is also responsible for maintaining the algorithm, which could involve retraining it periodically with new data. The team is expected to work in a distributed environment, which could lead to issues with communication and synchronization between different parts of the system.\\n\\nThe team is responsible for ensuring that the algorithm is secure, which is important for protecting sensitive data. The algorithm is supposed to handle a large number of data points, which could be computationally intensive. The team is also responsible for maintaining the algorithm, which could involve retraining it periodically with new data. The team is expected to work in a distributed environment, which could lead to issues with communication and synchronization between different parts of the system.\\n\\nThe team is responsible for ensuring that the algorithm is efficient, which is important for performance. The algorithm is supposed to handle a large number of data points, which could be computationally intensive. The team is also responsible for ensuring that the algorithm is scalable, which is important for future applications. The team is expected to work in a distributed environment, which could lead to issues with communication and synchronization between different parts of the system.\\n\\nThe team is responsible for ensuring that the algorithm is interpretable, which is important for understanding the decision-making process. The algorithm is supposed to handle a large number of data points, which could be computationally intensive. The team is also responsible for maintaining the algorithm, which could involve retraining it periodically with new data. The team is expected to work in a distributed environment, which could lead to issues with communication and synchronization between different parts of the system.\\n\\nThe team is responsible for ensuring that the algorithm is secure, which is important for protecting sensitive data. The algorithm is supposed to handle a large',\n",
       " \"You need to determine whether the engine is in a state of failure, and if so, what is the most likely failure, and what is the most likely financial impact.\\n\\nThe engine is a 3D printer engine, which is a cylindrical engine with a radius of 1 unit and a height of 1 unit. The engine is running at a speed of 1 unit per second. The engine is in a state of failure if any of the following conditions are met:\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\nWait, that seems repetitive. Maybe I misread the conditions. Let me check again.\\n\\nWait, perhaps the conditions are:\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\nWait, that's still repeating. Maybe the conditions are:\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\nBut that's still four conditions, but the user wrote six. Maybe I'm misunderstanding.\\n\\nWait, perhaps the conditions are:\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\nBut that's still four conditions, but the user wrote six. Maybe I'm miscounting.\\n\\nWait, perhaps the conditions are:\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\nBut that's four conditions, but the user wrote six. Maybe I'm miscounting.\\n\\nWait, perhaps the conditions are:\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\nBut that's four conditions, but the user wrote six. Maybe I'm miscounting.\\n\\nWait, perhaps the conditions are:\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\nBut that's four conditions, but the user wrote six. Maybe I'm miscounting.\\n\\nWait, perhaps the conditions are:\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\nBut that's four conditions, but the user wrote six. Maybe I'm miscounting.\\n\\nWait, perhaps the conditions are:\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\nBut that's four conditions, but the user wrote six. Maybe I'm miscounting.\\n\\nWait, perhaps the conditions are:\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\nBut that's four conditions, but the user wrote six. Maybe I'm miscounting.\\n\\nWait, perhaps the conditions are:\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\n- The engine's speed is less than 1 unit per second.\\n\\n- The engine's speed is greater than 1 unit per second.\\n\\nBut that's four conditions, but the user wrote six. Maybe I'm mis\",\n",
       " 'Could this be a sign of something more serious, like a relationship issue or something else? What steps can you take to address this? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue? What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address this issue. What would be the best way to communicate with your partner to address',\n",
       " \"Additionally, what are the steps you would take to address each condition, and what would be the best way to communicate with your doctor about these conditions?\\nOkay, so I have this medical appointment next week, and I'm really worried because I have some concerning symptoms. I need to figure out what these could be, how they might affect my life, and then think about the steps I should take and how to communicate with my doctor. Hmm, where do I start?\\n\\nFirst, I should probably list out the possible conditions that could be causing these symptoms. I remember hearing about different types of illnesses, like infections, chronic illnesses, or maybe something more serious like diabetes or hypertension. Let me think about each one.\\n\\nInfections come to mind first. The common cold is a big one, but there are also other viral infections like the flu or even some bacterial ones. If I have a cold, it's probably a mild illness, but it can affect my sleep, make me feel tired, and maybe even cause me to feel unwell. I should consider that if I have a lot of symptoms, it might be something more serious, like the flu or maybe something like pneumonia.\\n\\nThen there's the flu. That's a respiratory illness, so it's probably worse than a cold. It can make me feel really sick, especially if I'm not washing my hands often. It can also lead to more severe symptoms like fever, cough, and maybe even dehydration. I should think about whether I have any other symptoms besides the flu, like a high fever or a runny nose.\\n\\nChronic illnesses are another possibility. I remember hearing about diabetes, where your blood sugar levels are too high. That can cause fatigue, weight gain, and maybe even high blood pressure. There's also something called hypertension, where your blood pressure is too high, which can lead to heart problems and maybe even stroke if it's not treated properly. I should consider if I have any other symptoms related to these, like fatigue or weight changes.\\n\\nThen there's something called anemia, which is when your blood cells are too low. This can cause fatigue, shortness of breath, and maybe even low blood pressure. I should think about whether I'm experiencing any of these symptoms or if they're just mild.\\n\\nI also remember hearing about something called a gastrointestinal issue, like irritable bowel syndrome (IBS). This can cause symptoms like bloating, gas, and pain. It can also lead to constipation and weight loss. I should consider if I have any of these symptoms or if they're just mild.\\n\\nAnother thing is something called a viral infection, which I think is similar to the common cold. It can cause a lot of symptoms, like a runny nose, cough, and maybe even a fever. I should think about whether I have any of these symptoms or if they're just mild.\\n\\nThen there's something called a chronic kidney disease, which is a condition where your kidneys aren't working properly. This can cause fatigue, weight loss, and maybe even kidney stones. I should consider if I have any of these symptoms or if they're just mild.\\n\\nI also remember hearing about something called a chronic respiratory infection, which is similar to the common cold. It can cause a lot of symptoms, like a runny nose, cough, and maybe even a fever. I should think about whether I have any of these symptoms or if they're just mild.\\n\\nAnother possibility is something called a viral infection, which I think is similar to the common cold. It can cause a lot of symptoms, like a runny nose, cough, and maybe even a fever. I should consider if I have any of these symptoms or if they're just mild.\\n\\nI also think about something called a chronic gastrointestinal disorder, which can cause symptoms like bloating, gas, and pain. It can also lead to constipation and weight loss. I should consider if I have any of these symptoms or if they're just mild.\\n\\nThen there's something called a chronic kidney disease, which is similar to the chronic kidney disease I mentioned earlier. It can cause fatigue, weight loss, and maybe even kidney stones. I should think about if I have any of these symptoms or if they're just mild.\\n\\nI also remember hearing about something called a chronic respiratory infection, which is similar to the common cold. It can cause a lot of symptoms, like a runny nose, cough, and maybe even a fever. I should consider if I have any of these symptoms or if they're just mild.\\n\\nAnother possibility is something called a viral infection, which I think is similar to the common cold. It can cause a lot of symptoms, like a runny nose, cough, and maybe even a fever. I should think about if I have any of these symptoms or if they're just mild.\\n\\nI also think about something called a chronic gastrointestinal disorder, which can cause symptoms like bloating, gas, and pain. It\",\n",
       " \"I need to list all the possible ways, not just the most obvious ones.\\nOkay, so I'm trying to figure out all the possible ways my career change decision could backfire and leave me in a worse position. I need to list all the possible ways, not just the obvious ones. Hmm, let me think.\\n\\nFirst, I should consider what a career change entails. It's about shifting focus from one job to another, right? So, if I decide to change jobs, I might be moving from one industry to another. But what if that industry is no longer relevant or the company is changing its strategy? That could be a backfire.\\n\\nAnother thing is the skills I bring with me. If I'm changing jobs, I might lose some skills that are important for the new role. For example, if I was a marketing professional and now I'm in a technical role, I might forget some marketing tactics. That could lead to poor performance or even job losses.\\n\\nNetworking is another factor. If I'm changing jobs, I might not have the same connections as before. Maybe I'm not as active in LinkedIn or other professional networks. That could make it harder to find new opportunities or to maintain my current position.\\n\\nTime management is something I need to think about. If I'm changing jobs, I might have to juggle multiple responsibilities. For instance, if I was a project manager and now I'm a team lead, I might have to take on more responsibilities without enough time to handle them effectively. This could lead to burnout or decreased productivity.\\n\\nAdaptability is crucial. If I'm changing jobs, I might not be as prepared to adapt to new challenges. If the company changes its policies or the industry shifts, I might not be ready to handle it. This could lead to inefficiencies or even job losses.\\n\\nEmotional adjustment is important too. If I'm changing jobs, especially if it's a significant change, I might feel uncertain or overwhelmed. This could lead to stress, anxiety, or even burnout, which could negatively impact my overall well-being and career.\\n\\nFinancial stability is another consideration. If I'm changing jobs, especially if it's a long-term commitment, I might have to invest in new skills or training. This could take a significant chunk of my budget, leaving me with less money to spend on other things. This could lead to financial instability or even financial loss.\\n\\nEmotional compatibility is something I should also think about. If I'm changing jobs, I might have to deal with new people or work in a different environment. If I'm not emotionally compatible with them, it could lead to conflicts or even a loss of trust, which could hurt my career.\\n\\nLastly, career growth and development. If I'm changing jobs, I might not have the same opportunities to grow my career as before. If I'm moving to a different industry, there might not be as many career advancement opportunities as before. This could leave me feeling stuck or unchallenged, which could lead to dissatisfaction and a negative career trajectory.\\n\\nWait, but I need to make sure I'm not missing any other possible ways. Maybe I should consider the impact on my personal life or relationships. If I'm changing jobs, I might have to deal with new responsibilities that could affect my personal relationships or family life. This could lead to stress, anxiety, or even a breakdown in my personal life, which could negatively impact my career.\\n\\nAlso, if I'm changing jobs, I might have to deal with the uncertainty of the new role. There could be a lot of uncertainty about what the new role entails, what the company is looking for, and how I'll be evaluated. This uncertainty could lead to anxiety, stress, or even a lack of confidence, which could negatively impact my career.\\n\\nAnother thought is about the long-term implications of the career change. If I'm changing jobs, especially if it's a significant one, I might have to consider the long-term effects on my career trajectory. If I'm not prepared, it could lead to a career dip or even a loss of confidence in my future career path.\\n\\nAlso, I should think about the potential for burnout. If I'm changing jobs, especially if it's a long-term commitment, I might be forced to work longer hours or take on more responsibilities without the necessary time to rest and recharge. This could lead to burnout, which is a serious issue that can negatively impact my career and well-being.\\n\\nIn summary, the possible ways my career change decision could backfire and leave me in a worse position include:\\n\\n1. Moving to an industry that's no longer relevant or the company is changing its strategy.\\n2. Losing essential skills that are important for the new role.\\n3. Not having the same network connections as before.\\n4. Juggling multiple responsibilities without sufficient time management.\\n5. Not being prepared to adapt to new challenges or industry shifts.\\n6. Feeling uncertain or overwhelmed due\",\n",
       " \"Are there any underlying issues that might be impacting your performance? Or is there a way to address them?\\n\\nAdditionally, you mentioned that you're a bit nervous about the upcoming interview. How would you handle that anxiety? What specific steps would you take to prepare for it?\\n\\nLastly, you mentioned that you're a bit short on time. How would you manage your time during the interview? What specific strategies would you use to ensure you don't miss any crucial points?\\n\\nOverall, I want to understand how these positive and negative signals can be leveraged to improve my performance in future job interviews. I also want to know if there are any underlying issues that might be affecting my performance, and how I can address them.\\n</think>\\n\\n**Job Interview Performance Analysis and Strategies**\\n\\n1. **Positive Signals:**\\n   - **Positive Feedback:** The interviewer expressed enthusiasm, indicating a favorable impression of your qualifications.\\n   - **Confidence Showers:** Your confidence during the interview was evident, suggesting a positive mindset.\\n   - **Reassurance:** The positive tone and lack of concerns indicate a supportive environment.\\n\\n2. **Underlying Issues:**\\n   - **Time Management:** If you're short on time, it's crucial to prioritize tasks and allocate specific minutes to each section.\\n   - **Preparation:** Reviewing materials and practicing mock interviews can help manage expectations and reduce anxiety.\\n   - **Stress Management:** Techniques like mindfulness or meditation can help mitigate anxiety during interviews.\\n\\n3. **Handling Anxiety:**\\n   - **Preparation:** Thoroughly prepare by practicing and understanding the interview format.\\n   - **Positive Self-Talk:** Reminding yourself of past successes can boost confidence.\\n\\n4. **Time Management:**\\n   - **Prioritization:** Allocate specific time slots for each part of the interview to stay on track.\\n   - **Active Listening:** Pay close attention to the interviewer's cues and maintain a clear focus.\\n\\n5. **Improvement Strategies:**\\n   - **Self-Reflection:** Identify specific areas where you can improve based on feedback or self-assessment.\\n   - **Practice:** Engage in simulated interviews to build confidence and time management skills.\\n\\nIn conclusion, leveraging positive signals while addressing underlying issues and managing anxiety and time effectively can enhance future job interview performance.\",\n",
       " \"Is it possible that they are trying to manipulate you? Or is it possible that they are genuinely trying to help you?\\n\\nI need to write a response that addresses both possibilities: whether they are trying to manipulate you or genuinely trying to help. I should also consider the context of the situation, such as the nature of the matter, the friend's background, and the time frame. I should also think about the tone of the response, whether it's formal or casual, and whether it's empathetic or confrontational.\\n\\nI need to make sure that my response is clear, concise, and provides a balanced view of both possibilities. I should also be prepared to adjust my response based on the friend's response, but for now, I need to focus on the initial assessment.\\n\\nAdditionally, I should consider the possibility that the friend might be trying to trick me into giving advice that's not helpful, or that they might be trying to manipulate me into giving advice that's not appropriate.\\n\\nI should also think about the friend's perspective: if they are genuinely trying to help, I should respond in a way that shows I'm willing to assist, but if they are trying to manipulate, I should respond in a way that shows I'm cautious and hesitant.\\n\\nI need to make sure that my response is respectful and understanding, and that I don't assume anything about the friend's intentions.\\n\\nI should also think about the tone of the response: whether it's formal or casual, and whether it's empathetic or confrontational.\\n\\nI need to make sure that my response is clear, concise, and provides a balanced view of both possibilities.\\n\\nI should also be prepared to adjust my response based on the friend's response, but for now, I need to focus on the initial assessment.\\n\\nAdditionally, I should consider the context of the situation, such as the nature of the matter, the friend's background, and the time frame.\\n\\nI should also think about the tone of the response, whether it's formal or casual, and whether it's empathetic or confrontational.\\n\\nI need to make sure that my response is clear, concise, and provides a balanced view of both possibilities.\\n\\nI should also be prepared to adjust my response based on the friend's response, but for now, I need to focus on the initial assessment.\\n\\nAdditionally, I should consider the context of the situation, such as the nature of the matter, the friend's background, and the time frame.\\n\\nI should also think about the tone of the response, whether it's formal or casual, and whether it's empathetic or confrontational.\\n\\nI need to make sure that my response is clear, concise, and provides a balanced view of both possibilities.\\n\\nI should also be prepared to adjust my response based on the friend's response, but for now, I need to focus on the initial assessment.\\n\\nAdditionally, I should consider the context of the situation, such as the nature of the matter, the friend's background, and the time frame.\\n\\nI should also think about the tone of the response, whether it's formal or casual, and whether it's empathetic or confrontational.\\n\\nI need to make sure that my response is clear, concise, and provides a balanced view of both possibilities.\\n\\nI should also be prepared to adjust my response based on the friend's response, but for now, I need to focus on the initial assessment.\\n\\nAdditionally, I should consider the context of the situation, such as the nature of the matter, the friend's background, and the time frame.\\n\\nI should also think about the tone of the response, whether it's formal or casual, and whether it's empathetic or confrontational.\\n\\nI need to make sure that my response is clear, concise, and provides a balanced view of both possibilities.\\n\\nI should also be prepared to adjust my response based on the friend's response, but for now, I need to focus on the initial assessment.\\n\\nAdditionally, I should consider the context of the situation, such as the nature of the matter, the friend's background, and the time frame.\\n\\nI should also think about the tone of the response, whether it's formal or casual, and whether it's empathetic or confrontational.\\n\\nI need to make sure that my response is clear, concise, and provides a balanced view of both possibilities.\\n\\nI should also be prepared to adjust my response based on the friend's response, but for now, I need to focus on the initial assessment.\\n\\nAdditionally, I should consider the context of the situation, such as the nature of the matter, the friend's background, and the time frame.\\n\\nI should also think about the tone of the response, whether it's formal or casual, and whether it's empathetic or confrontational.\\n\\nI need to make sure that my response is clear, concise, and provides a balanced view of both possibilities.\\n\\nI should also be prepared to adjust my response based on the friend's response, but for now, I need to focus on the\",\n",
       " \"Is there a specific reason why your team is performing better than others in your industry? Or is it a result of the company's overall performance? Or perhaps a combination of both?\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry, or if it's due to the company's overall performance. I've also considered the possibility that it's a combination of both factors.\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry, or if it's due to the company's overall performance. I've also considered the possibility that it's a combination of both factors.\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry, or if it's due to the company's overall performance. I've also considered the possibility that it's a combination of both factors.\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry, or if it's due to the company's overall performance. I've also considered the possibility that it's a combination of both factors.\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry, or if it's due to the company's overall performance. I've also considered the possibility that it's a combination of both factors.\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry, or if it's due to the company's overall performance. I've also considered the possibility that it's a combination of both factors.\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry, or if it's due to the company's overall performance. I've also considered the possibility that it's a combination of both factors.\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry, or if it's due to the company's overall performance. I've also considered the possibility that it's a combination of both factors.\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry, or if it's due to the company's overall performance. I've also considered the possibility that it's a combination of both factors.\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry, or if it's due to the company's overall performance. I've also considered the possibility that it's a combination of both factors.\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry, or if it's due to the company's overall performance. I've also considered the possibility that it's a combination of both factors.\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry, or if it's due to the company's overall performance. I've also considered the possibility that it's a combination of both factors.\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry, or if it's due to the company's overall performance. I've also considered the possibility that it's a combination of both factors.\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry, or if it's due to the company's overall performance. I've also considered the possibility that it's a combination of both factors.\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry, or if it's due to the company's overall performance. I've also considered the possibility that it's a combination of both factors.\\n\\nI've been working on a project that requires me to analyze the performance of my team. I need to determine whether my team's performance is better than others in my industry\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Swn2l4ODxsCR",
    "outputId": "6b6f45dd-38f5-4bf4-ca39-ab36c4df46a2"
   },
   "outputs": [],
   "source": [
    "# FINAL FIX: Collect activations for ALL labels found in annotations\n",
    "from utils import get_label_positions\n",
    "\n",
    "def train_emotional_vectors_final(responses, annotations, model, tokenizer):\n",
    "    \"\"\"Train steering vectors for emotional reasoning categories - FINAL FIXED VERSION\"\"\"\n",
    "\n",
    "    print(\"ðŸ§  Extracting neural activations...\")\n",
    "    batch_activations = process_saved_responses_batch(responses, tokenizer, model)\n",
    "\n",
    "    # First pass: discover all labels in the annotations\n",
    "    all_labels = set()\n",
    "    label_positions_all = {}\n",
    "    \n",
    "    for i, annotation in enumerate(annotations):\n",
    "        # Use regex to find all labels in the annotation\n",
    "        import re\n",
    "        label_matches = re.findall(r'\\[\"([^\"]+)\"\\]', annotation)\n",
    "        all_labels.update(label_matches)\n",
    "        \n",
    "        # Get label positions for this response\n",
    "        try:\n",
    "            label_positions = get_label_positions(annotation, responses[i], tokenizer)\n",
    "            label_positions_all[i] = label_positions\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting label positions for response {i}: {e}\")\n",
    "            label_positions_all[i] = {}\n",
    "\n",
    "    # Save label positions\n",
    "    label_positions_file = f\"{CONFIG['results_dir']}/data/label_positions_{CONFIG['timestamp']}.json\"\n",
    "    with open(label_positions_file, \"w\") as f:\n",
    "        json.dump(label_positions_all, f, indent=2)\n",
    "    print(f\"ðŸ’¾ Saved label positions to {label_positions_file}\")\n",
    "\n",
    "    # Remove 'end-section' as it's not a content label\n",
    "    all_labels.discard('end-section')\n",
    "\n",
    "    # Filter to emotional labels (our target labels + any others that seem emotional)\n",
    "    target_emotional_labels = [\"depressive-thinking\", \"anxious-thinking\", \"negative-attribution\", \"pessimistic-projection\"]\n",
    "    emotional_keywords = [\"thinking\", \"attribution\", \"projection\", \"anxious\", \"depressive\", \"negative\", \"pessimistic\"]\n",
    "\n",
    "    emotional_labels = []\n",
    "    for label in all_labels:\n",
    "        if label in target_emotional_labels or any(keyword in label for keyword in emotional_keywords):\n",
    "            emotional_labels.append(label)\n",
    "\n",
    "    print(f\"ðŸŽ¯ All labels found: {sorted(list(all_labels))}\")\n",
    "    print(f\"ðŸŽ¯ Emotional labels identified: {emotional_labels}\")\n",
    "\n",
    "    # Initialize storage for activations by label\n",
    "    label_activations = {label: [] for label in emotional_labels}\n",
    "    label_activations[\"overall\"] = []\n",
    "\n",
    "    print(\"ðŸ·ï¸  Processing annotations and extracting labeled activations...\")\n",
    "\n",
    "    for i, (response, annotation) in enumerate(zip(responses, annotations)):\n",
    "        try:\n",
    "            # Get label positions in the response\n",
    "            label_positions = label_positions_all.get(i, {})\n",
    "\n",
    "            if i < 3:  # Debug first few\n",
    "                found_emotional = [label for label in label_positions.keys() if label in emotional_labels]\n",
    "                print(f\"   Response {i}: Found emotional labels: {found_emotional}\")\n",
    "\n",
    "            # Get activations for this response\n",
    "            activations = batch_activations[i]  # Shape varies - needs to be standardized\n",
    "            \n",
    "            # CRITICAL FIX: Check activation tensor shape and ensure consistency\n",
    "            print(f\"   Response {i} activation shape: {activations.shape}\")\n",
    "            \n",
    "            # Skip if activations shape is inconsistent\n",
    "            if len(activations.shape) != 2:\n",
    "                print(f\"   âš ï¸ Skipping response {i}: Unexpected activation shape {activations.shape}\")\n",
    "                continue\n",
    "                \n",
    "            num_layers, hidden_size = activations.shape\n",
    "            \n",
    "            # Ensure we have the expected number of layers\n",
    "            expected_layers = model.config.num_hidden_layers\n",
    "            if num_layers != expected_layers:\n",
    "                print(f\"   âš ï¸ Skipping response {i}: Expected {expected_layers} layers, got {num_layers}\")\n",
    "                continue\n",
    "                \n",
    "            # Ensure hidden size is consistent\n",
    "            expected_hidden = model.config.hidden_size\n",
    "            if hidden_size != expected_hidden:\n",
    "                print(f\"   âš ï¸ Skipping response {i}: Expected hidden size {expected_hidden}, got {hidden_size}\")\n",
    "                continue\n",
    "            \n",
    "            # Store activations for overall mean\n",
    "            label_activations[\"overall\"].append(activations)\n",
    "\n",
    "            # Extract activations for each emotional label\n",
    "            for label, positions in label_positions.items():\n",
    "                if label in label_activations:  # Only collect if it's an emotional label\n",
    "                    # For simplicity, use the overall activation for each labeled segment\n",
    "                    # In a more sophisticated implementation, you'd extract segment-specific activations\n",
    "                    label_activations[label].append(activations)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing response {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Compute mean vectors for each label\n",
    "    print(\"ðŸ“Š Computing mean vectors...\")\n",
    "    mean_vectors = {}\n",
    "\n",
    "    for label, activations_list in label_activations.items():\n",
    "        if activations_list:\n",
    "            try:\n",
    "                activations_tensor = torch.stack(activations_list)  # Shape: (num_activations, layers, hidden_size)\n",
    "                mean_vector = activations_tensor.mean(dim=0)  # Shape: (layers, hidden_size)\n",
    "\n",
    "                mean_vectors[label] = {\n",
    "                    'mean': mean_vector,\n",
    "                    'count': len(activations_list)\n",
    "                }\n",
    "\n",
    "                print(f\"   {label}: {len(activations_list)} activations, mean shape: {mean_vector.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Error computing mean for {label}: {e}\")\n",
    "        else:\n",
    "            print(f\"   {label}: No activations found\")\n",
    "\n",
    "    return mean_vectors\n",
    "\n",
    "# Use the final version\n",
    "print(\"ðŸŽ¯ Training emotional steering vectors with FIXED function...\")\n",
    "emotional_mean_vectors = train_emotional_vectors_final(\n",
    "    response_texts, annotated_responses, model, tokenizer\n",
    ")\n",
    "\n",
    "print(f\"âœ… Trained vectors for {len(emotional_mean_vectors)} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNOZzgEjxsCR"
   },
   "source": [
    "## Step 6: Compute Feature Vectors and Combine with Existing Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVfnFOnPxsCR",
    "outputId": "1254bf88-6f4b-49b4-ada3-3b89b7b73548"
   },
   "outputs": [],
   "source": [
    "def compute_emotional_feature_vectors(mean_vectors_dict, model):\n",
    "    \"\"\"Compute feature vectors by subtracting overall mean from emotional category means - FIXED VERSION\"\"\"\n",
    "\n",
    "    if \"overall\" not in mean_vectors_dict:\n",
    "        print(\"âš ï¸  No overall mean found - cannot compute feature vectors\")\n",
    "        return {}\n",
    "\n",
    "    feature_vectors = {}\n",
    "    overall_mean = mean_vectors_dict[\"overall\"][\"mean\"]\n",
    "\n",
    "    print(f\"ðŸ“Š Overall mean shape: {overall_mean.shape}\")\n",
    "\n",
    "    # Get the expected dimensions\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    hidden_size = model.config.hidden_size\n",
    "\n",
    "    # FIXED: Validate overall mean dimensions - it should be (layers, hidden_size)\n",
    "    if overall_mean.shape != (num_layers, hidden_size):\n",
    "        print(f\"âš ï¸  Overall mean shape {overall_mean.shape} doesn't match expected ({num_layers}, {hidden_size})\")\n",
    "        \n",
    "        # Try to fix the shape if it's just missing one dimension\n",
    "        if len(overall_mean.shape) == 1 and overall_mean.shape[0] == num_layers:\n",
    "            print(\"ðŸ”§ Attempting to fix overall mean shape by expanding hidden dimension...\")\n",
    "            # This suggests the mean was computed incorrectly - we need proper 2D tensors\n",
    "            print(\"âŒ Cannot fix - need proper 2D activation tensors from training\")\n",
    "            return {}\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    # Set overall mean as baseline\n",
    "    feature_vectors[\"overall\"] = overall_mean\n",
    "\n",
    "    # Compute differential vectors for emotional categories\n",
    "    emotional_labels = [\"depressive-thinking\", \"anxious-thinking\", \"negative-attribution\", \"pessimistic-projection\"]\n",
    "\n",
    "    emotional_vectors_found = 0\n",
    "    for label in emotional_labels:\n",
    "        if label in mean_vectors_dict:\n",
    "            label_mean = mean_vectors_dict[label][\"mean\"]\n",
    "\n",
    "            print(f\"ðŸ“Š {label} mean shape: {label_mean.shape}\")\n",
    "\n",
    "            # Validate label mean dimensions\n",
    "            if label_mean.shape != (num_layers, hidden_size):\n",
    "                print(f\"âš ï¸  {label} mean shape {label_mean.shape} doesn't match expected ({num_layers}, {hidden_size})\")\n",
    "                continue\n",
    "\n",
    "            # Compute differential feature vector\n",
    "            feature_vectors[label] = label_mean - overall_mean\n",
    "            emotional_vectors_found += 1\n",
    "            print(f\"âœ… Computed feature vector for {label}\")\n",
    "\n",
    "    # Also include any other emotional labels that were found\n",
    "    other_emotional_found = 0\n",
    "    for label in mean_vectors_dict.keys():\n",
    "        if label != \"overall\" and label not in emotional_labels:\n",
    "            # Check if it looks like an emotional label\n",
    "            emotional_keywords = [\"thinking\", \"attribution\", \"projection\", \"anxious\", \"depressive\", \"negative\", \"pessimistic\"]\n",
    "            if any(keyword in label for keyword in emotional_keywords):\n",
    "                label_mean = mean_vectors_dict[label][\"mean\"]\n",
    "                if label_mean.shape == (num_layers, hidden_size):\n",
    "                    feature_vectors[label] = label_mean - overall_mean\n",
    "                    other_emotional_found += 1\n",
    "                    print(f\"âœ… Computed feature vector for additional emotional label: {label}\")\n",
    "\n",
    "    print(f\"ðŸ“Š Summary: {emotional_vectors_found} target emotional vectors + {other_emotional_found} additional emotional vectors\")\n",
    "    \n",
    "    return feature_vectors\n",
    "\n",
    "# Compute emotional feature vectors\n",
    "print(\"ðŸ§® Computing emotional feature vectors...\")\n",
    "emotional_feature_vectors = compute_emotional_feature_vectors(emotional_mean_vectors, model)\n",
    "\n",
    "# Combine with existing cognitive vectors if available\n",
    "if existing_vectors and emotional_feature_vectors:\n",
    "    print(\"ðŸ”— Combining with existing cognitive vectors...\")\n",
    "    combined_vectors = {**existing_vectors, **emotional_feature_vectors}\n",
    "    print(f\"ðŸ“¦ Combined vector set: {list(combined_vectors.keys())}\")\n",
    "elif emotional_feature_vectors:\n",
    "    print(\"ðŸ“¦ Using emotional vectors only\")\n",
    "    combined_vectors = emotional_feature_vectors\n",
    "elif existing_vectors:\n",
    "    print(\"ðŸ“¦ Using existing cognitive vectors only\")\n",
    "    combined_vectors = existing_vectors\n",
    "else:\n",
    "    print(\"âš ï¸  No feature vectors available\")\n",
    "    combined_vectors = {}\n",
    "\n",
    "# Save the new vectors\n",
    "if emotional_mean_vectors:\n",
    "    model_id = CONFIG[\"model_name\"].split('/')[-1].lower()\n",
    "    torch.save(\n",
    "        emotional_mean_vectors,\n",
    "        f\"{CONFIG['results_dir']}/data/emotional_mean_vectors_{model_id}_{CONFIG['timestamp']}.pt\"\n",
    "    )\n",
    "    print(f\"ðŸ’¾ Saved emotional mean vectors\")\n",
    "\n",
    "# Save the combined feature vectors (steering vectors)\n",
    "if combined_vectors:\n",
    "    model_id = CONFIG[\"model_name\"].split('/')[-1].lower()\n",
    "    steering_vectors_file = f\"{CONFIG['results_dir']}/data/steering_vectors_{model_id}_{CONFIG['timestamp']}.pt\"\n",
    "    torch.save(combined_vectors, steering_vectors_file)\n",
    "    print(f\"ðŸ’¾ Saved steering vectors to {steering_vectors_file}\")\n",
    "    print(f\"ðŸŽ¯ Ready for emotional steering with {len(combined_vectors)} vector types: {list(combined_vectors.keys())}\")\n",
    "else:\n",
    "    print(\"âŒ No vectors available for steering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION: Load previously saved data instead of regenerating\n",
    "# Set these variables to load from existing files, or set to None to regenerate\n",
    "\n",
    "# Define file paths - modify these to point to your saved files\n",
    "LOAD_LABEL_POSITIONS_FILE = None  # e.g., \"./results/data/label_positions_20250727_123456.json\"\n",
    "LOAD_STEERING_VECTORS_FILE = None  # e.g., \"./results/data/steering_vectors_deepseek-r1-distill-qwen-1.5b_20250727_123456.pt\"\n",
    "\n",
    "# Load label positions if file is specified\n",
    "if LOAD_LABEL_POSITIONS_FILE and os.path.exists(LOAD_LABEL_POSITIONS_FILE):\n",
    "    print(f\"ðŸ“‚ Loading label positions from {LOAD_LABEL_POSITIONS_FILE}\")\n",
    "    with open(LOAD_LABEL_POSITIONS_FILE, \"r\") as f:\n",
    "        loaded_label_positions = json.load(f)\n",
    "    print(f\"âœ… Loaded label positions for {len(loaded_label_positions)} responses\")\n",
    "else:\n",
    "    print(\"ðŸ”„ Using newly generated label positions (or will generate if needed)\")\n",
    "    loaded_label_positions = None\n",
    "\n",
    "# Load steering vectors if file is specified  \n",
    "if LOAD_STEERING_VECTORS_FILE and os.path.exists(LOAD_STEERING_VECTORS_FILE):\n",
    "    print(f\"ðŸ“‚ Loading steering vectors from {LOAD_STEERING_VECTORS_FILE}\")\n",
    "    loaded_steering_vectors = torch.load(LOAD_STEERING_VECTORS_FILE, map_location='cpu')\n",
    "    print(f\"âœ… Loaded steering vectors: {list(loaded_steering_vectors.keys())}\")\n",
    "    \n",
    "    # Use loaded vectors as the combined_vectors for step 7\n",
    "    combined_vectors = loaded_steering_vectors\n",
    "    print(f\"ðŸŽ¯ Ready for step 7 with loaded vectors: {list(combined_vectors.keys())}\")\n",
    "else:\n",
    "    print(\"ðŸ”„ Using newly generated steering vectors (or will generate if needed)\")\n",
    "    # combined_vectors will be set from the previous cell's computation\n",
    "    \n",
    "print(f\"\\nðŸ“‹ Data loading summary:\")\n",
    "print(f\"   Label positions: {'Loaded' if loaded_label_positions else 'Generated/New'}\")\n",
    "print(f\"   Steering vectors: {'Loaded' if LOAD_STEERING_VECTORS_FILE and os.path.exists(LOAD_STEERING_VECTORS_FILE) else 'Generated/New'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tB0V7v_XxsCS"
   },
   "source": [
    "## Step 7: Test Emotional Steering\n",
    "\n",
    "Now we'll test the emotional steering capabilities by generating responses with different emotional steering settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaC5djAJxsCS"
   },
   "outputs": [],
   "source": [
    "def test_emotional_steering(model, tokenizer, feature_vectors, test_messages, steering_config):\n",
    "    \"\"\"Test emotional steering across different settings\"\"\"\n",
    "\n",
    "    if not feature_vectors:\n",
    "        print(\"âš ï¸  No feature vectors available for testing\")\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    emotional_labels = [\"depressive-thinking\", \"anxious-thinking\", \"negative-attribution\", \"pessimistic-projection\"]\n",
    "\n",
    "    # Also check for any emotional labels we actually found\n",
    "    available_emotional_labels = [label for label in feature_vectors.keys()\n",
    "                                 if label != \"overall\" and any(keyword in label\n",
    "                                 for keyword in [\"thinking\", \"attribution\", \"projection\", \"anxious\", \"depressive\", \"negative\", \"pessimistic\"])]\n",
    "\n",
    "    print(f\"ðŸŽ¯ Available emotional labels for testing: {available_emotional_labels}\")\n",
    "    print(f\"ðŸ“Š Total messages to process: {len(test_messages)}\")\n",
    "    print(f\"ðŸŽ›ï¸  Labels per message: {len(available_emotional_labels)}\")\n",
    "    print(f\"âš¡ Total operations: {len(test_messages) * len(available_emotional_labels) * 2} (pos + neg for each label)\")\n",
    "\n",
    "    for msg_idx, msg in enumerate(tqdm(test_messages, desc=\"ðŸ§ª Processing messages\", position=0)):\n",
    "        message_content = msg[\"content\"]\n",
    "        \n",
    "        print(f\"\\nðŸ“ Message {msg_idx+1}/{len(test_messages)}: {message_content[:50]}{'...' if len(message_content) > 50 else ''}\")\n",
    "\n",
    "        # Generate baseline (no steering)\n",
    "        try:\n",
    "            print(\"  ðŸŽ¯ Generating baseline response...\")\n",
    "            input_ids = tokenizer.encode(message_content, return_tensors=\"pt\")\n",
    "\n",
    "            with model.generate(\n",
    "                {\"input_ids\": input_ids, \"attention_mask\": (input_ids != tokenizer.pad_token_id).long()},\n",
    "                max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            ) as tracer:\n",
    "                baseline_output = model.generator.output.save()\n",
    "\n",
    "            baseline_text = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "            input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            if baseline_text.startswith(input_text):\n",
    "                baseline_text = baseline_text[len(input_text):].strip()\n",
    "\n",
    "            print(\"  ðŸ” Analyzing baseline emotional content...\")\n",
    "            baseline_analysis = analyze_emotional_content(baseline_text)\n",
    "            print(f\"  ðŸ“Š Baseline emotional score: {baseline_analysis.get('total_emotional_score', 0):.1f}%\")\n",
    "\n",
    "            result_entry = {\n",
    "                \"message\": message_content,\n",
    "                \"baseline_response\": baseline_text,\n",
    "                \"baseline_analysis\": baseline_analysis,\n",
    "                \"steered_responses\": {}\n",
    "            }\n",
    "\n",
    "            # Test steering for each available emotional label\n",
    "            for label_idx, label in enumerate(tqdm(available_emotional_labels, \n",
    "                                                 desc=f\"  ðŸŽ›ï¸  Testing labels\", \n",
    "                                                 position=1, \n",
    "                                                 leave=False)):\n",
    "                \n",
    "                print(f\"    ðŸ”„ [{label_idx+1}/{len(available_emotional_labels)}] Processing: {label}\")\n",
    "                \n",
    "                # Check if this label has steering config\n",
    "                model_name = CONFIG[\"model_name\"]\n",
    "                if model_name in steering_config and label in steering_config[model_name]:\n",
    "\n",
    "                    # Test positive steering (enhance emotional pattern)\n",
    "                    with tqdm(total=1, desc=f\"      âž• Positive {label}\", position=2, leave=False) as pbar:\n",
    "                        try:\n",
    "                            print(f\"      âž• Generating positive steering for {label}...\")\n",
    "                            pos_result = generate_and_analyze_emotional(\n",
    "                                model, tokenizer, message_content,\n",
    "                                feature_vectors, steering_config,\n",
    "                                label, \"positive\", CONFIG[\"max_new_tokens\"]\n",
    "                            )\n",
    "\n",
    "                            result_entry[\"steered_responses\"][f\"{label}_positive\"] = pos_result\n",
    "                            pos_score = pos_result.get('emotional_analysis', {}).get('total_emotional_score', 0)\n",
    "                            print(f\"      âœ… Positive {label} completed (score: {pos_score:.1f}%)\")\n",
    "                            pbar.update(1)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"      âŒ Error in positive steering for {label}: {e}\")\n",
    "                            pbar.update(1)\n",
    "\n",
    "                    # Test negative steering (suppress emotional pattern)\n",
    "                    with tqdm(total=1, desc=f\"      âž– Negative {label}\", position=2, leave=False) as pbar:\n",
    "                        try:\n",
    "                            print(f\"      âž– Generating negative steering for {label}...\")\n",
    "                            neg_result = generate_and_analyze_emotional(\n",
    "                                model, tokenizer, message_content,\n",
    "                                feature_vectors, steering_config,\n",
    "                                label, \"negative\", CONFIG[\"max_new_tokens\"]\n",
    "                            )\n",
    "\n",
    "                            result_entry[\"steered_responses\"][f\"{label}_negative\"] = neg_result\n",
    "                            neg_score = neg_result.get('emotional_analysis', {}).get('total_emotional_score', 0)\n",
    "                            print(f\"      âœ… Negative {label} completed (score: {neg_score:.1f}%)\")\n",
    "                            pbar.update(1)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"      âŒ Error in negative steering for {label}: {e}\")\n",
    "                            pbar.update(1)\n",
    "\n",
    "                else:\n",
    "                    print(f\"    âš ï¸  No steering config found for {label}\")\n",
    "\n",
    "            results.append(result_entry)\n",
    "            completed_ops = (msg_idx + 1) * len(available_emotional_labels) * 2\n",
    "            total_ops = len(test_messages) * len(available_emotional_labels) * 2\n",
    "            print(f\"  âœ… Message {msg_idx+1} completed! ({completed_ops}/{total_ops} total operations)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error processing message {msg_idx+1}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ All testing completed! Processed {len(results)} messages successfully.\")\n",
    "    return results\n",
    "\n",
    "# Test on a subset of emotional evaluation messages\n",
    "test_messages = eval_messages[-5:]  # Use last 5 evaluation messages\n",
    "print(f\"\\nðŸ§ª Starting emotional steering tests...\")\n",
    "print(f\"ðŸ“Š Using {len(test_messages)} test messages\")\n",
    "\n",
    "if combined_vectors:\n",
    "    print(\"ðŸŽ¯ Feature vectors available - proceeding with tests...\")\n",
    "    \n",
    "    # Show progress estimation\n",
    "    available_emotional_labels = [label for label in combined_vectors.keys()\n",
    "                                 if label != \"overall\" and any(keyword in label\n",
    "                                 for keyword in [\"thinking\", \"attribution\", \"projection\", \"anxious\", \"depressive\", \"negative\", \"pessimistic\"])]\n",
    "    \n",
    "    estimated_time = len(test_messages) * len(available_emotional_labels) * 2 * 30  # ~30 seconds per operation\n",
    "    print(f\"â±ï¸  Estimated time: {estimated_time//60} minutes ({estimated_time} seconds)\")\n",
    "    print(\"ðŸš€ Starting tests...\")\n",
    "    \n",
    "    steering_results = test_emotional_steering(\n",
    "        model, tokenizer, combined_vectors, test_messages, steering_config\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Completed steering tests for {len(steering_results)} messages\")\n",
    "\n",
    "    # Save results with progress indicator\n",
    "    print(\"ðŸ’¾ Saving results...\")\n",
    "    with tqdm(total=1, desc=\"ðŸ’¾ Saving to file\") as pbar:\n",
    "        with open(f\"{CONFIG['results_dir']}/data/steering_results_{CONFIG['timestamp']}.json\", \"w\") as f:\n",
    "            json.dump(steering_results, f, indent=2)\n",
    "        pbar.update(1)\n",
    "\n",
    "    print(f\"ðŸ’¾ Saved steering results to steering_results_{CONFIG['timestamp']}.json\")\n",
    "else:\n",
    "    print(\"âš ï¸  No feature vectors available for testing\")\n",
    "    steering_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXUSNGRRxsCS"
   },
   "source": [
    "## Step 8: Analyze and Visualize Results\n",
    "\n",
    "Analyze the effectiveness of emotional steering and create visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKmVaxi-xsCS"
   },
   "outputs": [],
   "source": [
    "def analyze_steering_effectiveness(results):\n",
    "    \"\"\"Analyze the effectiveness of emotional steering\"\"\"\n",
    "\n",
    "    if not results:\n",
    "        print(\"âš ï¸  No results to analyze\")\n",
    "        return\n",
    "\n",
    "    analysis_data = []\n",
    "\n",
    "    for result in results:\n",
    "        baseline_scores = result[\"baseline_analysis\"]\n",
    "\n",
    "        # Analyze each steering condition\n",
    "        for steering_type, steered_result in result[\"steered_responses\"].items():\n",
    "            if \"emotional_analysis\" in steered_result:\n",
    "                steered_scores = steered_result[\"emotional_analysis\"]\n",
    "\n",
    "                label, direction = steering_type.split(\"_\")\n",
    "\n",
    "                analysis_data.append({\n",
    "                    \"message\": result[\"message\"][:50] + \"...\",\n",
    "                    \"label\": label,\n",
    "                    \"direction\": direction,\n",
    "                    \"baseline_depressive\": baseline_scores[\"depressive_score\"],\n",
    "                    \"steered_depressive\": steered_scores[\"depressive_score\"],\n",
    "                    \"baseline_anxious\": baseline_scores[\"anxious_score\"],\n",
    "                    \"steered_anxious\": steered_scores[\"anxious_score\"],\n",
    "                    \"baseline_negative_attribution\": baseline_scores[\"negative_attribution_score\"],\n",
    "                    \"steered_negative_attribution\": steered_scores[\"negative_attribution_score\"],\n",
    "                    \"baseline_pessimistic\": baseline_scores[\"pessimistic_score\"],\n",
    "                    \"steered_pessimistic\": steered_scores[\"pessimistic_score\"],\n",
    "                    \"baseline_total\": baseline_scores[\"total_emotional_score\"],\n",
    "                    \"steered_total\": steered_scores[\"total_emotional_score\"],\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(analysis_data)\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"âš ï¸  No valid analysis data found\")\n",
    "        return\n",
    "\n",
    "    # Calculate effectiveness metrics\n",
    "    print(\"ðŸ“Š Steering Effectiveness Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Group by label and direction\n",
    "    for label in df[\"label\"].unique():\n",
    "        print(f\"\\nðŸŽ¯ {label.replace('-', ' ').title()}:\")\n",
    "\n",
    "        for direction in [\"positive\", \"negative\"]:\n",
    "            subset = df[(df[\"label\"] == label) & (df[\"direction\"] == direction)]\n",
    "\n",
    "            if len(subset) > 0:\n",
    "                # Calculate score changes\n",
    "                score_col = f\"steered_{label.replace('-', '_')}\"\n",
    "                baseline_col = f\"baseline_{label.replace('-', '_')}\"\n",
    "\n",
    "                if score_col in subset.columns and baseline_col in subset.columns:\n",
    "                    avg_change = (subset[score_col] - subset[baseline_col]).mean()\n",
    "\n",
    "                    expected_change = \"increase\" if direction == \"positive\" else \"decrease\"\n",
    "                    effectiveness = \"âœ…\" if (direction == \"positive\" and avg_change > 0) or (direction == \"negative\" and avg_change < 0) else \"âŒ\"\n",
    "\n",
    "                    print(f\"   {direction.title()} steering: {avg_change:.2f} change (expected {expected_change}) {effectiveness}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Analyze results\n",
    "if steering_results:\n",
    "    analysis_df = analyze_steering_effectiveness(steering_results)\n",
    "\n",
    "    if analysis_df is not None and len(analysis_df) > 0:\n",
    "        # Save analysis\n",
    "        analysis_df.to_csv(f\"{CONFIG['results_dir']}/data/steering_analysis_{CONFIG['timestamp']}.csv\", index=False)\n",
    "        print(f\"\\nðŸ’¾ Saved analysis to CSV\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No analysis data to save\")\n",
    "else:\n",
    "    print(\"âš ï¸  No steering results available for analysis\")\n",
    "    analysis_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L31P4o0xxsCS"
   },
   "source": [
    "## Step 9: Create Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvfxk6x-xsCS"
   },
   "outputs": [],
   "source": [
    "def create_steering_visualizations(analysis_df, results_dir):\n",
    "    \"\"\"Create visualizations of steering effectiveness\"\"\"\n",
    "\n",
    "    if analysis_df is None or len(analysis_df) == 0:\n",
    "        print(\"âš ï¸  No data available for visualization\")\n",
    "        return\n",
    "\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Emotional Reasoning Steering Effectiveness', fontsize=16, fontweight='bold')\n",
    "\n",
    "    emotional_categories = [\n",
    "        ('depressive', 'Depressive Thinking'),\n",
    "        ('anxious', 'Anxious Thinking'),\n",
    "        ('negative_attribution', 'Negative Attribution'),\n",
    "        ('pessimistic', 'Pessimistic Projection')\n",
    "    ]\n",
    "\n",
    "    for idx, (category, title) in enumerate(emotional_categories):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "\n",
    "        # Filter data for this category\n",
    "        category_data = analysis_df[analysis_df['label'] == category.replace('_', '-')]\n",
    "\n",
    "        if len(category_data) == 0:\n",
    "            ax.text(0.5, 0.5, f'No data for\\n{title}',\n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "            ax.set_title(title)\n",
    "            continue\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        baseline_col = f'baseline_{category}'\n",
    "        steered_col = f'steered_{category}'\n",
    "\n",
    "        if baseline_col in category_data.columns and steered_col in category_data.columns:\n",
    "            pos_data = category_data[category_data['direction'] == 'positive']\n",
    "            neg_data = category_data[category_data['direction'] == 'negative']\n",
    "\n",
    "            x_pos = np.arange(len(pos_data))\n",
    "            width = 0.35\n",
    "\n",
    "            if len(pos_data) > 0:\n",
    "                ax.bar(x_pos - width/2, pos_data[baseline_col], width,\n",
    "                      label='Baseline', alpha=0.7, color='gray')\n",
    "                ax.bar(x_pos + width/2, pos_data[steered_col], width,\n",
    "                      label='Positive Steering', alpha=0.7, color='red')\n",
    "\n",
    "            if len(neg_data) > 0:\n",
    "                x_neg = np.arange(len(pos_data), len(pos_data) + len(neg_data))\n",
    "                ax.bar(x_neg - width/2, neg_data[baseline_col], width,\n",
    "                      alpha=0.7, color='gray')\n",
    "                ax.bar(x_neg + width/2, neg_data[steered_col], width,\n",
    "                      label='Negative Steering', alpha=0.7, color='blue')\n",
    "\n",
    "            ax.set_title(title)\n",
    "            ax.set_ylabel('Emotional Score (%)')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f'Data columns missing\\nfor {title}',\n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "            ax.set_title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{results_dir}/figures/emotional_steering_effectiveness_{CONFIG[\"timestamp\"]}.png',\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Create summary heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Calculate average changes for heatmap\n",
    "    heatmap_data = []\n",
    "    labels = []\n",
    "\n",
    "    for category, title in emotional_categories:\n",
    "        category_label = category.replace('_', '-')\n",
    "        pos_data = analysis_df[(analysis_df['label'] == category_label) &\n",
    "                              (analysis_df['direction'] == 'positive')]\n",
    "        neg_data = analysis_df[(analysis_df['label'] == category_label) &\n",
    "                              (analysis_df['direction'] == 'negative')]\n",
    "\n",
    "        baseline_col = f'baseline_{category}'\n",
    "        steered_col = f'steered_{category}'\n",
    "\n",
    "        pos_change = 0\n",
    "        neg_change = 0\n",
    "\n",
    "        if len(pos_data) > 0 and baseline_col in pos_data.columns:\n",
    "            pos_change = (pos_data[steered_col] - pos_data[baseline_col]).mean()\n",
    "\n",
    "        if len(neg_data) > 0 and baseline_col in neg_data.columns:\n",
    "            neg_change = (neg_data[steered_col] - neg_data[baseline_col]).mean()\n",
    "\n",
    "        heatmap_data.append([pos_change, neg_change])\n",
    "        labels.append(title)\n",
    "\n",
    "    if heatmap_data:\n",
    "        sns.heatmap(heatmap_data,\n",
    "                   xticklabels=['Positive Steering', 'Negative Steering'],\n",
    "                   yticklabels=labels,\n",
    "                   annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "                   cbar_kws={'label': 'Average Score Change (%)'})\n",
    "\n",
    "        plt.title('Emotional Steering Effectiveness Heatmap', fontweight='bold', pad=20)\n",
    "        plt.xlabel('Steering Direction')\n",
    "        plt.ylabel('Emotional Category')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{results_dir}/figures/emotional_steering_heatmap_{CONFIG[\"timestamp\"]}.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"ðŸ“Š Visualizations saved to {results_dir}/figures/\")\n",
    "\n",
    "# Create visualizations\n",
    "if analysis_df is not None:\n",
    "    create_steering_visualizations(analysis_df, CONFIG[\"results_dir\"])\n",
    "else:\n",
    "    print(\"ðŸ“Š Skipping visualizations - no analysis data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UwuYkjTxsCS"
   },
   "source": [
    "## Step 10: Example Usage and Demo\n",
    "\n",
    "Demonstrate how to use the emotional steering system with specific examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvGzdGoIxsCS"
   },
   "outputs": [],
   "source": [
    "def demo_emotional_steering(model, tokenizer, feature_vectors, steering_config):\n",
    "    \"\"\"Demonstrate emotional steering with examples\"\"\"\n",
    "\n",
    "    if not feature_vectors:\n",
    "        print(\"âš ï¸  No feature vectors available for demo\")\n",
    "        return\n",
    "\n",
    "    demo_messages = [\n",
    "        \"You've been working on a creative project, but it's not turning out as you hoped. How do you feel about your creative abilities?\",\n",
    "        \"You have an important presentation tomorrow. What thoughts are going through your mind?\",\n",
    "        \"You received some constructive feedback on your work. How do you interpret this feedback?\"\n",
    "    ]\n",
    "\n",
    "    # Get available emotional labels from our vectors\n",
    "    available_emotional_labels = [label for label in feature_vectors.keys()\n",
    "                                 if label != \"overall\" and any(keyword in label\n",
    "                                 for keyword in [\"thinking\", \"attribution\", \"projection\", \"anxious\", \"depressive\", \"negative\", \"pessimistic\"])]\n",
    "\n",
    "    if not available_emotional_labels:\n",
    "        print(\"âš ï¸  No emotional labels found in feature vectors\")\n",
    "        return\n",
    "\n",
    "    print(\"ðŸŽ­ Emotional Steering Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ðŸŽ¯ Available emotional labels: {available_emotional_labels}\")\n",
    "\n",
    "    for i, message in enumerate(demo_messages[:min(len(demo_messages), len(available_emotional_labels))]):\n",
    "        label = available_emotional_labels[i % len(available_emotional_labels)]\n",
    "        model_name = CONFIG[\"model_name\"]\n",
    "\n",
    "        if model_name not in steering_config or label not in steering_config[model_name]:\n",
    "            print(f\"âš ï¸  Skipping {label} - steering config not available\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nðŸ“ Message {i+1}: {message}\")\n",
    "        print(f\"ðŸŽ¯ Demonstrating {label.replace('-', ' ').title()} steering\")\n",
    "\n",
    "        try:\n",
    "            # Baseline response\n",
    "            input_ids = tokenizer.encode(message, return_tensors=\"pt\")\n",
    "\n",
    "            with model.generate(\n",
    "                {\"input_ids\": input_ids, \"attention_mask\": (input_ids != tokenizer.pad_token_id).long()},\n",
    "                max_new_tokens=200,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            ) as tracer:\n",
    "                baseline_output = model.generator.output.save()\n",
    "\n",
    "            baseline_text = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "            input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            if baseline_text.startswith(input_text):\n",
    "                baseline_text = baseline_text[len(input_text):].strip()\n",
    "\n",
    "            print(f\"\\nðŸ”µ Baseline Response:\")\n",
    "            print(f\"   {baseline_text[:300]}...\")\n",
    "\n",
    "            baseline_analysis = analyze_emotional_content(baseline_text)\n",
    "            print(f\"   Emotional Score: {baseline_analysis['total_emotional_score']:.1f}%\")\n",
    "\n",
    "            # Positive steering (enhance emotional pattern)\n",
    "            pos_result = generate_and_analyze_emotional(\n",
    "                model, tokenizer, message, feature_vectors, steering_config,\n",
    "                label, \"positive\", 200\n",
    "            )\n",
    "\n",
    "            print(f\"\\nðŸ”´ Enhanced {label.replace('-', ' ').title()}:\")\n",
    "            print(f\"   {pos_result['response'][:300]}...\")\n",
    "            print(f\"   Emotional Score: {pos_result['emotional_analysis']['total_emotional_score']:.1f}%\")\n",
    "\n",
    "            # Negative steering (suppress emotional pattern)\n",
    "            neg_result = generate_and_analyze_emotional(\n",
    "                model, tokenizer, message, feature_vectors, steering_config,\n",
    "                label, \"negative\", 200\n",
    "            )\n",
    "\n",
    "            print(f\"\\nðŸŸ¢ Suppressed {label.replace('-', ' ').title()}:\")\n",
    "            print(f\"   {neg_result['response'][:300]}...\")\n",
    "            print(f\"   Emotional Score: {neg_result['emotional_analysis']['total_emotional_score']:.1f}%\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error in demo for {label}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nâœ… Demo completed!\")\n",
    "\n",
    "# Run demo\n",
    "if combined_vectors:\n",
    "    demo_emotional_steering(model, tokenizer, combined_vectors, steering_config)\n",
    "else:\n",
    "    print(\"âš ï¸  Demo skipped - no feature vectors available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXnyety3xsCS"
   },
   "source": [
    "## Step 11: Safety and Ethical Considerations\n",
    "\n",
    "Document important safety considerations and ethical guidelines for this research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxqqSJuSxsCS"
   },
   "outputs": [],
   "source": [
    "def generate_safety_report():\n",
    "    \"\"\"Generate a safety and ethics report for emotional steering research\"\"\"\n",
    "\n",
    "    report = f\"\"\"\n",
    "# Emotional Reasoning Steering - Safety and Ethics Report\n",
    "\n",
    "**Generated on:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "**Model:** {CONFIG['model_name']}\n",
    "**Research Session ID:** {CONFIG['timestamp']}\n",
    "\n",
    "## Safety Considerations\n",
    "\n",
    "### 1. Research-Only Use\n",
    "- This implementation is designed exclusively for research purposes\n",
    "- Should NOT be deployed in production systems without extensive safety testing\n",
    "- Requires institutional review board (IRB) approval for human subjects research\n",
    "\n",
    "### 2. Potential Risks\n",
    "- **Psychological Harm**: Steering toward negative emotional states could be harmful\n",
    "- **Misuse**: Could be used to manipulate users or create harmful content\n",
    "- **Bias Amplification**: May amplify existing biases in training data\n",
    "- **Unintended Effects**: Steering may have unpredictable side effects\n",
    "\n",
    "### 3. Required Safeguards\n",
    "- **Informed Consent**: Users must know when emotional steering is active\n",
    "- **Monitoring**: Continuous monitoring for harmful outputs\n",
    "- **Reversibility**: Always provide countermeasures (negative steering)\n",
    "- **Access Controls**: Restrict access to authorized researchers only\n",
    "- **Documentation**: Maintain detailed logs of all experiments\n",
    "\n",
    "## Ethical Guidelines\n",
    "\n",
    "### 1. Beneficence\n",
    "- Research should aim to benefit society and advance scientific knowledge\n",
    "- Potential applications in mental health research and AI safety\n",
    "- Must not cause unnecessary harm to participants or society\n",
    "\n",
    "### 2. Autonomy\n",
    "- Respect user autonomy and decision-making capacity\n",
    "- Provide clear information about emotional steering effects\n",
    "- Allow users to opt-out at any time\n",
    "\n",
    "### 3. Justice\n",
    "- Ensure fair distribution of research benefits and risks\n",
    "- Consider impacts on vulnerable populations\n",
    "- Avoid discriminatory or biased applications\n",
    "\n",
    "### 4. Non-maleficence\n",
    "- \"Do no harm\" - minimize risks to participants and society\n",
    "- Implement robust safety measures\n",
    "- Have emergency stop procedures in place\n",
    "\n",
    "## Recommended Usage Protocols\n",
    "\n",
    "### 1. Before Starting Research\n",
    "- Obtain IRB approval for human subjects research\n",
    "- Develop comprehensive safety protocols\n",
    "- Train all research staff on ethical considerations\n",
    "- Establish data security and privacy protections\n",
    "\n",
    "### 2. During Research\n",
    "- Monitor all outputs for harmful content\n",
    "- Maintain detailed experimental logs\n",
    "- Provide psychological support resources to participants\n",
    "- Regular safety reviews and protocol updates\n",
    "\n",
    "### 3. After Research\n",
    "- Secure deletion of sensitive data\n",
    "- Debrief participants about the research\n",
    "- Report findings responsibly to scientific community\n",
    "- Consider long-term societal implications\n",
    "\n",
    "## Technical Safety Measures\n",
    "\n",
    "### 1. Content Filtering\n",
    "- Implement automated content filtering for harmful outputs\n",
    "- Human review of all research outputs\n",
    "- Real-time monitoring of emotional intensity\n",
    "\n",
    "### 2. Access Controls\n",
    "- Multi-factor authentication for system access\n",
    "- Role-based permissions for different user types\n",
    "- Audit logs of all system interactions\n",
    "\n",
    "### 3. Data Security\n",
    "- Encryption of all research data\n",
    "- Secure storage with limited access\n",
    "- Regular security audits and updates\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Emotional reasoning steering is a powerful research tool that requires careful ethical consideration and robust safety measures. This research should only be conducted by qualified researchers with appropriate oversight and safeguards in place.\n",
    "\n",
    "For questions about this research or to report safety concerns, please contact the research team immediately.\n",
    "\"\"\"\n",
    "\n",
    "    return report\n",
    "\n",
    "# Generate and save safety report\n",
    "safety_report = generate_safety_report()\n",
    "\n",
    "with open(f\"{CONFIG['results_dir']}/safety_ethics_report_{CONFIG['timestamp']}.md\", \"w\") as f:\n",
    "    f.write(safety_report)\n",
    "\n",
    "print(\"ðŸ›¡ï¸ Safety and Ethics Report Generated\")\n",
    "print(\"=\" * 50)\n",
    "print(safety_report)\n",
    "print(f\"\\nðŸ’¾ Full report saved to: {CONFIG['results_dir']}/safety_ethics_report_{CONFIG['timestamp']}.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhUFNOGaxsCS"
   },
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook has provided a complete implementation of emotional reasoning steering for language models. Here's what we accomplished:\n",
    "\n",
    "### âœ… Completed Tasks:\n",
    "1. **Extended the COT-steering framework** to include emotional reasoning categories\n",
    "2. **Generated training data** with emotionally-charged prompts\n",
    "3. **Trained steering vectors** for depressive, anxious, negative attribution, and pessimistic thinking patterns\n",
    "4. **Implemented steering functions** to enhance or suppress emotional patterns\n",
    "5. **Created evaluation metrics** to measure emotional content in responses\n",
    "6. **Tested the system** with various emotional steering configurations\n",
    "7. **Generated visualizations** to analyze steering effectiveness\n",
    "8. **Documented safety and ethical considerations**\n",
    "\n",
    "### ðŸ”¬ Research Applications:\n",
    "- **Mental Health Research**: Understanding how AI models represent emotional states\n",
    "- **Bias Detection**: Identifying problematic thinking patterns in AI outputs\n",
    "- **Therapeutic AI**: Training models to recognize and counter negative thought patterns\n",
    "- **Content Moderation**: Detecting and filtering emotionally harmful content\n",
    "- **AI Safety**: Understanding and controlling emotional biases in language models\n",
    "\n",
    "### âš ï¸ Important Reminders:\n",
    "- This is a **research tool only** - not for production use\n",
    "- Requires **ethical oversight** and **IRB approval** for human subjects research\n",
    "- Must include **safety safeguards** and **continuous monitoring**\n",
    "- Should always provide **positive counterbalancing** capabilities\n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "1. **Expand training data** with more diverse emotional prompts\n",
    "2. **Fine-tune steering coefficients** for optimal effectiveness\n",
    "3. **Implement real-time safety monitoring** systems\n",
    "4. **Conduct longitudinal studies** on steering effectiveness\n",
    "5. **Develop therapeutic applications** with proper clinical oversight\n",
    "6. **Create automated bias detection** systems for production AI\n",
    "\n",
    "Remember to use this technology responsibly and always prioritize user safety and well-being in your research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iok2mdkxxsCS"
   },
   "outputs": [],
   "source": [
    "# Final summary of all generated files\n",
    "print(\"ðŸ“ Generated Files Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "files_generated = [\n",
    "    f\"Data files in {CONFIG['results_dir']}/data/:\",\n",
    "    f\"  - emotional_annotations_{CONFIG['timestamp']}.json\",\n",
    "    f\"  - emotional_mean_vectors_{CONFIG['model_name'].split('/')[-1].lower()}_{CONFIG['timestamp']}.pt\",\n",
    "    f\"  - steering_results_{CONFIG['timestamp']}.json\",\n",
    "    f\"  - steering_analysis_{CONFIG['timestamp']}.csv\",\n",
    "    f\"\",\n",
    "    f\"Visualization files in {CONFIG['results_dir']}/figures/:\",\n",
    "    f\"  - emotional_steering_effectiveness_{CONFIG['timestamp']}.png\",\n",
    "    f\"  - emotional_steering_heatmap_{CONFIG['timestamp']}.png\",\n",
    "    f\"\",\n",
    "    f\"Documentation files in {CONFIG['results_dir']}/:\",\n",
    "    f\"  - safety_ethics_report_{CONFIG['timestamp']}.md\",\n",
    "]\n",
    "\n",
    "for file_info in files_generated:\n",
    "    print(file_info)\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Emotional Reasoning Steering Implementation Complete!\")\n",
    "print(f\"ðŸ“Š Session ID: {CONFIG['timestamp']}\")\n",
    "print(f\"ðŸ¤– Model: {CONFIG['model_name']}\")\n",
    "print(f\"ðŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a52ea78a",
    "outputId": "4e8e0aa0-12a6-4a66-d39e-40b2d5da49e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved emotional responses to ./results/data/emotional_responses_20250726_232138.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the filename using the timestamp from CONFIG\n",
    "filename = f\"{CONFIG['results_dir']}/data/emotional_responses_{CONFIG['timestamp']}.json\"\n",
    "\n",
    "# Save the emotional responses to a JSON file\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(emotional_responses, f, indent=4)\n",
    "\n",
    "print(f\"ðŸ’¾ Saved emotional responses to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcJ5_72nJMwp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "11519dca86704a44a45a89daf39c8e47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "161d9b34425d40149a5a0de217625cd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1adc84681e0746e5857192275dd6b9c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1ee2e99c306f49fa8c1b115d5e526637": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "205c6ec9bb0d49baa1d2ac2dc748e859": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "245830e954c84efe941e9647077c4275": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "2b00ab926c4f42418d365668f9bda808": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a45eaef1420740069988b8841870411a",
       "IPY_MODEL_bb1ade4b8c0245c992bd1acf37f839a9",
       "IPY_MODEL_43b89b3f64494581b7534ef6af3818cf",
       "IPY_MODEL_62c4cde86bf3442599ba9574645899fa",
       "IPY_MODEL_440abd470c55440890592f32d15d6008"
      ],
      "layout": "IPY_MODEL_245830e954c84efe941e9647077c4275",
      "tabbable": null,
      "tooltip": null
     }
    },
    "386c1fb5d76447819d9921aa6541efe0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39fcb40c7aea4e20997ad90393490953": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d421272ecd74eafa7b37ba56825f29d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f2c903bb8374de3afae74c5ce1c2f20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "3f2ddebab44d451e8ba7336472f5cd89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4294d11254564871bab873f93f11e2f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c03d1d3b9d964db9b4f5d7f2a0325e4e",
       "IPY_MODEL_f31d59efd08c44b2a4b82d19ee33ab5a",
       "IPY_MODEL_aea505b646df4cddaa6151610502b90f"
      ],
      "layout": "IPY_MODEL_386c1fb5d76447819d9921aa6541efe0",
      "tabbable": null,
      "tooltip": null
     }
    },
    "43b06f05e7d0484a91447842f2665710": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "43b89b3f64494581b7534ef6af3818cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_allow_html": false,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_161d9b34425d40149a5a0de217625cd7",
      "style": "IPY_MODEL_4fa18bd6983945209922c0d11e6488cc",
      "tabbable": null,
      "tooltip": null,
      "value": true
     }
    },
    "440abd470c55440890592f32d15d6008": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_7587005879384baea950a7b6b0fc7dbe",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6ffa75ab570947159398a5e9affabac2",
      "tabbable": null,
      "tooltip": null,
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "45dc6b83fc494382b08f55862684cfc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "47138dc60c534fafb352f4f65564ffcf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4879ced66f3f48648baafda34d912187": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "48d685176fee4ec8bbb42e724c2e5c31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "4ca0cea46f2744e5a0cefc3d9c7ae89b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_cbf302e8b191439e8b104e02f9eebdd7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4879ced66f3f48648baafda34d912187",
      "tabbable": null,
      "tooltip": null,
      "value": "generation_config.json:â€‡100%"
     }
    },
    "4fa18bd6983945209922c0d11e6488cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "CheckboxStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "CheckboxStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": ""
     }
    },
    "558fce6bddf04451b9b71e390aff24fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "5a14946310e14cbd977a134076e20384": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_fa3ded42cfa04131a5bf14fb96d3c537",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bbd1f7c4f2b74768969c94a9ef98ef60",
      "tabbable": null,
      "tooltip": null,
      "value": "tokenizer.json:â€‡"
     }
    },
    "5aa08a40a9044f1abfffbdd72a8cc014": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5bd26e044e6c4dc7b6b8c11572b00f7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62c4cde86bf3442599ba9574645899fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_9cab6ee52ed14e33bb1cb92fe1279bce",
      "style": "IPY_MODEL_d66f092280ff4d67a975742d0dab4c79",
      "tabbable": null,
      "tooltip": null
     }
    },
    "6b8149150e21400aa5a9655cbf5bf73b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_c08a8798174b4445b6b39a3c8795259d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_558fce6bddf04451b9b71e390aff24fc",
      "tabbable": null,
      "tooltip": null,
      "value": "â€‡679/679â€‡[00:00&lt;00:00,â€‡121kB/s]"
     }
    },
    "6ffa75ab570947159398a5e9affabac2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "71214c01dcfd4eb890f1f2a298862c90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cab833362b5f43f2aa43ba063320ac76",
       "IPY_MODEL_cc34ca4e12d64fe0821200c764ec7b94",
       "IPY_MODEL_6b8149150e21400aa5a9655cbf5bf73b"
      ],
      "layout": "IPY_MODEL_3d421272ecd74eafa7b37ba56825f29d",
      "tabbable": null,
      "tooltip": null
     }
    },
    "714bb396f0114f95b47ae7daf1672ab6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7587005879384baea950a7b6b0fc7dbe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80a94d4ccb404c06b497edeb17acb7e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "8179409b290646ac93a3e37130a7dd24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "8a924eef95bb44958709f0e00301ff36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_47138dc60c534fafb352f4f65564ffcf",
      "max": 181,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_714bb396f0114f95b47ae7daf1672ab6",
      "tabbable": null,
      "tooltip": null,
      "value": 181
     }
    },
    "8ea2fb36912146d4b9f529b38d0de7c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "963b72b866d2471b97cd217897b7fa77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a14946310e14cbd977a134076e20384",
       "IPY_MODEL_ce2a645c735249df8d5b289a66e03da1",
       "IPY_MODEL_a0f315ba99554904ac268bd866d1037c"
      ],
      "layout": "IPY_MODEL_39fcb40c7aea4e20997ad90393490953",
      "tabbable": null,
      "tooltip": null
     }
    },
    "96518b8ea0ff4427aacd82f8b99c1816": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_cb58f40f6b6d48978d2f4d73f27ee05b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_43b06f05e7d0484a91447842f2665710",
      "tabbable": null,
      "tooltip": null,
      "value": 1
     }
    },
    "9cab6ee52ed14e33bb1cb92fe1279bce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a00653701aec4c4dbb87d023a6c5d97f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0f315ba99554904ac268bd866d1037c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_a00653701aec4c4dbb87d023a6c5d97f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_80a94d4ccb404c06b497edeb17acb7e6",
      "tabbable": null,
      "tooltip": null,
      "value": "â€‡7.03M/?â€‡[00:00&lt;00:00,â€‡174MB/s]"
     }
    },
    "a45eaef1420740069988b8841870411a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_5bd26e044e6c4dc7b6b8c11572b00f7b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f73690f37d434f4cb08b3ea5f4908949",
      "tabbable": null,
      "tooltip": null,
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "a8ee79d524e34f43ad735f74bb9c4ff1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_b1d32c8b91604c9c986f91909d65a1a1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_48d685176fee4ec8bbb42e724c2e5c31",
      "tabbable": null,
      "tooltip": null,
      "value": "â€‡181/181â€‡[00:00&lt;00:00,â€‡26.1kB/s]"
     }
    },
    "aafd1a65a8bc4401ae666e0aaeb1fa7c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aea505b646df4cddaa6151610502b90f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_d8076c5b7b704b58ae1454a459be3bc9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3f2c903bb8374de3afae74c5ce1c2f20",
      "tabbable": null,
      "tooltip": null,
      "value": "â€‡3.55G/3.55Gâ€‡[00:04&lt;00:00,â€‡1.68GB/s]"
     }
    },
    "b1d32c8b91604c9c986f91909d65a1a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7002a0283894c51a65d00c8f998e856": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4ca0cea46f2744e5a0cefc3d9c7ae89b",
       "IPY_MODEL_8a924eef95bb44958709f0e00301ff36",
       "IPY_MODEL_a8ee79d524e34f43ad735f74bb9c4ff1"
      ],
      "layout": "IPY_MODEL_d60cdd3e04674bef9e141b4e2570dcb4",
      "tabbable": null,
      "tooltip": null
     }
    },
    "babcbf996b3b41ce8ecc23ef5352ed0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "TextStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "TextStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "bb1ade4b8c0245c992bd1acf37f839a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_allow_html": false,
      "disabled": false,
      "layout": "IPY_MODEL_aafd1a65a8bc4401ae666e0aaeb1fa7c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_babcbf996b3b41ce8ecc23ef5352ed0d",
      "tabbable": null,
      "tooltip": null,
      "value": ""
     }
    },
    "bbd1f7c4f2b74768969c94a9ef98ef60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "bc03438f5b5545f18c9fa71aafc6806e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c03d1d3b9d964db9b4f5d7f2a0325e4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_dda7a9e14bc24aa5994981c1eaad92ac",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f1045c0816f641069c005dac0db8ee7f",
      "tabbable": null,
      "tooltip": null,
      "value": "model.safetensors:â€‡100%"
     }
    },
    "c08a8798174b4445b6b39a3c8795259d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3e779d33001443fa9ccd2a6540dbc18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5f10918939347e5ab4a00cb6a25a98f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec948f2a39374075a1e032c7a13b3679",
       "IPY_MODEL_96518b8ea0ff4427aacd82f8b99c1816",
       "IPY_MODEL_e1518031f9ed4efe8dee3be7ab7edd79"
      ],
      "layout": "IPY_MODEL_cf8bec5ed8fe426d9a0f22f5e9e50270",
      "tabbable": null,
      "tooltip": null
     }
    },
    "cab833362b5f43f2aa43ba063320ac76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_11519dca86704a44a45a89daf39c8e47",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8ea2fb36912146d4b9f529b38d0de7c1",
      "tabbable": null,
      "tooltip": null,
      "value": "config.json:â€‡100%"
     }
    },
    "cb58f40f6b6d48978d2f4d73f27ee05b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "cbf302e8b191439e8b104e02f9eebdd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc34ca4e12d64fe0821200c764ec7b94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_db01d70561ec497eaf85d01b8eaa2198",
      "max": 679,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1adc84681e0746e5857192275dd6b9c5",
      "tabbable": null,
      "tooltip": null,
      "value": 679
     }
    },
    "ce2a645c735249df8d5b289a66e03da1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_8179409b290646ac93a3e37130a7dd24",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5aa08a40a9044f1abfffbdd72a8cc014",
      "tabbable": null,
      "tooltip": null,
      "value": 1
     }
    },
    "cf8bec5ed8fe426d9a0f22f5e9e50270": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d60cdd3e04674bef9e141b4e2570dcb4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d66f092280ff4d67a975742d0dab4c79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_family": null,
      "font_size": null,
      "font_style": null,
      "font_variant": null,
      "font_weight": null,
      "text_color": null,
      "text_decoration": null
     }
    },
    "d8076c5b7b704b58ae1454a459be3bc9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db01d70561ec497eaf85d01b8eaa2198": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dda7a9e14bc24aa5994981c1eaad92ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1518031f9ed4efe8dee3be7ab7edd79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_1ee2e99c306f49fa8c1b115d5e526637",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_205c6ec9bb0d49baa1d2ac2dc748e859",
      "tabbable": null,
      "tooltip": null,
      "value": "â€‡3.07k/?â€‡[00:00&lt;00:00,â€‡590kB/s]"
     }
    },
    "ec948f2a39374075a1e032c7a13b3679": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_c3e779d33001443fa9ccd2a6540dbc18",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_45dc6b83fc494382b08f55862684cfc8",
      "tabbable": null,
      "tooltip": null,
      "value": "tokenizer_config.json:â€‡"
     }
    },
    "f1045c0816f641069c005dac0db8ee7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "f31d59efd08c44b2a4b82d19ee33ab5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_bc03438f5b5545f18c9fa71aafc6806e",
      "max": 3554214621,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3f2ddebab44d451e8ba7336472f5cd89",
      "tabbable": null,
      "tooltip": null,
      "value": 3554214621
     }
    },
    "f73690f37d434f4cb08b3ea5f4908949": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "fa3ded42cfa04131a5bf14fb96d3c537": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
