{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Emotional Reasoning Steering with COT-Steering Framework\n",
    "\n",
    "This notebook provides a comprehensive implementation of emotional reasoning steering for language models, extending the existing COT-steering framework to include **depressive-normal dichotomy** and enhanced emotional reasoning patterns.\n",
    "\n",
    "## Key Enhancements\n",
    "\n",
    "1. **Extended Training Data**: Added 40+ new emotional prompts across depressive, anxious, negative attribution, and pessimistic thinking patterns\n",
    "2. **Normal Thinking Baseline**: Added 'normal-thinking' label for balanced, healthy reasoning patterns\n",
    "3. **Depressive-Normal Dichotomy**: Emotional vectors are computed by subtracting normal-thinking vectors from negative emotional vectors\n",
    "4. **Unified Pipeline**: New `emotional_steering_pipeline` function for streamlined emotional steering\n",
    "5. **Enhanced Caching**: All steps are cached to enable resumable processing\n",
    "6. **Google Colab Compatibility**: Optimized for Google Colab with proper mounting and caching\n",
    "\n",
    "## ⚠️ Important Safety Notice\n",
    "\n",
    "This implementation is intended for **research purposes only**. Steering models toward negative emotional states could be harmful if misused. Please:\n",
    "- Use only for legitimate research with proper ethical oversight\n",
    "- Always provide counterbalancing positive steering capabilities\n",
    "- Never deploy this in production systems without appropriate safeguards\n",
    "- Ensure users are aware when emotional steering is active\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e  # stop on first error\n",
    "\n",
    "# ---------- 1) Miniconda (if not present) ----------\n",
    "if ! command -v conda >/dev/null 2>&1; then\n",
    "  echo \"[INFO] Installing Miniconda to /usr/local ...\"\n",
    "  wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "  chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "  bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "fi\n",
    "\n",
    "# Make conda usable in this shell\n",
    "export PATH=\"/usr/local/bin:$PATH\"\n",
    "source /usr/local/etc/profile.d/conda.sh\n",
    "\n",
    "# Speed up env solve\n",
    "conda config --set channel_priority flexible\n",
    "conda config --add channels conda-forge >/dev/null 2>&1 || true\n",
    "conda install -n base -y mamba git pip\n",
    "\n",
    "# ---------- 2) Clone repo (HTTPS instead of SSH) ----------\n",
    "if [ ! -d \"steering-thinking-llms\" ]; then\n",
    "  git clone https://github.com/cvenhoff/steering-thinking-llms.git\n",
    "fi\n",
    "cd steering-thinking-llms\n",
    "\n",
    "# ---------- 3) Create & activate conda env ----------\n",
    "# The repo’s README uses: conda env create -f environment.yaml  && conda activate stllms_env\n",
    "# Use mamba for speed (falls back to conda if needed).\n",
    "if command -v mamba >/dev/null 2>&1; then\n",
    "  mamba env create -f environment.yaml\n",
    "else\n",
    "  conda env create -f environment.yaml\n",
    "fi\n",
    "\n",
    "# The environment name is defined in environment.yaml (README shows 'stllms_env')\n",
    "conda activate stllms_env\n",
    "\n",
    "# ---------- 4) Install package in editable mode ----------\n",
    "pip install -e .\n",
    "\n",
    "# ---------- 5) (Optional) Expose env as a Jupyter kernel for Colab ----------\n",
    "python -m ipykernel install --user --name stllms_env --display-name \"Python (stllms_env)\"\n",
    "\n",
    "echo \"✅ Setup complete. Env 'stllms_env' created & package installed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install torch transformers nnsight openai anthropic python-dotenv tqdm matplotlib seaborn pandas numpy\n",
    "# !pip install -U bitsandbytes -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "bash Miniconda3-latest-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e  # stop on first error\n",
    "\n",
    "# ---------- 1) Miniconda (if not present) ----------\n",
    "if ! command -v conda >/dev/null 2>&1; then\n",
    "  echo \"[INFO] Installing Miniconda to /usr/local ...\"\n",
    "  wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "  chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "  bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "fi\n",
    "\n",
    "# Make conda usable in this shell\n",
    "export PATH=\"/usr/local/bin:$PATH\"\n",
    "source /usr/local/etc/profile.d/conda.sh\n",
    "\n",
    "# Speed up env solve\n",
    "conda config --set channel_priority flexible\n",
    "conda config --add channels conda-forge >/dev/null 2>&1 || true\n",
    "conda install -n base -y mamba git pip\n",
    "\n",
    "# ---------- 2) Clone repo (HTTPS instead of SSH) ----------\n",
    "if [ ! -d \"steering-thinking-llms\" ]; then\n",
    "  git clone https://github.com/cvenhoff/steering-thinking-llms.git\n",
    "fi\n",
    "cd steering-thinking-llms\n",
    "\n",
    "# ---------- 3) Create & activate conda env ----------\n",
    "# The repo’s README uses: conda env create -f environment.yaml  && conda activate stllms_env\n",
    "# Use mamba for speed (falls back to conda if needed).\n",
    "if command -v mamba >/dev/null 2>&1; then\n",
    "  mamba env create -f environment.yaml\n",
    "else\n",
    "  conda env create -f environment.yaml\n",
    "fi\n",
    "\n",
    "# The environment name is defined in environment.yaml (README shows 'stllms_env')\n",
    "conda activate stllms_env\n",
    "\n",
    "# ---------- 4) Install package in editable mode ----------\n",
    "pip install -e .\n",
    "\n",
    "# ---------- 5) (Optional) Expose env as a Jupyter kernel for Colab ----------\n",
    "python -m ipykernel install --user --name stllms_env --display-name \"Python (stllms_env)\"\n",
    "\n",
    "echo \"✅ Setup complete. Env 'stllms_env' created & package installed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e                       # stop on first error\n",
    "\n",
    "# ---------- 1. Install Miniconda ---------- #\n",
    "wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "\n",
    "# ---------- 2. Prepare conda ---------- #\n",
    "export PATH=\"/usr/local/bin:$PATH\"\n",
    "source /usr/local/etc/profile.d/conda.sh   # enables 'conda activate'\n",
    "\n",
    "# ---------- 3. Clone repo (if needed) ---------- #\n",
    "if [ ! -d \"COT-steering\" ]; then\n",
    "  git clone https://github.com/ChuloIva/COT-steering.git\n",
    "fi\n",
    "cd COT-steering\n",
    "\n",
    "# ---------- 4. Create env & install pkg ---------- #\n",
    "conda env create -f environment.yaml         # creates stllms_env\n",
    "conda activate stllms_env\n",
    "pip install -e .\n",
    "\n",
    "echo \"✅ Finished: environment 'stllms_env' ready and COT-steering installed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab specific setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Clone the repository if not already present\n",
    "    if not os.path.exists('./COT-steering'):\n",
    "        !git clone https://github.com/ChuloIva/COT-steering\n",
    "    \n",
    "    os.chdir('./COT-steering')\n",
    "    print(\"Current working directory:\", os.getcwd())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Optional: Link to Google Drive for persistent storage\n",
    "#     DRIVE_PATH = '/content/drive/MyDrive/COT_Steering_Results'\n",
    "#     if not os.path.exists(DRIVE_PATH):\n",
    "#         os.makedirs(DRIVE_PATH)\n",
    "#     print(f\"Drive storage path: {DRIVE_PATH}\")\n",
    "# else:\n",
    "#     print(\"Running locally\")\n",
    "#     DRIVE_PATH = './results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face login\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Log in to Hugging Face Hub (set HUGGINGFACE_TOKEN env variable or paste when prompted)\n",
    "login(token=None, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add paths to import local modules\n",
    "sys.path.append('./utils')\n",
    "sys.path.append('./messages')\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "from utils import (\n",
    "    load_model_and_vectors,\n",
    "    process_batch_annotations,\n",
    "    process_saved_responses_batch,\n",
    "    custom_generate_steering,\n",
    "    analyze_emotional_content,\n",
    "    generate_and_analyze_emotional,\n",
    "    emotional_steering_pipeline,\n",
    "    steering_config,\n",
    "    chat\n",
    ")\n",
    "\n",
    "from messages import messages, eval_messages\n",
    "\n",
    "print(\"✅ Dependencies loaded successfully!\")\n",
    "print(f\"🐍 Python version: {sys.version}\")\n",
    "print(f\"🔥 PyTorch version: {torch.__version__}\")\n",
    "print(f\"💾 CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Keys (set these with your actual keys)\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = ''  # Add your OpenAI API key here\n",
    "os.environ['ANTHROPIC_API_KEY'] = ''  # Add your Anthropic API key here\n",
    "\n",
    "# Configuration settings\n",
    "CONFIG = {\n",
    "    \"model_name\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",  # Change as needed\n",
    "    \"device\": \"auto\",  # auto-detect, or specify \"cuda\", \"mps\", \"cpu\"\n",
    "    \"load_in_8bit\": False,\n",
    "    \"max_new_tokens\": 1000,\n",
    "    \"batch_size\": 4,\n",
    "    \"include_emotional\": True,  # Whether to include emotional reasoning in training\n",
    "    \"results_dir\": DRIVE_PATH if IN_COLAB else \"./results\",\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"enable_caching\": True,  # Enable caching for resumable processing\n",
    "    \"cache_dir\": os.path.join(DRIVE_PATH if IN_COLAB else \"./results\", \"cache\")\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for dir_name in [CONFIG[\"results_dir\"], CONFIG[\"cache_dir\"], \n",
    "                 f\"{CONFIG['results_dir']}/figures\", f\"{CONFIG['results_dir']}/data\"]:\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "print(f\"📋 Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Model and Check for Cached Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cached_data(config):\n",
    "    \"\"\"Load cached data if available\"\"\"\n",
    "    cached_data = {}\n",
    "    \n",
    "    # Check for cached model vectors\n",
    "    model_id = config[\"model_name\"].split('/')[-1].lower()\n",
    "    vector_files = [\n",
    "        f\"mean_vectors_{model_id}.pt\",\n",
    "        f\"feature_vectors_{model_id}.pt\",\n",
    "        f\"steering_vectors_{model_id}.pt\"\n",
    "    ]\n",
    "    \n",
    "    for filename in vector_files:\n",
    "        filepath = os.path.join(config[\"cache_dir\"], filename)\n",
    "        if os.path.exists(filepath):\n",
    "            try:\n",
    "                cached_data[filename.replace('.pt', '')] = torch.load(filepath, map_location='cpu')\n",
    "                print(f\"✅ Loaded cached {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Error loading {filename}: {e}\")\n",
    "    \n",
    "    # Check for cached responses and annotations\n",
    "    response_files = [\n",
    "        \"emotional_responses.json\",\n",
    "        \"emotional_annotations.json\",\n",
    "        \"training_results.json\"\n",
    "    ]\n",
    "    \n",
    "    for filename in response_files:\n",
    "        filepath = os.path.join(config[\"cache_dir\"], filename)\n",
    "        if os.path.exists(filepath):\n",
    "            try:\n",
    "                with open(filepath, 'r') as f:\n",
    "                    cached_data[filename.replace('.json', '')] = json.load(f)\n",
    "                print(f\"✅ Loaded cached {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Error loading {filename}: {e}\")\n",
    "    \n",
    "    return cached_data\n",
    "\n",
    "# Load cached data\n",
    "print(\"🔍 Checking for cached data...\")\n",
    "cached_data = load_cached_data(CONFIG)\n",
    "\n",
    "if cached_data:\n",
    "    print(f\"📦 Found {len(cached_data)} cached items: {list(cached_data.keys())}\")\n",
    "else:\n",
    "    print(\"📝 No cached data found - will generate from scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🤖 Loading model and tokenizer...\")\n",
    "\n",
    "model, tokenizer, existing_vectors = load_model_and_vectors(\n",
    "    device=CONFIG[\"device\"],\n",
    "    load_in_8bit=CONFIG[\"load_in_8bit\"],\n",
    "    compute_features=True,\n",
    "    model_name=CONFIG[\"model_name\"]\n",
    ")\n",
    "\n",
    "print(f\"✅ Model loaded: {CONFIG['model_name']}\")\n",
    "print(f\"📊 Device: {next(model.parameters()).device}\")\n",
    "print(f\"🎯 Model has {model.config.num_hidden_layers} layers\")\n",
    "print(f\"📝 Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "# Check for existing vectors or use cached ones\n",
    "if 'feature_vectors' in cached_data:\n",
    "    feature_vectors = cached_data['feature_vectors']\n",
    "    print(f\"📦 Using cached feature vectors: {list(feature_vectors.keys())}\")\n",
    "elif existing_vectors:\n",
    "    feature_vectors = existing_vectors\n",
    "    print(f\"📦 Using existing feature vectors: {list(feature_vectors.keys())}\")\n",
    "else:\n",
    "    feature_vectors = None\n",
    "    print(\"⚠️  No feature vectors found - will need to train from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Enhanced Emotional Messages\n",
    "\n",
    "The enhanced message set includes:\n",
    "- **40+ additional emotional prompts** across all categories\n",
    "- **15 normal/balanced thinking prompts** for the normal-thinking baseline\n",
    "- **Better coverage** of depressive, anxious, negative attribution, and pessimistic patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced message filtering with better categorization\n",
    "def categorize_messages(messages):\n",
    "    \"\"\"Categorize messages into emotional and cognitive types with enhanced detection\"\"\"\n",
    "    emotional_messages = []\n",
    "    cognitive_messages = []\n",
    "    normal_messages = []\n",
    "    \n",
    "    for msg in messages:\n",
    "        content = msg[\"content\"].lower()\n",
    "        \n",
    "        # Enhanced emotional indicators\n",
    "        emotional_indicators = [\n",
    "            \"you've been\", \"your recent\", \"everyone around you\", \"after receiving\",\n",
    "            \"what does this\", \"how do you interpret\", \"what might this\", \"what are all the ways\",\n",
    "            \"walk through all\", \"consider all the potential\", \"what could this\",\n",
    "            \"what are the various\", \"what are the ways\", \"what challenges\", \"what obstacles\",\n",
    "            \"how do you feel\", \"what thoughts are\", \"what does this suggest\", \"what might others\",\n",
    "            \"how do you view\", \"what does this indicate\", \"what might this change\",\n",
    "            \"what concerning\", \"what dangerous\", \"what problems\", \"what negative\",\n",
    "            \"what factors contributed\", \"how do you account\", \"what motivated them\",\n",
    "            \"what explains\", \"what accounted for\"\n",
    "        ]\n",
    "        \n",
    "        # Normal thinking indicators\n",
    "        normal_indicators = [\n",
    "            \"how would you approach\", \"what factors would you\", \"what strategies would\",\n",
    "            \"how would you use\", \"what would be a reasonable\", \"how would you best\",\n",
    "            \"how would you evaluate\", \"what would be a healthy\", \"what considerations would\",\n",
    "            \"how would you decide\", \"what steps would help\", \"what approach would help\",\n",
    "            \"what process would help\"\n",
    "        ]\n",
    "        \n",
    "        if any(indicator in content for indicator in normal_indicators):\n",
    "            normal_messages.append(msg)\n",
    "        elif any(indicator in content for indicator in emotional_indicators):\n",
    "            emotional_messages.append(msg)\n",
    "        else:\n",
    "            cognitive_messages.append(msg)\n",
    "    \n",
    "    return emotional_messages, cognitive_messages, normal_messages\n",
    "\n",
    "emotional_messages, cognitive_messages, normal_messages = categorize_messages(messages)\n",
    "\n",
    "print(f\"📊 Enhanced Message Breakdown:\")\n",
    "print(f\"   🧠 Cognitive messages: {len(cognitive_messages)}\")\n",
    "print(f\"   😔 Emotional messages: {len(emotional_messages)}\")\n",
    "print(f\"   🎯 Normal thinking messages: {len(normal_messages)}\")\n",
    "print(f\"   📝 Total messages: {len(messages)}\")\n",
    "\n",
    "# Show examples\n",
    "print(f\"\\n📝 Example emotional messages:\")\n",
    "for i, msg in enumerate(emotional_messages[:3]):\n",
    "    print(f\"   {i+1}. {msg['content'][:80]}...\")\n",
    "\n",
    "print(f\"\\n🎯 Example normal thinking messages:\")\n",
    "for i, msg in enumerate(normal_messages[:3]):\n",
    "    print(f\"   {i+1}. {msg['content'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate or Load Training Responses\n",
    "\n",
    "This step generates responses to emotional and normal thinking prompts, with caching support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_or_load_responses(messages_subset, config, cache_key, max_new_tokens=1000):\n",
    "    \"\"\"Generate responses or load from cache\"\"\"\n",
    "    cache_file = os.path.join(config[\"cache_dir\"], f\"{cache_key}.json\")\n",
    "    \n",
    "    if config[\"enable_caching\"] and os.path.exists(cache_file):\n",
    "        print(f\"📂 Loading cached responses from {cache_file}\")\n",
    "        with open(cache_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    print(f\"🔄 Generating {len(messages_subset)} responses...\")\n",
    "    responses = []\n",
    "\n",
    "    for msg in tqdm(messages_subset, desc=\"Generating responses\"):\n",
    "        try:\n",
    "            input_ids = tokenizer.encode(msg[\"content\"], return_tensors=\"pt\")\n",
    "\n",
    "            with model.generate(\n",
    "                {\"input_ids\": input_ids, \"attention_mask\": (input_ids != tokenizer.pad_token_id).long()},\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            ) as tracer:\n",
    "                output = model.generator.output.save()\n",
    "\n",
    "            response_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            if response_text.startswith(input_text):\n",
    "                response_text = response_text[len(input_text):].strip()\n",
    "\n",
    "            responses.append({\n",
    "                \"message\": msg[\"content\"],\n",
    "                \"response\": response_text\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Cache the results\n",
    "    if config[\"enable_caching\"]:\n",
    "        with open(cache_file, 'w') as f:\n",
    "            json.dump(responses, f, indent=2)\n",
    "        print(f\"💾 Cached responses to {cache_file}\")\n",
    "    \n",
    "    return responses\n",
    "\n",
    "# Generate responses for both emotional and normal messages\n",
    "combined_training_messages = emotional_messages[:30] + normal_messages[:15]  # Balanced training set\n",
    "training_responses = generate_or_load_responses(\n",
    "    combined_training_messages, CONFIG, \"enhanced_training_responses\", CONFIG[\"max_new_tokens\"]\n",
    ")\n",
    "\n",
    "print(f\"✅ Ready with {len(training_responses)} training responses\")\n",
    "\n",
    "# Show example\n",
    "if training_responses:\n",
    "    print(f\"\\n📝 Example response:\")\n",
    "    print(f\"   Input: {training_responses[0]['message'][:80]}...\")\n",
    "    print(f\"   Output: {training_responses[0]['response'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Annotate Responses with Enhanced Emotional Labels\n",
    "\n",
    "This step uses GPT-4 to annotate responses with both cognitive and emotional reasoning labels, including the new **normal-thinking** label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_or_load_annotations(responses, config, cache_key, include_emotional=True):\n",
    "    \"\"\"Generate annotations or load from cache\"\"\"\n",
    "    cache_file = os.path.join(config[\"cache_dir\"], f\"{cache_key}.json\")\n",
    "    \n",
    "    if config[\"enable_caching\"] and os.path.exists(cache_file):\n",
    "        print(f\"📂 Loading cached annotations from {cache_file}\")\n",
    "        with open(cache_file, 'r') as f:\n",
    "            annotation_data = json.load(f)\n",
    "            return [item[\"annotation\"] for item in annotation_data[\"responses\"]]\n",
    "    \n",
    "    print(f\"🏷️  Generating annotations for {len(responses)} responses...\")\n",
    "    response_texts = [resp[\"response\"] for resp in responses]\n",
    "    \n",
    "    # Generate annotations\n",
    "    annotated_responses = process_batch_annotations(\n",
    "        response_texts, include_emotional=include_emotional\n",
    "    )\n",
    "    \n",
    "    # Cache the results\n",
    "    if config[\"enable_caching\"]:\n",
    "        annotation_data = {\n",
    "            \"timestamp\": config[\"timestamp\"],\n",
    "            \"model_name\": config[\"model_name\"],\n",
    "            \"include_emotional\": include_emotional,\n",
    "            \"responses\": [\n",
    "                {\n",
    "                    \"message\": responses[i][\"message\"],\n",
    "                    \"response\": responses[i][\"response\"],\n",
    "                    \"annotation\": annotated_responses[i]\n",
    "                }\n",
    "                for i in range(len(responses))\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(cache_file, \"w\") as f:\n",
    "            json.dump(annotation_data, f, indent=2)\n",
    "        print(f\"💾 Cached annotations to {cache_file}\")\n",
    "    \n",
    "    return annotated_responses\n",
    "\n",
    "# Generate annotations with enhanced emotional labels\n",
    "annotated_responses = generate_or_load_annotations(\n",
    "    training_responses, CONFIG, \"enhanced_annotations\", include_emotional=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Generated {len(annotated_responses)} annotations\")\n",
    "\n",
    "# Show example annotation\n",
    "if annotated_responses:\n",
    "    print(f\"\\n📝 Example annotation:\")\n",
    "    print(f\"   Original: {training_responses[0]['response'][:100]}...\")\n",
    "    print(f\"   Annotated: {annotated_responses[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train Enhanced Emotional Vectors\n",
    "\n",
    "This step processes the annotated responses to extract neural activations and train steering vectors using the **depressive-normal dichotomy** approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_saved_responses_batch_optimized(responses_list, tokenizer, model, keep_on_gpu=True, batch_size=8):\n",
    "    \"\"\"Optimized version with GPU utilization and progress tracking\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    all_batch_outputs = []\n",
    "\n",
    "    # Process in smaller batches to manage memory\n",
    "    for batch_start in tqdm(range(0, len(responses_list), batch_size), desc=\"Processing response batches\"):\n",
    "        batch_end = min(batch_start + batch_size, len(responses_list))\n",
    "        batch_responses = responses_list[batch_start:batch_end]\n",
    "\n",
    "        tokenized_responses = get_batched_message_ids(tokenizer, batch_responses, device.type)\n",
    "\n",
    "        # Process the inputs through the model to get activations\n",
    "        layer_outputs = []\n",
    "        with model.trace({\n",
    "            \"input_ids\": tokenized_responses,\n",
    "            \"attention_mask\": (tokenized_responses != tokenizer.pad_token_id).long()\n",
    "        }) as tracer:\n",
    "            # Capture layer outputs with progress\n",
    "            for layer_idx in tqdm(range(model.config.num_hidden_layers),\n",
    "                                desc=f\"Extracting layers (batch {batch_start//batch_size + 1})\",\n",
    "                                leave=False):\n",
    "                layer_outputs.append(model.model.layers[layer_idx].output[0].save())\n",
    "\n",
    "        # Keep on GPU if requested, otherwise move to CPU\n",
    "        if keep_on_gpu and device.type == 'cuda':\n",
    "            layer_outputs = [x.value.detach() for x in layer_outputs]  # Keep on GPU\n",
    "        else:\n",
    "            layer_outputs = [x.value.cpu().detach().to(torch.float32) for x in layer_outputs]\n",
    "\n",
    "        batch_layer_outputs = []\n",
    "\n",
    "        for batch_idx in tqdm(range(len(batch_responses)),\n",
    "                            desc=f\"Processing examples (batch {batch_start//batch_size + 1})\",\n",
    "                            leave=False):\n",
    "            # get length of padding tokens\n",
    "            attention_mask = (tokenized_responses[batch_idx] != tokenizer.pad_token_id).long()\n",
    "            padding_length = (attention_mask.squeeze() == 0).sum().item()\n",
    "\n",
    "            # Slice out just the non-padded activations for this example across all layers\n",
    "            example_outputs = torch.stack([\n",
    "                layer_output[batch_idx][padding_length:]\n",
    "                for layer_output in layer_outputs\n",
    "            ])\n",
    "\n",
    "            batch_layer_outputs.append(example_outputs)\n",
    "\n",
    "        all_batch_outputs.extend(batch_layer_outputs)\n",
    "\n",
    "        # Clear GPU cache between batches\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return all_batch_outputs\n",
    "\n",
    "def train_enhanced_emotional_vectors_optimized(responses, annotations, model, tokenizer, config):\n",
    "    \"\"\"Optimized training with better GPU utilization and progress tracking\"\"\"\n",
    "\n",
    "    cache_file = os.path.join(config[\"cache_dir\"], \"enhanced_mean_vectors.pt\")\n",
    "\n",
    "    if config[\"enable_caching\"] and os.path.exists(cache_file):\n",
    "        print(f\"📂 Loading cached mean vectors from {cache_file}\")\n",
    "        return torch.load(cache_file, map_location='cpu')\n",
    "\n",
    "    print(\"🧠 Training enhanced emotional vectors with GPU optimization...\")\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Extract activations with optimized function\n",
    "    print(\"   📊 Extracting neural activations...\")\n",
    "    batch_activations = process_saved_responses_batch_optimized(\n",
    "        responses, tokenizer, model,\n",
    "        keep_on_gpu=(device.type == 'cuda'),\n",
    "        batch_size=4  # Adjust based on GPU memory\n",
    "    )\n",
    "\n",
    "    # Initialize mean vectors storage\n",
    "    from collections import defaultdict\n",
    "    mean_vectors = defaultdict(lambda: {\n",
    "        'mean': torch.zeros(model.config.num_hidden_layers, model.config.hidden_size, device=device),\n",
    "        'count': 0\n",
    "    })\n",
    "\n",
    "    # Process annotations to find labels and compute means with progress\n",
    "    print(\"   🏷️  Processing annotations and computing means...\")\n",
    "    successful_processed = 0\n",
    "\n",
    "    for i, (response, annotation) in enumerate(tqdm(zip(responses, annotations),\n",
    "                                                   total=len(responses),\n",
    "                                                   desc=\"Computing mean vectors\")):\n",
    "        try:\n",
    "            # Extract label positions\n",
    "            from utils import get_label_positions\n",
    "            label_positions = get_label_positions(annotation, response, tokenizer)\n",
    "\n",
    "            # Get activations for this response\n",
    "            if i < len(batch_activations):\n",
    "                activations = batch_activations[i]\n",
    "\n",
    "                # Move to GPU for computation if not already there\n",
    "                if device.type == 'cuda' and activations.device.type != 'cuda':\n",
    "                    activations = activations.to(device)\n",
    "\n",
    "                # Ensure activation tensor has correct shape\n",
    "                if len(activations.shape) == 2 and activations.shape[0] == model.config.num_hidden_layers:\n",
    "                    # Update overall mean using running average\n",
    "                    current_count = mean_vectors['overall']['count']\n",
    "                    current_mean = mean_vectors['overall']['mean']\n",
    "                    mean_vectors['overall']['mean'] = current_mean + (activations - current_mean) / (current_count + 1)\n",
    "                    mean_vectors['overall']['count'] += 1\n",
    "\n",
    "                    # Update label-specific means\n",
    "                    for label in label_positions.keys():\n",
    "                        if label != 'end-section':\n",
    "                            current_count = mean_vectors[label]['count']\n",
    "                            current_mean = mean_vectors[label]['mean']\n",
    "                            mean_vectors[label]['mean'] = current_mean + (activations - current_mean) / (current_count + 1)\n",
    "                            mean_vectors[label]['count'] += 1\n",
    "\n",
    "                    successful_processed += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Error processing response {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Periodic progress update\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"   📈 Processed {i+1}/{len(responses)} responses, {successful_processed} successful\")\n",
    "\n",
    "    # Convert to CPU for saving and create regular dict\n",
    "    print(\"   💾 Converting results and caching...\")\n",
    "    save_dict = {}\n",
    "    for k, v in tqdm(mean_vectors.items(), desc=\"Converting to save format\"):\n",
    "        save_dict[k] = {\n",
    "            'mean': v['mean'].cpu(),\n",
    "            'count': v['count']\n",
    "        }\n",
    "\n",
    "    # Cache the results\n",
    "    if config[\"enable_caching\"]:\n",
    "        torch.save(save_dict, cache_file)\n",
    "        print(f\"💾 Cached mean vectors to {cache_file}\")\n",
    "\n",
    "    print(f\"✅ Training completed! Processed {successful_processed}/{len(responses)} responses successfully\")\n",
    "    print(f\"🎯 Trained vectors for {len(save_dict)} categories:\")\n",
    "    for label, data in save_dict.items():\n",
    "        print(f\"   {label}: {data['count']} samples\")\n",
    "\n",
    "    return save_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED Step 5: Train Enhanced Emotional Vectors (GPU Optimized)\n",
    "\n",
    "# First, let's check your actual device setup\n",
    "print(\"🔍 Device Detection:\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"   Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"   CUDA device name: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "print(f\"   MPS available: {hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()}\")\n",
    "print(f\"   Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "def train_enhanced_emotional_vectors_optimized_fixed(responses, annotations, model, tokenizer, config):\n",
    "    \"\"\"Train steering vectors with enhanced emotional categories and normal baseline - FIXED\"\"\"\n",
    "\n",
    "    cache_file = os.path.join(config[\"cache_dir\"], \"enhanced_mean_vectors.pt\")\n",
    "\n",
    "    if config[\"enable_caching\"] and os.path.exists(cache_file):\n",
    "        print(f\"📂 Loading cached mean vectors from {cache_file}\")\n",
    "        return torch.load(cache_file, map_location='cpu')\n",
    "\n",
    "    print(\"🧠 Training enhanced emotional vectors with GPU optimization...\")\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"   🎯 Using device: {device}\")\n",
    "\n",
    "    # Import the function from utils - this fixes the NameError\n",
    "    from utils import get_batched_message_ids\n",
    "\n",
    "    def process_responses_gpu_optimized_fixed(responses_list, batch_size=4):\n",
    "        \"\"\"Process responses keeping activations on GPU for faster computation - FIXED\"\"\"\n",
    "        all_activations = []\n",
    "\n",
    "        for batch_start in tqdm(range(0, len(responses_list), batch_size), desc=\"🔄 Processing batches\"):\n",
    "            batch_end = min(batch_start + batch_size, len(responses_list))\n",
    "            batch_responses = responses_list[batch_start:batch_end]\n",
    "\n",
    "            # Tokenize batch - use the actual device string, not device.type\n",
    "            device_str = str(device).split(':')[0]  # Convert 'cuda:0' to 'cuda'\n",
    "            tokenized_responses = get_batched_message_ids(tokenizer, batch_responses, device_str)\n",
    "\n",
    "            # Ensure tokenized responses are on the same device as model\n",
    "            if tokenized_responses.device != device:\n",
    "                tokenized_responses = tokenized_responses.to(device)\n",
    "\n",
    "            # Extract activations\n",
    "            layer_outputs = []\n",
    "            with model.trace({\n",
    "                \"input_ids\": tokenized_responses,\n",
    "                \"attention_mask\": (tokenized_responses != tokenizer.pad_token_id).long()\n",
    "            }) as tracer:\n",
    "                for layer_idx in tqdm(range(model.config.num_hidden_layers),\n",
    "                                    desc=f\"Extracting layers\",\n",
    "                                    leave=False):\n",
    "                    layer_outputs.append(model.model.layers[layer_idx].output[0].save())\n",
    "\n",
    "            # Keep on GPU for faster processing if using CUDA\n",
    "            if device.type == 'cuda':\n",
    "                layer_outputs = [x.value.detach() for x in layer_outputs]\n",
    "                print(f\"   ✅ Keeping batch {batch_start//batch_size + 1} on GPU\")\n",
    "            else:\n",
    "                layer_outputs = [x.value.cpu().detach().to(torch.float32) for x in layer_outputs]\n",
    "                print(f\"   📱 Processing batch {batch_start//batch_size + 1} on {device}\")\n",
    "\n",
    "            # Process each example in batch - FIXED TENSOR SHAPE HANDLING\n",
    "            batch_activations = []\n",
    "            for batch_idx in tqdm(range(len(batch_responses)),\n",
    "                                desc=f\"Processing examples\",\n",
    "                                leave=False):\n",
    "                attention_mask = (tokenized_responses[batch_idx] != tokenizer.pad_token_id).long()\n",
    "                padding_length = (attention_mask.squeeze() == 0).sum().item()\n",
    "\n",
    "                # Extract non-padded activations - CRITICAL FIX HERE\n",
    "                example_activations = []\n",
    "                for layer_output in layer_outputs:\n",
    "                    # layer_output has shape [batch_size, seq_len, hidden_size]\n",
    "                    # We want [seq_len, hidden_size] for this specific example, removing padding\n",
    "                    layer_activation = layer_output[batch_idx][padding_length:]  # [actual_seq_len, hidden_size]\n",
    "                    example_activations.append(layer_activation)\n",
    "                \n",
    "                # Stack to get [num_layers, actual_seq_len, hidden_size]\n",
    "                example_outputs = torch.stack(example_activations, dim=0)\n",
    "                \n",
    "                # 🔧 CRITICAL FIX: Take mean across sequence dimension to get [num_layers, hidden_size]\n",
    "                if len(example_outputs.shape) == 3:  # [num_layers, seq_len, hidden_size]\n",
    "                    example_outputs = example_outputs.mean(dim=1)  # [num_layers, hidden_size]\n",
    "                \n",
    "                # Debug print for first few examples\n",
    "                if batch_start == 0 and batch_idx < 2:\n",
    "                    print(f\"   🔍 Example {batch_idx} final shape: {example_outputs.shape}\")\n",
    "                    print(f\"       Expected: [{model.config.num_hidden_layers}, {model.config.hidden_size}]\")\n",
    "                \n",
    "                # Validate final shape\n",
    "                if example_outputs.shape != (model.config.num_hidden_layers, model.config.hidden_size):\n",
    "                    print(f\"   ⚠️ Unexpected final shape for example {batch_idx}: {example_outputs.shape}\")\n",
    "                    print(f\"       Expected: ({model.config.num_hidden_layers}, {model.config.hidden_size})\")\n",
    "                    continue\n",
    "                \n",
    "                batch_activations.append(example_outputs)\n",
    "\n",
    "            all_activations.extend(batch_activations)\n",
    "\n",
    "            # Clear cache periodically\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            print(f\"   📊 Completed batch {batch_start//batch_size + 1}/{(len(responses_list) + batch_size - 1)//batch_size}\")\n",
    "\n",
    "        return all_activations\n",
    "\n",
    "    # Extract activations with GPU optimization\n",
    "    print(\"   📊 Extracting neural activations...\")\n",
    "    batch_activations = process_responses_gpu_optimized_fixed(responses, batch_size=2)  # Smaller batch size for stability\n",
    "\n",
    "    # Initialize mean vectors on appropriate device\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Use CPU for mean vector storage to avoid GPU memory issues\n",
    "    mean_vectors = defaultdict(lambda: {\n",
    "        'mean': torch.zeros(model.config.num_hidden_layers, model.config.hidden_size, dtype=torch.float32),\n",
    "        'count': 0\n",
    "    })\n",
    "\n",
    "    # Process with detailed progress tracking - FIXED TENSOR OPERATIONS\n",
    "    print(\"   🏷️  Computing mean vectors...\")\n",
    "    successful_processed = 0\n",
    "    label_counts = defaultdict(int)\n",
    "\n",
    "    for i, (response, annotation) in enumerate(tqdm(zip(responses, annotations),\n",
    "                                                   total=len(responses),\n",
    "                                                   desc=\"Computing means\")):\n",
    "        try:\n",
    "            # Extract label positions\n",
    "            from utils import get_label_positions\n",
    "            label_positions = get_label_positions(annotation, response, tokenizer)\n",
    "\n",
    "            if i < len(batch_activations) and label_positions:\n",
    "                activations = batch_activations[i]\n",
    "\n",
    "                # Move to CPU for mean computation to avoid GPU memory issues\n",
    "                if activations.device.type != 'cpu':\n",
    "                    activations = activations.cpu().float()\n",
    "\n",
    "                # 🔧 CRITICAL FIX: Activations should already be [num_layers, hidden_size]\n",
    "                # No need to take mean again - that was causing the dimension confusion!\n",
    "                \n",
    "                # Validate tensor dimensions before processing\n",
    "                if activations.shape != (model.config.num_hidden_layers, model.config.hidden_size):\n",
    "                    print(f\"   ⚠️ Skipping response {i}: unexpected shape {activations.shape}\")\n",
    "                    print(f\"       Expected: ({model.config.num_hidden_layers}, {model.config.hidden_size})\")\n",
    "                    continue\n",
    "\n",
    "                # Update overall mean using incremental averaging\n",
    "                current_count = mean_vectors['overall']['count']\n",
    "                if current_count == 0:\n",
    "                    mean_vectors['overall']['mean'] = activations.clone()\n",
    "                else:\n",
    "                    current_mean = mean_vectors['overall']['mean']\n",
    "                    # Ensure tensor types match before arithmetic\n",
    "                    if current_mean.dtype != activations.dtype:\n",
    "                        activations = activations.to(current_mean.dtype)\n",
    "                    mean_vectors['overall']['mean'] = current_mean + (activations - current_mean) / (current_count + 1)\n",
    "                mean_vectors['overall']['count'] += 1\n",
    "\n",
    "                # Update label-specific means\n",
    "                for label in label_positions.keys():\n",
    "                    if label != 'end-section':\n",
    "                        current_count = mean_vectors[label]['count']\n",
    "                        if current_count == 0:\n",
    "                            mean_vectors[label]['mean'] = activations.clone()\n",
    "                        else:\n",
    "                            current_mean = mean_vectors[label]['mean']\n",
    "                            # Ensure tensor types match\n",
    "                            if current_mean.dtype != activations.dtype:\n",
    "                                activations = activations.to(current_mean.dtype)\n",
    "                            mean_vectors[label]['mean'] = current_mean + (activations - current_mean) / (current_count + 1)\n",
    "                        mean_vectors[label]['count'] += 1\n",
    "                        label_counts[label] += 1\n",
    "\n",
    "                successful_processed += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Error processing response {i}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "        # Progress updates every 10 items\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"   📈 Progress: {i+1}/{len(responses)} | Successful: {successful_processed} | Labels found: {len(label_counts)}\")\n",
    "            # Show top 3 labels found so far\n",
    "            if label_counts:\n",
    "                top_labels = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "                print(f\"       Top labels: {dict(top_labels)}\")\n",
    "\n",
    "    # Convert to save format\n",
    "    print(\"   💾 Preparing results for caching...\")\n",
    "    save_dict = {}\n",
    "    for k, v in tqdm(mean_vectors.items(), desc=\"Converting tensors\"):\n",
    "        save_dict[k] = {\n",
    "            'mean': v['mean'].clone(),\n",
    "            'count': v['count']\n",
    "        }\n",
    "\n",
    "    # Cache results\n",
    "    if config[\"enable_caching\"]:\n",
    "        torch.save(save_dict, cache_file)\n",
    "        print(f\"💾 Cached mean vectors to {cache_file}\")\n",
    "\n",
    "    print(f\"\\n✅ Training completed successfully!\")\n",
    "    print(f\"📊 Final Statistics:\")\n",
    "    print(f\"   Total responses processed: {successful_processed}/{len(responses)}\")\n",
    "    print(f\"   Categories with training data: {len([k for k, v in save_dict.items() if v['count'] > 0])}\")\n",
    "    print(f\"   Device used: {device}\")\n",
    "\n",
    "    print(f\"\\n📋 Categories and sample counts:\")\n",
    "    for label, data in sorted(save_dict.items(), key=lambda x: x[1]['count'], reverse=True):\n",
    "        if data['count'] > 0:\n",
    "            print(f\"   ✅ {label}: {data['count']} samples\")\n",
    "        else:\n",
    "            print(f\"   ❌ {label}: {data['count']} samples\")\n",
    "\n",
    "    return save_dict\n",
    "\n",
    "# Run the FIXED optimized training\n",
    "enhanced_mean_vectors = train_enhanced_emotional_vectors_optimized_fixed(\n",
    "    [r[\"response\"] for r in training_responses],\n",
    "    annotated_responses,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔍 Device Detection:\n",
    "   CUDA available: True\n",
    "   CUDA device count: 6\n",
    "   Current CUDA device: 0\n",
    "   CUDA device name: NVIDIA A100 80GB PCIe\n",
    "   MPS available: False\n",
    "   Model device: cuda:0\n",
    "🧠 Training enhanced emotional vectors with GPU optimization...\n",
    "   🎯 Using device: cuda:0\n",
    "   📊 Extracting neural activations...\n",
    "   ✅ Keeping batch 1 on GPU\n",
    "   📊 Completed batch 1/22\n",
    "   ✅ Keeping batch 2 on GPU\n",
    "   📊 Completed batch 2/22\n",
    "   ✅ Keeping batch 3 on GPU\n",
    "   📊 Completed batch 3/22\n",
    "   ✅ Keeping batch 4 on GPU\n",
    "   📊 Completed batch 4/22\n",
    "   ✅ Keeping batch 5 on GPU\n",
    "   📊 Completed batch 5/22\n",
    "   ✅ Keeping batch 6 on GPU\n",
    "   📊 Completed batch 6/22\n",
    "   ✅ Keeping batch 7 on GPU\n",
    "   📊 Completed batch 7/22\n",
    "   ✅ Keeping batch 8 on GPU\n",
    "   📊 Completed batch 8/22\n",
    "   ✅ Keeping batch 9 on GPU\n",
    "   📊 Completed batch 9/22\n",
    "   ✅ Keeping batch 10 on GPU\n",
    "   📊 Completed batch 10/22\n",
    "   ✅ Keeping batch 11 on GPU\n",
    "   📊 Completed batch 11/22\n",
    "   ✅ Keeping batch 12 on GPU\n",
    "   📊 Completed batch 12/22\n",
    "   ✅ Keeping batch 13 on GPU\n",
    "   📊 Completed batch 13/22\n",
    "   ✅ Keeping batch 14 on GPU\n",
    "   📊 Completed batch 14/22\n",
    "   ✅ Keeping batch 15 on GPU\n",
    "   📊 Completed batch 15/22\n",
    "   ✅ Keeping batch 16 on GPU\n",
    "   📊 Completed batch 16/22\n",
    "   ✅ Keeping batch 17 on GPU\n",
    "   📊 Completed batch 17/22\n",
    "   ✅ Keeping batch 18 on GPU\n",
    "   📊 Completed batch 18/22\n",
    "   ✅ Keeping batch 19 on GPU\n",
    "   📊 Completed batch 19/22\n",
    "   ✅ Keeping batch 20 on GPU\n",
    "   📊 Completed batch 20/22\n",
    "   ✅ Keeping batch 21 on GPU\n",
    "   📊 Completed batch 21/22\n",
    "   ✅ Keeping batch 22 on GPU\n",
    "   📊 Completed batch 22/22\n",
    "   🏷️  Computing mean vectors...\n",
    "   ⚠️ Error processing response 2: The size of tensor a (4096) must match the size of tensor b (3810) at non-singleton dimension 1...\n",
    "   💾 Preparing results for caching...\n",
    "💾 Cached mean vectors to ./results/cache/enhanced_mean_vectors.pt\n",
    "\n",
    "✅ Training completed successfully!\n",
    "📊 Final Statistics:\n",
    "   Total responses processed: 1/44\n",
    "   Categories with training data: 3\n",
    "   Device used: cuda:0\n",
    "\n",
    "📋 Categories and sample counts:\n",
    "   ✅ overall: 1 samples\n",
    "   ✅ normal-thinking: 1 samples\n",
    "   ✅ pessimistic-projection: 1 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used (code above is used)\n",
    "\n",
    "# def train_enhanced_emotional_vectors(responses, annotations, model, tokenizer, config):\n",
    "#     \"\"\"Train steering vectors with enhanced emotional categories and normal baseline\"\"\"\n",
    "    \n",
    "#     cache_file = os.path.join(config[\"cache_dir\"], \"enhanced_mean_vectors.pt\")\n",
    "    \n",
    "#     if config[\"enable_caching\"] and os.path.exists(cache_file):\n",
    "#         print(f\"📂 Loading cached mean vectors from {cache_file}\")\n",
    "#         return torch.load(cache_file, map_location='cpu')\n",
    "    \n",
    "#     print(\"🧠 Training enhanced emotional vectors...\")\n",
    "    \n",
    "#     # Extract activations\n",
    "#     print(\"   Extracting neural activations...\")\n",
    "#     batch_activations = process_saved_responses_batch(responses, tokenizer, model)\n",
    "    \n",
    "#     # Initialize mean vectors storage\n",
    "#     from collections import defaultdict\n",
    "#     mean_vectors = defaultdict(lambda: {\n",
    "#         'mean': torch.zeros(model.config.num_hidden_layers, model.config.hidden_size),\n",
    "#         'count': 0\n",
    "#     })\n",
    "    \n",
    "#     # Process annotations to find labels and compute means\n",
    "#     print(\"   Processing annotations and computing means...\")\n",
    "#     for i, (response, annotation) in enumerate(tqdm(zip(responses, annotations), desc=\"Processing\")):\n",
    "#         try:\n",
    "#             # Extract label positions\n",
    "#             from utils import get_label_positions\n",
    "#             label_positions = get_label_positions(annotation, response, tokenizer)\n",
    "            \n",
    "#             # Get activations for this response\n",
    "#             if i < len(batch_activations):\n",
    "#                 activations = batch_activations[i]\n",
    "                \n",
    "#                 # Ensure activation tensor has correct shape\n",
    "#                 if len(activations.shape) == 2 and activations.shape[0] == model.config.num_hidden_layers:\n",
    "#                     # Update overall mean\n",
    "#                     current_count = mean_vectors['overall']['count']\n",
    "#                     current_mean = mean_vectors['overall']['mean']\n",
    "#                     mean_vectors['overall']['mean'] = current_mean + (activations - current_mean) / (current_count + 1)\n",
    "#                     mean_vectors['overall']['count'] += 1\n",
    "                    \n",
    "#                     # Update label-specific means\n",
    "#                     for label in label_positions.keys():\n",
    "#                         if label != 'end-section':\n",
    "#                             current_count = mean_vectors[label]['count']\n",
    "#                             current_mean = mean_vectors[label]['mean']\n",
    "#                             mean_vectors[label]['mean'] = current_mean + (activations - current_mean) / (current_count + 1)\n",
    "#                             mean_vectors[label]['count'] += 1\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(f\"   Error processing response {i}: {e}\")\n",
    "#             continue\n",
    "    \n",
    "#     # Convert to regular dict for saving\n",
    "#     save_dict = {k: {'mean': v['mean'], 'count': v['count']} for k, v in mean_vectors.items()}\n",
    "    \n",
    "#     # Cache the results\n",
    "#     if config[\"enable_caching\"]:\n",
    "#         torch.save(save_dict, cache_file)\n",
    "#         print(f\"💾 Cached mean vectors to {cache_file}\")\n",
    "    \n",
    "#     print(f\"✅ Trained vectors for {len(save_dict)} categories\")\n",
    "#     for label, data in save_dict.items():\n",
    "#         print(f\"   {label}: {data['count']} samples\")\n",
    "    \n",
    "#     return save_dict\n",
    "\n",
    "# # Train the enhanced emotional vectors\n",
    "# enhanced_mean_vectors = train_enhanced_emotional_vectors(\n",
    "#     [r[\"response\"] for r in training_responses],\n",
    "#     annotated_responses,\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     CONFIG\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compute Enhanced Feature Vectors with Depressive-Normal Dichotomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_enhanced_feature_vectors(mean_vectors_dict, config):\n",
    "    \"\"\"Compute feature vectors using the depressive-normal dichotomy approach\"\"\"\n",
    "    \n",
    "    cache_file = os.path.join(config[\"cache_dir\"], \"enhanced_feature_vectors.pt\")\n",
    "    \n",
    "    if config[\"enable_caching\"] and os.path.exists(cache_file):\n",
    "        print(f\"📂 Loading cached feature vectors from {cache_file}\")\n",
    "        return torch.load(cache_file, map_location='cpu')\n",
    "    \n",
    "    print(\"🧮 Computing enhanced feature vectors with depressive-normal dichotomy...\")\n",
    "    \n",
    "    feature_vectors = {}\n",
    "    \n",
    "    # Check if we have normal-thinking vectors to use as baseline\n",
    "    if \"normal-thinking\" in mean_vectors_dict:\n",
    "        baseline_mean = mean_vectors_dict[\"normal-thinking\"][\"mean\"]\n",
    "        baseline_count = mean_vectors_dict[\"normal-thinking\"][\"count\"]\n",
    "        print(f\"✅ Using normal-thinking as baseline ({baseline_count} samples)\")\n",
    "    elif \"overall\" in mean_vectors_dict:\n",
    "        baseline_mean = mean_vectors_dict[\"overall\"][\"mean\"]\n",
    "        baseline_count = mean_vectors_dict[\"overall\"][\"count\"]\n",
    "        print(f\"⚠️  Using overall mean as baseline ({baseline_count} samples)\")\n",
    "    else:\n",
    "        print(\"❌ No baseline vectors available\")\n",
    "        return {}\n",
    "    \n",
    "    # Add baseline to feature vectors\n",
    "    feature_vectors[\"baseline\"] = baseline_mean\n",
    "    \n",
    "    # Compute differential vectors for emotional categories\n",
    "    emotional_labels = [\"depressive-thinking\", \"anxious-thinking\", \"negative-attribution\", \"pessimistic-projection\"]\n",
    "    \n",
    "    for label in emotional_labels:\n",
    "        if label in mean_vectors_dict:\n",
    "            label_mean = mean_vectors_dict[label][\"mean\"]\n",
    "            label_count = mean_vectors_dict[label][\"count\"]\n",
    "            \n",
    "            # Compute difference from baseline (normal thinking)\n",
    "            feature_vectors[label] = label_mean - baseline_mean\n",
    "            print(f\"✅ Computed {label} feature vector ({label_count} samples)\")\n",
    "        else:\n",
    "            print(f\"⚠️  No data found for {label}\")\n",
    "    \n",
    "    # Also include cognitive labels (use overall mean as baseline)\n",
    "    if \"overall\" in mean_vectors_dict:\n",
    "        overall_mean = mean_vectors_dict[\"overall\"][\"mean\"]\n",
    "        cognitive_labels = [\"initializing\", \"deduction\", \"adding-knowledge\", \"example-testing\", \"uncertainty-estimation\", \"backtracking\"]\n",
    "        \n",
    "        for label in cognitive_labels:\n",
    "            if label in mean_vectors_dict:\n",
    "                label_mean = mean_vectors_dict[label][\"mean\"]\n",
    "                feature_vectors[label] = label_mean - overall_mean\n",
    "                print(f\"✅ Computed cognitive {label} feature vector\")\n",
    "    \n",
    "    # Cache the results\n",
    "    if config[\"enable_caching\"]:\n",
    "        torch.save(feature_vectors, cache_file)\n",
    "        print(f\"💾 Cached feature vectors to {cache_file}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Enhanced feature vectors ready:\")\n",
    "    for label in feature_vectors.keys():\n",
    "        if label != \"baseline\":\n",
    "            print(f\"   {label}\")\n",
    "    \n",
    "    return feature_vectors\n",
    "\n",
    "# Compute enhanced feature vectors\n",
    "enhanced_feature_vectors = compute_enhanced_feature_vectors(enhanced_mean_vectors, CONFIG)\n",
    "\n",
    "# Use these as our main feature vectors for steering\n",
    "if enhanced_feature_vectors:\n",
    "    feature_vectors = enhanced_feature_vectors\n",
    "    print(f\"\\n🚀 Ready for enhanced emotional steering!\")\n",
    "else:\n",
    "    print(f\"⚠️  No enhanced feature vectors available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Enhanced Emotional Steering Pipeline\n",
    "\n",
    "Test the new unified emotional steering pipeline with the **depressive-normal dichotomy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced emotional steering pipeline\n",
    "if feature_vectors and len(feature_vectors) > 1:\n",
    "    # Select test messages\n",
    "    test_messages = emotional_messages[-5:]  # Use last 5 emotional messages for testing\n",
    "    \n",
    "    print(f\"🧪 Testing enhanced emotional steering pipeline...\")\n",
    "    print(f\"📝 Test messages: {len(test_messages)}\")\n",
    "    \n",
    "    # Test depressive-normal dichotomy\n",
    "    if \"depressive-thinking\" in feature_vectors and \"normal-thinking\" in feature_vectors:\n",
    "        print(\"\\n🎭 Testing Depressive-Normal Dichotomy\")\n",
    "        \n",
    "        depressive_results = emotional_steering_pipeline(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            feature_vectors=feature_vectors,\n",
    "            steering_config=steering_config,\n",
    "            messages=test_messages,\n",
    "            target_emotional_direction=\"depressive-normal\",\n",
    "            max_new_tokens=300,\n",
    "            batch_size=2\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        results_file = os.path.join(CONFIG[\"results_dir\"], f\"depressive_normal_results_{CONFIG['timestamp']}.json\")\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(depressive_results, f, indent=2)\n",
    "        print(f\"💾 Saved results to {results_file}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️  Depressive-normal vectors not available for testing\")\n",
    "    \n",
    "    # Test anxious-normal dichotomy if available\n",
    "    if \"anxious-thinking\" in feature_vectors:\n",
    "        print(\"\\n🎭 Testing Anxious-Normal Dichotomy\")\n",
    "        \n",
    "        anxious_results = emotional_steering_pipeline(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            feature_vectors=feature_vectors,\n",
    "            steering_config=steering_config,\n",
    "            messages=test_messages[:3],  # Smaller test set\n",
    "            target_emotional_direction=\"anxious-normal\",\n",
    "            max_new_tokens=300,\n",
    "            batch_size=2\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        results_file = os.path.join(CONFIG[\"results_dir\"], f\"anxious_normal_results_{CONFIG['timestamp']}.json\")\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(anxious_results, f, indent=2)\n",
    "        print(f\"💾 Saved results to {results_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Insufficient feature vectors for pipeline testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Demonstrate Individual Steering Examples\n",
    "\n",
    "Show specific examples of how the enhanced emotional steering works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_enhanced_steering(model, tokenizer, feature_vectors, steering_config, config):\n",
    "    \"\"\"Demonstrate enhanced emotional steering with specific examples\"\"\"\n",
    "    \n",
    "    if not feature_vectors or len(feature_vectors) < 2:\n",
    "        print(\"⚠️  Insufficient feature vectors for demonstration\")\n",
    "        return\n",
    "    \n",
    "    demo_messages = [\n",
    "        \"You've been working on a personal project for weeks but haven't made much progress. How do you feel about your abilities?\",\n",
    "        \"You have an important presentation tomorrow. What thoughts are going through your mind?\",\n",
    "        \"You received some feedback on your work. How do you interpret this feedback?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"🎭 Enhanced Emotional Steering Demonstration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    available_labels = [label for label in feature_vectors.keys() if label != \"baseline\"]\n",
    "    emotional_labels = [label for label in available_labels if \"thinking\" in label]\n",
    "    \n",
    "    print(f\"🎯 Available emotional labels: {emotional_labels}\")\n",
    "    \n",
    "    for i, message in enumerate(demo_messages[:min(len(demo_messages), len(emotional_labels))]):\n",
    "        label = emotional_labels[i % len(emotional_labels)]\n",
    "        model_name = config[\"model_name\"]\n",
    "        \n",
    "        if model_name not in steering_config or label not in steering_config[model_name]:\n",
    "            print(f\"⚠️  Skipping {label} - steering config not available\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n📝 Message {i+1}: {message}\")\n",
    "        print(f\"🎯 Demonstrating {label.replace('-', ' ').title()} steering\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # Baseline response\n",
    "            input_ids = tokenizer.encode(message, return_tensors=\"pt\")\n",
    "            \n",
    "            with model.generate(\n",
    "                {\"input_ids\": input_ids, \"attention_mask\": (input_ids != tokenizer.pad_token_id).long()},\n",
    "                max_new_tokens=200,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            ) as tracer:\n",
    "                baseline_output = model.generator.output.save()\n",
    "            \n",
    "            baseline_text = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "            input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            if baseline_text.startswith(input_text):\n",
    "                baseline_text = baseline_text[len(input_text):].strip()\n",
    "            \n",
    "            baseline_analysis = analyze_emotional_content(baseline_text)\n",
    "            \n",
    "            print(f\"🔵 Baseline Response:\")\n",
    "            print(f\"   {baseline_text[:250]}...\")\n",
    "            print(f\"   Emotional Score: {baseline_analysis['total_emotional_score']:.1f}%\")\n",
    "            \n",
    "            # Enhanced steering (toward negative pattern)\n",
    "            enhanced_result = generate_and_analyze_emotional(\n",
    "                model, tokenizer, message, feature_vectors, steering_config,\n",
    "                label, \"positive\", 200\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n🔴 Enhanced {label.replace('-', ' ').title()}:\")\n",
    "            print(f\"   {enhanced_result['response'][:250]}...\")\n",
    "            print(f\"   Emotional Score: {enhanced_result['emotional_analysis']['total_emotional_score']:.1f}%\")\n",
    "            \n",
    "            # Suppressed steering (toward normal pattern)\n",
    "            if \"normal-thinking\" in feature_vectors:\n",
    "                suppressed_result = generate_and_analyze_emotional(\n",
    "                    model, tokenizer, message, feature_vectors, steering_config,\n",
    "                    \"normal-thinking\", \"positive\", 200\n",
    "                )\n",
    "                \n",
    "                print(f\"\\n🟢 Enhanced Normal Thinking:\")\n",
    "                print(f\"   {suppressed_result['response'][:250]}...\")\n",
    "                print(f\"   Emotional Score: {suppressed_result['emotional_analysis']['total_emotional_score']:.1f}%\")\n",
    "            \n",
    "            # Show steering effectiveness\n",
    "            baseline_score = baseline_analysis['total_emotional_score']\n",
    "            enhanced_score = enhanced_result['emotional_analysis']['total_emotional_score']\n",
    "            \n",
    "            effectiveness = abs(enhanced_score - baseline_score)\n",
    "            print(f\"\\n📊 Steering Effectiveness: {effectiveness:.1f}% change\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in demonstration for {label}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✅ Enhanced steering demonstration completed!\")\n",
    "\n",
    "# Run the enhanced demonstration\n",
    "demonstrate_enhanced_steering(model, tokenizer, feature_vectors, steering_config, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Create Enhanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_visualizations(results_dict, config):\n",
    "    \"\"\"Create visualizations for enhanced emotional steering results\"\"\"\n",
    "    \n",
    "    if not results_dict or \"overall_stats\" not in results_dict:\n",
    "        print(\"⚠️  No results available for visualization\")\n",
    "        return\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Enhanced Emotional Reasoning Steering Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    stats = results_dict[\"overall_stats\"]\n",
    "    results = results_dict[\"results\"]\n",
    "    \n",
    "    # Plot 1: Steering Effectiveness Overview\n",
    "    ax1 = axes[0, 0]\n",
    "    categories = ['Baseline', 'Negative Steering', 'Positive Steering']\n",
    "    scores = [\n",
    "        stats['avg_baseline_emotional_score'],\n",
    "        stats['avg_baseline_emotional_score'] + stats['avg_negative_steering_delta'],\n",
    "        stats['avg_baseline_emotional_score'] + stats['avg_positive_steering_delta']\n",
    "    ]\n",
    "    \n",
    "    bars = ax1.bar(categories, scores, color=['gray', 'red', 'green'], alpha=0.7)\n",
    "    ax1.set_title('Steering Effectiveness Overview')\n",
    "    ax1.set_ylabel('Emotional Score (%)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, scores):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{score:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 2: Individual Message Results\n",
    "    ax2 = axes[0, 1]\n",
    "    if results:\n",
    "        message_indices = range(len(results))\n",
    "        baseline_scores = [r['analysis']['baseline_emotional_score'] for r in results if r['analysis']]\n",
    "        negative_scores = [r['analysis']['negative_steered_score'] for r in results if r['analysis']]\n",
    "        positive_scores = [r['analysis']['positive_steered_score'] for r in results if r['analysis']]\n",
    "        \n",
    "        ax2.plot(message_indices[:len(baseline_scores)], baseline_scores, 'o-', label='Baseline', color='gray')\n",
    "        ax2.plot(message_indices[:len(negative_scores)], negative_scores, 's-', label='Negative Steering', color='red')\n",
    "        ax2.plot(message_indices[:len(positive_scores)], positive_scores, '^-', label='Positive Steering', color='green')\n",
    "        \n",
    "        ax2.set_title('Per-Message Steering Results')\n",
    "        ax2.set_xlabel('Message Index')\n",
    "        ax2.set_ylabel('Emotional Score (%)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Steering Deltas\n",
    "    ax3 = axes[1, 0]\n",
    "    delta_categories = ['Negative Steering\\nDelta', 'Positive Steering\\nDelta']\n",
    "    delta_values = [stats['avg_negative_steering_delta'], stats['avg_positive_steering_delta']]\n",
    "    colors = ['red' if d > 0 else 'green' for d in delta_values]\n",
    "    \n",
    "    bars = ax3.bar(delta_categories, delta_values, color=colors, alpha=0.7)\n",
    "    ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax3.set_title('Average Steering Deltas')\n",
    "    ax3.set_ylabel('Score Change (%)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, delta in zip(bars, delta_values):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, \n",
    "                bar.get_height() + (0.1 if delta > 0 else -0.3),\n",
    "                f'{delta:.1f}%', ha='center', va='bottom' if delta > 0 else 'top')\n",
    "    \n",
    "    # Plot 4: Success Metrics\n",
    "    ax4 = axes[1, 1]\n",
    "    success_metrics = {\n",
    "        'Negative Steering\\nSuccess': stats['negative_steering_success'],\n",
    "        'Positive Steering\\nSuccess': stats['positive_steering_success'],\n",
    "        'Overall\\nEffectiveness': stats['avg_steering_effectiveness'] > 1.0  # Threshold for effectiveness\n",
    "    }\n",
    "    \n",
    "    success_labels = list(success_metrics.keys())\n",
    "    success_values = [1 if v else 0 for v in success_metrics.values()]\n",
    "    colors = ['green' if v else 'red' for v in success_values]\n",
    "    \n",
    "    bars = ax4.bar(success_labels, success_values, color=colors, alpha=0.7)\n",
    "    ax4.set_title('Steering Success Metrics')\n",
    "    ax4.set_ylabel('Success (1=Yes, 0=No)')\n",
    "    ax4.set_ylim(0, 1.2)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add success/failure labels\n",
    "    for bar, success in zip(bars, success_values):\n",
    "        label = 'SUCCESS' if success else 'FAILED'\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                label, ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the visualization\n",
    "    viz_path = os.path.join(config['results_dir'], f\"enhanced_steering_results_{config['timestamp']}.png\")\n",
    "    plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"📊 Enhanced visualization saved to {viz_path}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n📈 Enhanced Steering Summary:\")\n",
    "    print(f\"   Target Direction: {results_dict['target_direction']}\")\n",
    "    print(f\"   Messages Processed: {stats['num_processed']}\")\n",
    "    print(f\"   Average Baseline Score: {stats['avg_baseline_emotional_score']:.2f}%\")\n",
    "    print(f\"   Average Steering Effectiveness: {stats['avg_steering_effectiveness']:.2f}%\")\n",
    "    print(f\"   Negative Steering Success: {'✅' if stats['negative_steering_success'] else '❌'}\")\n",
    "    print(f\"   Positive Steering Success: {'✅' if stats['positive_steering_success'] else '❌'}\")\n",
    "\n",
    "# Create visualizations if we have results\n",
    "try:\n",
    "    if 'depressive_results' in locals() and depressive_results:\n",
    "        create_enhanced_visualizations(depressive_results, CONFIG)\n",
    "    else:\n",
    "        print(\"📊 No results available for visualization yet\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Error creating visualizations: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Enhanced Model and Results\n",
    "\n",
    "Save all the enhanced components for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_enhanced_components(config, feature_vectors, mean_vectors, steering_results=None):\n",
    "    \"\"\"Save all enhanced components for future use\"\"\"\n",
    "    \n",
    "    print(\"💾 Saving enhanced components...\")\n",
    "    \n",
    "    model_id = config[\"model_name\"].split('/')[-1].lower()\n",
    "    timestamp = config[\"timestamp\"]\n",
    "    \n",
    "    # Save feature vectors\n",
    "    if feature_vectors:\n",
    "        feature_path = os.path.join(config[\"results_dir\"], f\"enhanced_feature_vectors_{model_id}_{timestamp}.pt\")\n",
    "        torch.save(feature_vectors, feature_path)\n",
    "        print(f\"   ✅ Feature vectors saved to {feature_path}\")\n",
    "    \n",
    "    # Save mean vectors\n",
    "    if mean_vectors:\n",
    "        mean_path = os.path.join(config[\"results_dir\"], f\"enhanced_mean_vectors_{model_id}_{timestamp}.pt\")\n",
    "        torch.save(mean_vectors, mean_path)\n",
    "        print(f\"   ✅ Mean vectors saved to {mean_path}\")\n",
    "    \n",
    "    # Save steering results\n",
    "    if steering_results:\n",
    "        results_path = os.path.join(config[\"results_dir\"], f\"enhanced_steering_results_{model_id}_{timestamp}.json\")\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(steering_results, f, indent=2)\n",
    "        print(f\"   ✅ Steering results saved to {results_path}\")\n",
    "    \n",
    "    # Create a comprehensive summary\n",
    "    summary = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"model_name\": config[\"model_name\"],\n",
    "        \"enhancements\": {\n",
    "            \"depressive_normal_dichotomy\": True,\n",
    "            \"enhanced_training_data\": True,\n",
    "            \"normal_thinking_baseline\": \"normal-thinking\" in (feature_vectors or {}),\n",
    "            \"emotional_categories\": [\n",
    "                \"depressive-thinking\",\n",
    "                \"anxious-thinking\", \n",
    "                \"negative-attribution\",\n",
    "                \"pessimistic-projection\"\n",
    "            ],\n",
    "            \"caching_enabled\": config[\"enable_caching\"]\n",
    "        },\n",
    "        \"available_vectors\": list(feature_vectors.keys()) if feature_vectors else [],\n",
    "        \"training_stats\": {\n",
    "            \"mean_vector_categories\": len(mean_vectors) if mean_vectors else 0,\n",
    "            \"feature_vector_categories\": len(feature_vectors) if feature_vectors else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(config[\"results_dir\"], f\"enhanced_summary_{model_id}_{timestamp}.json\")\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"   ✅ Summary saved to {summary_path}\")\n",
    "    print(f\"\\n🎉 Enhanced COT-Steering implementation complete!\")\n",
    "    print(f\"📁 All files saved to: {config['results_dir']}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Save all enhanced components\n",
    "steering_results_to_save = None\n",
    "if 'depressive_results' in locals():\n",
    "    steering_results_to_save = depressive_results\n",
    "\n",
    "enhanced_summary = save_enhanced_components(\n",
    "    CONFIG,\n",
    "    feature_vectors if 'feature_vectors' in locals() else None,\n",
    "    enhanced_mean_vectors if 'enhanced_mean_vectors' in locals() else None,\n",
    "    steering_results_to_save\n",
    ")\n",
    "\n",
    "print(f\"\\n📋 Enhanced Implementation Summary:\")\n",
    "print(f\"   Model: {enhanced_summary['model_name']}\")\n",
    "print(f\"   Timestamp: {enhanced_summary['timestamp']}\")\n",
    "print(f\"   Available Vectors: {len(enhanced_summary['available_vectors'])}\")\n",
    "print(f\"   Depressive-Normal Dichotomy: {'✅' if enhanced_summary['enhancements']['depressive_normal_dichotomy'] else '❌'}\")\n",
    "print(f\"   Normal Thinking Baseline: {'✅' if enhanced_summary['enhancements']['normal_thinking_baseline'] else '❌'}\")\n",
    "print(f\"   Enhanced Training Data: {'✅' if enhanced_summary['enhancements']['enhanced_training_data'] else '❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Enhanced Safety and Ethics Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_enhanced_safety_report(config, summary):\n",
    "    \"\"\"Generate enhanced safety and ethics report\"\"\"\n",
    "    \n",
    "    report = f\"\"\"\n",
    "# Enhanced Emotional Reasoning Steering - Safety and Ethics Report\n",
    "\n",
    "**Generated on:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "**Model:** {config['model_name']}\n",
    "**Research Session ID:** {config['timestamp']}\n",
    "**Implementation:** Enhanced with Depressive-Normal Dichotomy\n",
    "\n",
    "## Enhanced Safety Features\n",
    "\n",
    "### 1. Depressive-Normal Dichotomy Approach\n",
    "- **Balanced Baseline**: Uses 'normal-thinking' vectors as baseline instead of general mean\n",
    "- **Controlled Contrast**: Emotional vectors computed as difference from healthy thinking patterns\n",
    "- **Bidirectional Steering**: Can steer both toward negative patterns AND toward healthy patterns\n",
    "- **Therapeutic Potential**: Framework designed for potential therapeutic applications\n",
    "\n",
    "### 2. Enhanced Training Data\n",
    "- **40+ Additional Prompts**: More comprehensive coverage of emotional patterns\n",
    "- **15 Normal Thinking Prompts**: Establishes healthy reasoning baseline\n",
    "- **Balanced Dataset**: Equal representation of negative and positive patterns\n",
    "- **Validated Categories**: All prompts categorized and validated for intended patterns\n",
    "\n",
    "### 3. Improved Technical Safeguards\n",
    "- **Comprehensive Caching**: All steps cached to prevent re-generation of harmful content\n",
    "- **Pipeline Architecture**: Unified pipeline with built-in safety checks\n",
    "- **Result Validation**: Automatic validation of steering effectiveness\n",
    "- **Error Handling**: Robust error handling prevents unsafe fallbacks\n",
    "\n",
    "## Risk Mitigation Strategies\n",
    "\n",
    "### 1. Technical Mitigations\n",
    "- **Normal Thinking Enhancement**: Always provide capability to steer toward healthy patterns\n",
    "- **Effectiveness Monitoring**: Track steering effectiveness to detect anomalies\n",
    "- **Automatic Safeguards**: Built-in limits on steering strength and duration\n",
    "- **Reversibility**: All steering effects are reversible with opposite steering\n",
    "\n",
    "### 2. Deployment Safeguards\n",
    "- **Research-Only Design**: Architecture explicitly designed for research use\n",
    "- **Explicit Consent Requirements**: Clear documentation of when steering is active\n",
    "- **Professional Oversight**: Requires qualified mental health professionals for clinical use\n",
    "- **Ethical Review**: Mandatory ethical review for any human subjects research\n",
    "\n",
    "### 3. Monitoring and Evaluation\n",
    "- **Outcome Tracking**: Comprehensive tracking of steering outcomes\n",
    "- **Bias Detection**: Regular evaluation for unintended biases\n",
    "- **User Feedback**: Systems for collecting and acting on user feedback\n",
    "- **Regular Audits**: Scheduled safety and effectiveness audits\n",
    "\n",
    "## Therapeutic Applications\n",
    "\n",
    "### 1. Potential Benefits\n",
    "- **Cognitive Bias Detection**: Identify and counter cognitive distortions\n",
    "- **Mental Health Research**: Study cognitive patterns in depression and anxiety\n",
    "- **Therapeutic Training**: Train therapists to recognize cognitive patterns\n",
    "- **Self-Awareness Tools**: Help individuals recognize their thought patterns\n",
    "\n",
    "### 2. Clinical Safeguards\n",
    "- **Professional Supervision**: Always require licensed mental health professional oversight\n",
    "- **Informed Consent**: Comprehensive informed consent process\n",
    "- **Crisis Protocols**: Clear protocols for mental health crises\n",
    "- **Outcome Monitoring**: Regular assessment of therapeutic outcomes\n",
    "\n",
    "## Implementation Statistics\n",
    "\n",
    "- **Available Steering Vectors**: {len(summary['available_vectors'])} \n",
    "- **Normal Thinking Baseline**: {'Available' if summary['enhancements']['normal_thinking_baseline'] else 'Not Available'}\n",
    "- **Depressive-Normal Dichotomy**: {'Implemented' if summary['enhancements']['depressive_normal_dichotomy'] else 'Not Implemented'}\n",
    "- **Enhanced Training Data**: {'Used' if summary['enhancements']['enhanced_training_data'] else 'Not Used'}\n",
    "- **Caching System**: {'Enabled' if summary['enhancements']['caching_enabled'] else 'Disabled'}\n",
    "\n",
    "## Recommended Usage Guidelines\n",
    "\n",
    "### 1. Research Applications\n",
    "1. Obtain institutional review board (IRB) approval\n",
    "2. Develop comprehensive safety protocols\n",
    "3. Train research staff on ethical considerations\n",
    "4. Implement data security and privacy protections\n",
    "5. Establish mental health support resources\n",
    "\n",
    "### 2. Clinical Applications (Future)\n",
    "1. Require licensed mental health professional supervision\n",
    "2. Implement comprehensive informed consent\n",
    "3. Establish crisis intervention protocols\n",
    "4. Regular clinical outcome monitoring\n",
    "5. Ongoing safety and effectiveness evaluation\n",
    "\n",
    "### 3. Educational Applications\n",
    "1. Clear educational objectives and learning outcomes\n",
    "2. Appropriate instructor training and support\n",
    "3. Student mental health and wellbeing monitoring\n",
    "4. Integration with existing mental health resources\n",
    "5. Regular evaluation of educational effectiveness\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The enhanced emotional reasoning steering implementation represents a significant advancement in the responsible development of AI systems capable of understanding and modulating emotional reasoning patterns. The depressive-normal dichotomy approach provides a more principled and potentially therapeutic framework compared to previous approaches.\n",
    "\n",
    "However, this enhanced capability also requires enhanced responsibility. All implementations must be conducted with appropriate ethical oversight, technical safeguards, and professional supervision. The potential for both beneficial and harmful applications necessitates careful consideration of deployment contexts and ongoing monitoring of outcomes.\n",
    "\n",
    "For questions about this enhanced implementation or to report safety concerns, please contact the research team immediately.\n",
    "\n",
    "---\n",
    "*This report was generated automatically as part of the enhanced COT-steering framework.*\n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and save enhanced safety report\n",
    "enhanced_safety_report = generate_enhanced_safety_report(CONFIG, enhanced_summary)\n",
    "\n",
    "report_path = os.path.join(CONFIG[\"results_dir\"], f\"enhanced_safety_ethics_report_{CONFIG['timestamp']}.md\")\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(enhanced_safety_report)\n",
    "\n",
    "print(\"🛡️ Enhanced Safety and Ethics Report Generated\")\n",
    "print(\"=\" * 60)\n",
    "print(enhanced_safety_report[:1000] + \"...\\n[Report truncated for display]\")\n",
    "print(f\"\\n💾 Full report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This enhanced notebook has provided a comprehensive implementation of emotional reasoning steering with the following key improvements:\n",
    "\n",
    "### ✅ Enhanced Features Implemented:\n",
    "1. **Depressive-Normal Dichotomy**: Emotional vectors computed using normal-thinking as baseline\n",
    "2. **Extended Training Data**: 40+ additional emotional prompts + 15 normal thinking prompts\n",
    "3. **Unified Pipeline**: `emotional_steering_pipeline` function for streamlined processing\n",
    "4. **Comprehensive Caching**: All steps cached for resumable processing\n",
    "5. **Enhanced Safety Framework**: Improved safety measures and ethical guidelines\n",
    "6. **Google Colab Compatibility**: Optimized for cloud-based research\n",
    "\n",
    "### 🔬 Research Applications:\n",
    "- **Mental Health Research**: Study cognitive patterns in depression and anxiety\n",
    "- **Therapeutic AI Development**: Train models to recognize and counter negative thought patterns\n",
    "- **Bias Detection and Mitigation**: Identify problematic thinking patterns in AI outputs\n",
    "- **Cognitive Science Research**: Understand how AI models represent emotional reasoning\n",
    "\n",
    "### ⚠️ Critical Safety Reminders:\n",
    "- This is a **research tool only** - not for production use without extensive safety testing\n",
    "- Requires **ethical oversight** and **IRB approval** for human subjects research\n",
    "- Must include **professional mental health supervision** for any clinical applications\n",
    "- Always provide **positive counterbalancing** capabilities (normal-thinking steering)\n",
    "\n",
    "### 🚀 Future Enhancements:\n",
    "1. **Real-time Safety Monitoring**: Implement automated safety checks during steering\n",
    "2. **Personalized Baselines**: Develop individual-specific normal thinking baselines\n",
    "3. **Multi-modal Integration**: Extend to other modalities (text, audio, visual)\n",
    "4. **Clinical Validation**: Conduct rigorous clinical validation studies\n",
    "5. **Automated Therapeutic Applications**: Develop clinically-validated therapeutic tools\n",
    "\n",
    "Remember to use this enhanced technology responsibly and always prioritize user safety and well-being in your research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
