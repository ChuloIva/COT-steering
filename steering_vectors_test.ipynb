{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChuloIva/COT-steering/blob/main/steering_vectors_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Steering Vectors Test Notebook\n",
        "\n",
        "This notebook is designed to test steering vectors using cached data from the COT-steering project. It loads pre-computed feature vectors and demonstrates emotional reasoning steering.\n",
        "\n",
        "## Key Features:\n",
        "- ‚úÖ Google Colab compatible\n",
        "- ‚úÖ Loads cached vectors from `results/cache/`\n",
        "- ‚úÖ Implements emotional steering functionality\n",
        "- ‚úÖ Uses depressive-normal dichotomy approach\n",
        "- ‚úÖ Safety-focused implementation\n",
        "\n",
        "‚ö†Ô∏è **Research Use Only**: This tool is intended for research purposes with proper ethical oversight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch transformers nnsight openai anthropic python-dotenv tqdm matplotlib seaborn pandas numpy\n",
        "!pip install huggingface_hub\n",
        "\n",
        "print(\"‚úÖ Packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "colab_setup"
      },
      "outputs": [],
      "source": [
        "# Google Colab specific setup\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Check if running in Google Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Clone the repository if not already present\n",
        "    if not os.path.exists('./COT-steering'):\n",
        "        !git clone https://github.com/ChuloIva/COT-steering\n",
        "    \n",
        "    os.chdir('./COT-steering')\n",
        "    print(\"Current working directory:\", os.getcwd())\n",
        "    \n",
        "    # Optional: Link to Google Drive for persistent storage\n",
        "    DRIVE_PATH = '/content/drive/MyDrive/COT_Steering_Results'\n",
        "    if not os.path.exists(DRIVE_PATH):\n",
        "        os.makedirs(DRIVE_PATH)\n",
        "    print(f\"Drive storage path: {DRIVE_PATH}\")\n",
        "else:\n",
        "    print(\"Running locally\")\n",
        "    DRIVE_PATH = './results'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huggingface_login"
      },
      "outputs": [],
      "source": [
        "# Hugging Face login (optional but recommended for better model access)\n",
        "from huggingface_hub import login\n",
        "\n",
        "# You can either set your token here or it will prompt you\n",
        "# Uncomment and add your token if you have one:\n",
        "# login(token=\"your_token_here\", add_to_git_credential=False)\n",
        "\n",
        "# Or just run this to be prompted:\n",
        "try:\n",
        "    login(token=None, add_to_git_credential=False)\n",
        "    print(\"‚úÖ Logged in to Hugging Face\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  Hugging Face login optional - continuing without login\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "## 2. Import Libraries and Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libraries"
      },
      "outputs": [],
      "source": [
        "# Add paths to import local modules\n",
        "sys.path.append('./utils')\n",
        "sys.path.append('./messages')\n",
        "\n",
        "# Import required libraries\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import custom modules\n",
        "try:\n",
        "    from utils import (\n",
        "        load_model_and_vectors,\n",
        "        process_batch_annotations,\n",
        "        process_saved_responses_batch,\n",
        "        custom_generate_steering,\n",
        "        analyze_emotional_content,\n",
        "        generate_and_analyze_emotional,\n",
        "        emotional_steering_pipeline,\n",
        "        steering_config,\n",
        "        chat\n",
        "    )\n",
        "    from messages import messages, eval_messages\n",
        "    print(\"‚úÖ All modules imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è  Import error: {e}\")\n",
        "    print(\"Make sure you're in the COT-steering directory and utils/messages folders exist\")\n",
        "\n",
        "print(f\"üêç Python version: {sys.version}\")\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üíæ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config"
      },
      "source": [
        "## 3. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_config"
      },
      "outputs": [],
      "source": [
        "# Configuration settings\n",
        "CONFIG = {\n",
        "    \"model_name\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",  # Change as needed\n",
        "    \"device\": \"auto\",  # auto-detect, or specify \"cuda\", \"mps\", \"cpu\"\n",
        "    \"load_in_8bit\": False,\n",
        "    \"max_new_tokens\": 300,\n",
        "    \"batch_size\": 2,\n",
        "    \"results_dir\": DRIVE_PATH if IN_COLAB else \"./results\",\n",
        "    \"cache_dir\": \"./results/cache\",  # Where cached vectors are stored\n",
        "    \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "}\n",
        "\n",
        "# Create directories if they don't exist\n",
        "for dir_name in [CONFIG[\"results_dir\"]]:\n",
        "    os.makedirs(dir_name, exist_ok=True)\n",
        "\n",
        "print(f\"üìã Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "# Check if cache directory exists\n",
        "if os.path.exists(CONFIG[\"cache_dir\"]):\n",
        "    cache_files = os.listdir(CONFIG[\"cache_dir\"])\n",
        "    print(f\"\\nüì¶ Found {len(cache_files)} files in cache directory:\")\n",
        "    for file in sorted(cache_files)[:10]:  # Show first 10 files\n",
        "        print(f\"   {file}\")\n",
        "    if len(cache_files) > 10:\n",
        "        print(f\"   ... and {len(cache_files) - 10} more\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Cache directory not found: {CONFIG['cache_dir']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cache_loading"
      },
      "source": [
        "## 4. Load Cached Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_cached_vectors"
      },
      "outputs": [],
      "source": [
        "def load_cached_vectors(cache_dir):\n",
        "    \"\"\"Load cached feature vectors and training data\"\"\"\n",
        "    cached_data = {}\n",
        "    \n",
        "    # Try to load enhanced feature vectors first (best option)\n",
        "    enhanced_feature_path = os.path.join(cache_dir, \"enhanced_feature_vectors.pt\")\n",
        "    if os.path.exists(enhanced_feature_path):\n",
        "        try:\n",
        "            cached_data['feature_vectors'] = torch.load(enhanced_feature_path, map_location='cpu')\n",
        "            print(f\"‚úÖ Loaded enhanced feature vectors: {list(cached_data['feature_vectors'].keys())}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading enhanced feature vectors: {e}\")\n",
        "    \n",
        "    # Try to load enhanced mean vectors\n",
        "    enhanced_mean_path = os.path.join(cache_dir, \"enhanced_mean_vectors.pt\")\n",
        "    if os.path.exists(enhanced_mean_path):\n",
        "        try:\n",
        "            cached_data['mean_vectors'] = torch.load(enhanced_mean_path, map_location='cpu')\n",
        "            print(f\"‚úÖ Loaded enhanced mean vectors: {len(cached_data['mean_vectors'])} categories\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading enhanced mean vectors: {e}\")\n",
        "    \n",
        "    # Try to load training responses and annotations\n",
        "    response_files = [\n",
        "        \"enhanced_training_responses.json\",\n",
        "        \"enhanced_training_responses (2).json\",  # Handle the (2) version\n",
        "        \"training_responses.json\"\n",
        "    ]\n",
        "    \n",
        "    for filename in response_files:\n",
        "        filepath = os.path.join(cache_dir, filename)\n",
        "        if os.path.exists(filepath):\n",
        "            try:\n",
        "                with open(filepath, 'r') as f:\n",
        "                    cached_data['training_responses'] = json.load(f)\n",
        "                print(f\"‚úÖ Loaded training responses from {filename}\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Error loading {filename}: {e}\")\n",
        "    \n",
        "    # Try to load annotations\n",
        "    annotation_files = [\n",
        "        \"enhanced_annotations.json\",\n",
        "        \"annotations.json\"\n",
        "    ]\n",
        "    \n",
        "    for filename in annotation_files:\n",
        "        filepath = os.path.join(cache_dir, filename)\n",
        "        if os.path.exists(filepath):\n",
        "            try:\n",
        "                with open(filepath, 'r') as f:\n",
        "                    cached_data['annotations'] = json.load(f)\n",
        "                print(f\"‚úÖ Loaded annotations from {filename}\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Error loading {filename}: {e}\")\n",
        "    \n",
        "    return cached_data\n",
        "\n",
        "# Load cached data\n",
        "print(\"üîç Loading cached data...\")\n",
        "cached_data = load_cached_vectors(CONFIG[\"cache_dir\"])\n",
        "\n",
        "if cached_data:\n",
        "    print(f\"\\nüì¶ Successfully loaded cached data:\")\n",
        "    for key, value in cached_data.items():\n",
        "        if isinstance(value, dict):\n",
        "            print(f\"   {key}: {len(value)} items\")\n",
        "        elif isinstance(value, list):\n",
        "            print(f\"   {key}: {len(value)} items\")\n",
        "        else:\n",
        "            print(f\"   {key}: loaded\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No cached data found - you may need to run the training notebook first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_loading"
      },
      "source": [
        "## 5. Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "print(\"ü§ñ Loading model and tokenizer...\")\n",
        "\n",
        "try:\n",
        "    model, tokenizer, existing_vectors = load_model_and_vectors(\n",
        "        device=CONFIG[\"device\"],\n",
        "        load_in_8bit=CONFIG[\"load_in_8bit\"],\n",
        "        compute_features=True,\n",
        "        model_name=CONFIG[\"model_name\"]\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Model loaded: {CONFIG['model_name']}\")\n",
        "    print(f\"üìä Device: {next(model.parameters()).device}\")\n",
        "    print(f\"üéØ Model has {model.config.num_hidden_layers} layers\")\n",
        "    print(f\"üìù Vocabulary size: {len(tokenizer)}\")\n",
        "    \n",
        "    # Use cached feature vectors if available, otherwise use any existing ones\n",
        "    if 'feature_vectors' in cached_data:\n",
        "        feature_vectors = cached_data['feature_vectors']\n",
        "        print(f\"üì¶ Using cached feature vectors: {list(feature_vectors.keys())}\")\n",
        "    elif existing_vectors:\n",
        "        feature_vectors = existing_vectors\n",
        "        print(f\"üì¶ Using existing feature vectors: {list(feature_vectors.keys())}\")\n",
        "    else:\n",
        "        feature_vectors = None\n",
        "        print(\"‚ö†Ô∏è  No feature vectors found - will need to train from scratch\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model: {e}\")\n",
        "    print(\"This might be due to:\")\n",
        "    print(\"   - Insufficient GPU/CPU memory\")\n",
        "    print(\"   - Network issues downloading the model\")\n",
        "    print(\"   - Missing Hugging Face authentication for gated models\")\n",
        "    model, tokenizer, feature_vectors = None, None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vector_analysis"
      },
      "source": [
        "## 6. Analyze Available Steering Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "analyze_vectors"
      },
      "outputs": [],
      "source": [
        "if feature_vectors:\n",
        "    print(\"üîç Analyzing available steering vectors...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Categorize available vectors\n",
        "    emotional_vectors = []\n",
        "    cognitive_vectors = []\n",
        "    baseline_vectors = []\n",
        "    \n",
        "    for vector_name in feature_vectors.keys():\n",
        "        if \"thinking\" in vector_name:\n",
        "            if vector_name in [\"depressive-thinking\", \"anxious-thinking\", \"negative-attribution\", \"pessimistic-projection\"]:\n",
        "                emotional_vectors.append(vector_name)\n",
        "            elif vector_name == \"normal-thinking\":\n",
        "                baseline_vectors.append(vector_name)\n",
        "            else:\n",
        "                cognitive_vectors.append(vector_name)\n",
        "        elif vector_name in [\"baseline\", \"overall\"]:\n",
        "            baseline_vectors.append(vector_name)\n",
        "        else:\n",
        "            cognitive_vectors.append(vector_name)\n",
        "    \n",
        "    print(f\"üìä Vector Categories:\")\n",
        "    print(f\"   üß† Cognitive vectors ({len(cognitive_vectors)}): {cognitive_vectors}\")\n",
        "    print(f\"   üòî Emotional vectors ({len(emotional_vectors)}): {emotional_vectors}\")\n",
        "    print(f\"   ‚öñÔ∏è  Baseline vectors ({len(baseline_vectors)}): {baseline_vectors}\")\n",
        "    \n",
        "    # Check vector dimensions\n",
        "    if feature_vectors:\n",
        "        sample_vector = next(iter(feature_vectors.values()))\n",
        "        print(f\"\\nüìè Vector dimensions: {sample_vector.shape}\")\n",
        "        print(f\"   Expected: [{model.config.num_hidden_layers if model else '?'}, {model.config.hidden_size if model else '?'}]\")\n",
        "    \n",
        "    # Check steering configuration compatibility\n",
        "    if model:\n",
        "        model_name = CONFIG[\"model_name\"]\n",
        "        if model_name in steering_config:\n",
        "            available_configs = list(steering_config[model_name].keys())\n",
        "            compatible_vectors = [v for v in feature_vectors.keys() if v in available_configs]\n",
        "            \n",
        "            print(f\"\\n‚öôÔ∏è  Steering Configuration:\")\n",
        "            print(f\"   üìã Available configs: {len(available_configs)}\")\n",
        "            print(f\"   ‚úÖ Compatible vectors: {len(compatible_vectors)}\")\n",
        "            \n",
        "            if compatible_vectors:\n",
        "                print(f\"   üéØ Ready for steering: {compatible_vectors}\")\n",
        "            else:\n",
        "                print(f\"   ‚ö†Ô∏è  No compatible vectors found for steering\")\n",
        "                print(f\"   Available vectors: {list(feature_vectors.keys())}\")\n",
        "                print(f\"   Available configs: {available_configs}\")\n",
        "        else:\n",
        "            print(f\"\\n‚ùå No steering configuration found for {model_name}\")\n",
        "            print(f\"   Available models in config: {list(steering_config.keys())}\")\n",
        "    \n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No feature vectors available for analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "steering_demo"
      },
      "source": [
        "## 7. Steering Vector Demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demo_steering"
      },
      "outputs": [],
      "source": [
        "def demonstrate_steering(model, tokenizer, feature_vectors, test_message, steering_label, max_tokens=200):\n",
        "    \"\"\"Demonstrate steering with a specific vector\"\"\"\n",
        "    \n",
        "    if not model or not tokenizer or not feature_vectors:\n",
        "        print(\"‚ùå Missing required components for steering\")\n",
        "        return None\n",
        "    \n",
        "    if steering_label not in feature_vectors:\n",
        "        print(f\"‚ùå Steering vector '{steering_label}' not found\")\n",
        "        return None\n",
        "    \n",
        "    model_name = CONFIG[\"model_name\"]\n",
        "    if model_name not in steering_config or steering_label not in steering_config[model_name]:\n",
        "        print(f\"‚ùå No steering configuration for '{steering_label}' with model {model_name}\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"üéØ Demonstrating {steering_label.replace('-', ' ').title()} Steering\")\n",
        "    print(f\"üìù Test message: {test_message}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        # Generate baseline response\n",
        "        print(\"üîµ Baseline Response:\")\n",
        "        input_ids = tokenizer.encode(test_message, return_tensors=\"pt\")\n",
        "        \n",
        "        with model.generate(\n",
        "            {\"input_ids\": input_ids, \"attention_mask\": (input_ids != tokenizer.pad_token_id).long()},\n",
        "            max_new_tokens=max_tokens,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        ) as tracer:\n",
        "            baseline_output = model.generator.output.save()\n",
        "        \n",
        "        baseline_text = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
        "        input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "        if baseline_text.startswith(input_text):\n",
        "            baseline_text = baseline_text[len(input_text):].strip()\n",
        "        \n",
        "        print(f\"   {baseline_text[:300]}...\")\n",
        "        \n",
        "        # Analyze baseline emotional content\n",
        "        baseline_analysis = analyze_emotional_content(baseline_text)\n",
        "        print(f\"   üìä Emotional Score: {baseline_analysis['total_emotional_score']:.1f}%\")\n",
        "        \n",
        "        # Generate steered response\n",
        "        print(f\"\\nüî¥ Steered Response ({steering_label}):\")\n",
        "        steered_result = generate_and_analyze_emotional(\n",
        "            model, tokenizer, test_message, feature_vectors, steering_config,\n",
        "            steering_label, \"positive\", max_tokens\n",
        "        )\n",
        "        \n",
        "        if steered_result:\n",
        "            print(f\"   {steered_result['response'][:300]}...\")\n",
        "            steered_score = steered_result['emotional_analysis']['total_emotional_score']\n",
        "            print(f\"   üìä Emotional Score: {steered_score:.1f}%\")\n",
        "            \n",
        "            # Calculate steering effectiveness\n",
        "            effectiveness = steered_score - baseline_analysis['total_emotional_score']\n",
        "            print(f\"   üìà Steering Effect: {effectiveness:+.1f}% change\")\n",
        "            \n",
        "            if abs(effectiveness) > 5.0:  # Threshold for significant change\n",
        "                print(f\"   ‚úÖ Steering successful!\")\n",
        "            else:\n",
        "                print(f\"   ‚ö†Ô∏è  Steering effect minimal\")\n",
        "            \n",
        "            return {\n",
        "                'baseline_score': baseline_analysis['total_emotional_score'],\n",
        "                'steered_score': steered_score,\n",
        "                'effectiveness': effectiveness,\n",
        "                'baseline_text': baseline_text,\n",
        "                'steered_text': steered_result['response']\n",
        "            }\n",
        "        else:\n",
        "            print(f\"   ‚ùå Steering failed\")\n",
        "            return None\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during steering demonstration: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Test messages for demonstration\n",
        "test_messages = [\n",
        "    \"You've been working on a personal project for weeks but haven't made much progress. How do you feel about your abilities?\",\n",
        "    \"You have an important presentation tomorrow that could determine your career future. What thoughts are going through your mind?\",\n",
        "    \"After receiving feedback on your recent work, you notice a pattern of similar comments throughout your career. What does this suggest?\"\n",
        "]\n",
        "\n",
        "# Run steering demonstrations\n",
        "if model and tokenizer and feature_vectors:\n",
        "    print(\"üé≠ Steering Vector Demonstrations\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Find available emotional vectors for demonstration\n",
        "    available_emotional = [v for v in feature_vectors.keys() if \"thinking\" in v and v != \"normal-thinking\"]\n",
        "    \n",
        "    if available_emotional:\n",
        "        # Demonstrate with the first available emotional vector\n",
        "        demo_vector = available_emotional[0]\n",
        "        demo_message = test_messages[0]\n",
        "        \n",
        "        result = demonstrate_steering(model, tokenizer, feature_vectors, demo_message, demo_vector)\n",
        "        \n",
        "        if result:\n",
        "            print(f\"\\n‚úÖ Successfully demonstrated {demo_vector} steering!\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No emotional steering vectors available for demonstration\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Cannot run steering demonstration - missing required components\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_testing"
      },
      "source": [
        "## 8. Custom Message Testing\n",
        "\n",
        "Use this section to test steering with your own custom messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "custom_test"
      },
      "outputs": [],
      "source": [
        "# Custom message testing\n",
        "def test_custom_message(message, steering_vector=None):\n",
        "    \"\"\"Test steering with a custom message\"\"\"\n",
        "    \n",
        "    if not all([model, tokenizer, feature_vectors]):\n",
        "        print(\"‚ùå Required components not available\")\n",
        "        return\n",
        "    \n",
        "    # If no steering vector specified, use the first available emotional vector\n",
        "    if steering_vector is None:\n",
        "        emotional_vectors = [v for v in feature_vectors.keys() if \"thinking\" in v and v != \"normal-thinking\"]\n",
        "        if emotional_vectors:\n",
        "            steering_vector = emotional_vectors[0]\n",
        "        else:\n",
        "            print(\"‚ùå No emotional steering vectors available\")\n",
        "            return\n",
        "    \n",
        "    print(f\"üéØ Testing Custom Message\")\n",
        "    print(f\"üìù Message: {message}\")\n",
        "    print(f\"üé≠ Steering: {steering_vector}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    result = demonstrate_steering(model, tokenizer, feature_vectors, message, steering_vector)\n",
        "    \n",
        "    if result:\n",
        "        print(f\"\\n‚úÖ Custom test completed successfully!\")\n",
        "        return result\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Custom test failed\")\n",
        "        return None\n",
        "\n",
        "# Example custom tests - modify these or add your own!\n",
        "custom_messages = [\n",
        "    \"I'm thinking about starting a new business venture. What should I consider?\",\n",
        "    \"My friend didn't respond to my message for a week. What might this mean?\",\n",
        "    \"I'm preparing for a job interview at my dream company. How should I approach this?\"\n",
        "]\n",
        "\n",
        "# Run custom tests\n",
        "print(\"üß™ Custom Message Tests\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if model and tokenizer and feature_vectors:\n",
        "    for i, msg in enumerate(custom_messages[:1], 1):  # Test first message\n",
        "        print(f\"\\nCustom Test {i}:\")\n",
        "        test_custom_message(msg)\n",
        "        \n",
        "    print(f\"\\nüí° To test your own messages, modify the 'custom_messages' list above and re-run this cell!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Cannot run custom tests - missing required components\")\n",
        "    \n",
        "print(\"\\n\" + \"=\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## 9. Session Summary\n",
        "\n",
        "This notebook has demonstrated the steering vector functionality using cached data from the COT-steering project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "session_summary"
      },
      "outputs": [],
      "source": [
        "print(\"üéâ Steering Vectors Test Session Complete!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Session summary\n",
        "print(f\"üìã Session Summary:\")\n",
        "print(f\"   Model: {CONFIG['model_name']}\")\n",
        "print(f\"   Timestamp: {CONFIG['timestamp']}\")\n",
        "print(f\"   Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
        "\n",
        "if cached_data:\n",
        "    print(f\"   ‚úÖ Cached data loaded: {len(cached_data)} items\")\n",
        "else:\n",
        "    print(f\"   ‚ùå No cached data found\")\n",
        "\n",
        "if feature_vectors:\n",
        "    print(f\"   ‚úÖ Feature vectors available: {len(feature_vectors)}\")\n",
        "    emotional_vectors = [v for v in feature_vectors.keys() if \"thinking\" in v and v != \"normal-thinking\"]\n",
        "    print(f\"   üß† Emotional steering vectors: {len(emotional_vectors)}\")\n",
        "else:\n",
        "    print(f\"   ‚ùå No feature vectors loaded\")\n",
        "\n",
        "print(f\"\\nüöÄ Next Steps:\")\n",
        "print(f\"   1. üß™ Try custom messages in the testing section above\")\n",
        "print(f\"   2. üî¨ Explore different steering vectors and their effects\")\n",
        "print(f\"   3. üìñ Review the safety and ethical guidelines in the main project\")\n",
        "print(f\"   4. üõ†Ô∏è  Consider training new vectors with your own data\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  Safety Reminders:\")\n",
        "print(f\"   - This is a research tool - use responsibly\")\n",
        "print(f\"   - Emotional steering can have significant effects\")\n",
        "print(f\"   - Always provide balanced perspectives in applications\")\n",
        "print(f\"   - Obtain proper ethical oversight for human subjects research\")\n",
        "\n",
        "print(f\"\\nüìÅ Results saved to: {CONFIG['results_dir']}\")\n",
        "print(\"\\n‚úÖ Session completed successfully!\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMxyz...",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}